



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.dev0, mkdocs-material-4.6.0">
    
    
      
        <title>Losses - PyTorch Metric Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.1b62728e.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.a8b3c06d.css">
      
      
        
        
        <meta name="theme-color" content="#ef5350">
      
    
    
      <script src="../assets/javascripts/modernizr.268332fc.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="red" data-md-color-accent="red">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#losses" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href=".." title="PyTorch Metric Learning" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              PyTorch Metric Learning
            </span>
            <span class="md-header-nav__topic">
              
                Losses
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/KevinMusgrave/pytorch-metric-learning/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    KevinMusgrave/pytorch-metric-learning
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main" role="main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href=".." title="PyTorch Metric Learning" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    PyTorch Metric Learning
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/KevinMusgrave/pytorch-metric-learning/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    KevinMusgrave/pytorch-metric-learning
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Losses
      </label>
    
    <a href="./" title="Losses" class="md-nav__link md-nav__link--active">
      Losses
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#angularloss" class="md-nav__link">
    AngularLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#arcfaceloss" class="md-nav__link">
    ArcFaceLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basemetriclossfunction" class="md-nav__link">
    BaseMetricLossFunction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contrastiveloss" class="md-nav__link">
    ContrastiveLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cosfaceloss" class="md-nav__link">
    CosFaceLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#crossbatchmemory" class="md-nav__link">
    CrossBatchMemory
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fastaploss" class="md-nav__link">
    FastAPLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#genericpairloss" class="md-nav__link">
    GenericPairLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generalizedliftedstructureloss" class="md-nav__link">
    GeneralizedLiftedStructureLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#largemarginsoftmaxloss" class="md-nav__link">
    LargeMarginSoftmaxLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#marginloss" class="md-nav__link">
    MarginLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multiplelosses" class="md-nav__link">
    MultipleLosses
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multisimilarityloss" class="md-nav__link">
    MultiSimilarityLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ncaloss" class="md-nav__link">
    NCALoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#normalizedsoftmaxloss" class="md-nav__link">
    NormalizedSoftmaxLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#npairsloss" class="md-nav__link">
    NPairsLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ntxentloss" class="md-nav__link">
    NTXentLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#proxyncaloss" class="md-nav__link">
    ProxyNCALoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#signaltonoiseratiocontrastiveloss" class="md-nav__link">
    SignalToNoiseRatioContrastiveLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#softtripleloss" class="md-nav__link">
    SoftTripleLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spherefaceloss" class="md-nav__link">
    SphereFaceLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tripletmarginloss" class="md-nav__link">
    TripletMarginLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#weightregularizermixin" class="md-nav__link">
    WeightRegularizerMixin
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../miners/" title="Miners" class="md-nav__link">
      Miners
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../regularizers/" title="Regularizers" class="md-nav__link">
      Regularizers
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../samplers/" title="Samplers" class="md-nav__link">
      Samplers
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../trainers/" title="Trainers" class="md-nav__link">
      Trainers
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../testers/" title="Testers" class="md-nav__link">
      Testers
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../utils/" title="Utils" class="md-nav__link">
      Utils
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#angularloss" class="md-nav__link">
    AngularLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#arcfaceloss" class="md-nav__link">
    ArcFaceLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basemetriclossfunction" class="md-nav__link">
    BaseMetricLossFunction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contrastiveloss" class="md-nav__link">
    ContrastiveLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cosfaceloss" class="md-nav__link">
    CosFaceLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#crossbatchmemory" class="md-nav__link">
    CrossBatchMemory
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fastaploss" class="md-nav__link">
    FastAPLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#genericpairloss" class="md-nav__link">
    GenericPairLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generalizedliftedstructureloss" class="md-nav__link">
    GeneralizedLiftedStructureLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#largemarginsoftmaxloss" class="md-nav__link">
    LargeMarginSoftmaxLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#marginloss" class="md-nav__link">
    MarginLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multiplelosses" class="md-nav__link">
    MultipleLosses
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multisimilarityloss" class="md-nav__link">
    MultiSimilarityLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ncaloss" class="md-nav__link">
    NCALoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#normalizedsoftmaxloss" class="md-nav__link">
    NormalizedSoftmaxLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#npairsloss" class="md-nav__link">
    NPairsLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ntxentloss" class="md-nav__link">
    NTXentLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#proxyncaloss" class="md-nav__link">
    ProxyNCALoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#signaltonoiseratiocontrastiveloss" class="md-nav__link">
    SignalToNoiseRatioContrastiveLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#softtripleloss" class="md-nav__link">
    SoftTripleLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spherefaceloss" class="md-nav__link">
    SphereFaceLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tripletmarginloss" class="md-nav__link">
    TripletMarginLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#weightregularizermixin" class="md-nav__link">
    WeightRegularizerMixin
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/KevinMusgrave/pytorch-metric-learning/edit/master/docs/losses.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="losses">Losses<a class="headerlink" href="#losses" title="Permanent link">&para;</a></h1>
<p>All loss functions are used as follows:</p>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_metric_learning</span> <span class="kn">import</span> <span class="n">losses</span>
<span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SomeLoss</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</pre></div>


<p>Or if you are using a loss in conjunction with a miner:</p>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_metric_learning</span> <span class="kn">import</span> <span class="n">miners</span><span class="p">,</span> <span class="n">losses</span>
<span class="n">miner_func</span> <span class="o">=</span> <span class="n">miners</span><span class="o">.</span><span class="n">SomeMiner</span><span class="p">()</span>
<span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SomeLoss</span><span class="p">()</span>
<span class="n">miner_output</span> <span class="o">=</span> <span class="n">miner_func</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">losses</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">miner_output</span><span class="p">)</span>
</pre></div>


<h2 id="angularloss">AngularLoss<a class="headerlink" href="#angularloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1708.01682.pdf">Deep Metric Learning with Angular Loss</a></p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">AngularLoss</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">triplets_per_anchor</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>alpha</strong>: The angle (as described in the paper), specified in degrees.</li>
<li><strong>triplets_per_anchor</strong>: The number of triplets per element to sample within a batch. Can be an integer or the string "all". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is "all", then all possible triplets in the batch will be used.</li>
</ul>
<h2 id="arcfaceloss">ArcFaceLoss<a class="headerlink" href="#arcfaceloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1801.07698.pdf">ArcFace: Additive Angular Margin Loss for Deep Face Recognition</a></p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">ArcFaceLoss</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>margin</strong>: The angular margin penalty in degrees. </li>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>scale</strong>: The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.)</li>
</ul>
<p><strong>Other info</strong>: </p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts a <code>regularizer</code> and <code>reg_weight</code> as optional init arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">ArcFaceLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>


<h2 id="basemetriclossfunction">BaseMetricLossFunction<a class="headerlink" href="#basemetriclossfunction" title="Permanent link">&para;</a></h2>
<p>All loss functions extend this class and therefore inherit its <code>__init__</code> parameters.</p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">BaseMetricLossFunction</span><span class="p">(</span><span class="n">normalize_embeddings</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_class_per_param</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">learnable_param_names</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>normalize_embeddings</strong>: If True, embeddings will be normalized to have a Euclidean norm of 1 before the loss is computed.</li>
<li><strong>num_class_per_param</strong>: If <em>learnable_param_names</em> is set, then this represents the number of classes for each parameter. If your parameters don't have a separate value for each class, then you can leave this at None.</li>
<li><strong>learnable_param_names</strong>: A list of strings where each element is the name of attributes that should be converted to nn.Parameter. If None, then no parameters are converted. </li>
</ul>
<p><strong>Required Implementations</strong>:</p>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">indices_tuple</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</pre></div>


<h2 id="contrastiveloss">ContrastiveLoss<a class="headerlink" href="#contrastiveloss" title="Permanent link">&para;</a></h2>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">ContrastiveLoss</span><span class="p">(</span><span class="n">pos_margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> 
                    <span class="n">neg_margin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                    <span class="n">use_similarity</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                    <span class="n">power</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                    <span class="n">avg_non_zero_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>pos_margin</strong>: The distance (or similarity) over (under) which positive pairs will contribute to the loss.</li>
<li><strong>neg_margin</strong>: The distance (or similarity) under (over) which negative pairs will contribute to the loss.  </li>
<li><strong>use_similarity</strong>: If True, will use dot product between vectors instead of euclidean distance.</li>
<li><strong>power</strong>: Each pair's loss will be raised to this power.</li>
<li><strong>avg_non_zero_only</strong>: Only pairs that contribute non-zero loss will be used in the final loss. </li>
</ul>
<p>Note that the default values for <code>pos_margin</code> and <code>neg_margin</code> are suitable if <code>use_similarity = False</code>. If you set <code>use_similarity = True</code>, then more appropriate values would be <code>pos_margin = 1</code> and <code>neg_margin = 0</code>.</p>
<h2 id="cosfaceloss">CosFaceLoss<a class="headerlink" href="#cosfaceloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1801.09414.pdf">CosFace: Large Margin Cosine Loss for Deep Face Recognition</a></p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">CosFaceLoss</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>margin</strong>: The cosine margin penalty: <code>cos(theta) - margin</code>. The paper got optimal performance with margin values between 0.25 and 0.45.</li>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>scale</strong>: The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.)</li>
</ul>
<p><strong>Other info</strong>: </p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts a <code>regularizer</code> and <code>reg_weight</code> as optional init arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">CosFaceLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>


<h2 id="crossbatchmemory">CrossBatchMemory<a class="headerlink" href="#crossbatchmemory" title="Permanent link">&para;</a></h2>
<p>This wraps a loss function, and implements <a href="https://arxiv.org/pdf/1912.06798.pdf">Cross-Batch Memory for Embedding Learning</a>. It stores embeddings from previous iterations in a queue, and uses them to form more pairs/triplets with the current iteration's embeddings.</p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">CrossBatchMemory</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">memory_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">miner</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss function to be wrapped. For example, you could pass in <code>ContrastiveLoss()</code>.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>memory_size</strong>: The size of the memory queue.</li>
<li><strong>miner</strong>: An optional miner, which will be used to mine pairs/triplets from the memory queue.</li>
</ul>
<h2 id="fastaploss">FastAPLoss<a class="headerlink" href="#fastaploss" title="Permanent link">&para;</a></h2>
<p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Cakir_Deep_Metric_Learning_to_Rank_CVPR_2019_paper.pdf">Deep Metric Learning to Rank</a></p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">FastAPLoss</span><span class="p">(</span><span class="n">num_bins</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>num_bins</strong>: The number of soft histogram bins for calculating average precision</li>
</ul>
<h2 id="genericpairloss">GenericPairLoss<a class="headerlink" href="#genericpairloss" title="Permanent link">&para;</a></h2>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">GenericPairLoss</span><span class="p">(</span><span class="n">use_similarity</span><span class="p">,</span> <span class="n">mat_based_loss</span><span class="p">,</span> <span class="n">squared_distances</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>use_similarity</strong>: Set to True if the loss function uses pairwise similarity (dot product of each embedding pair). Otherwise, euclidean distance will be used.</li>
<li><strong>mat_based_loss</strong>: See required implementations.</li>
<li><strong>squared_distances</strong>: If True, then the euclidean distance will be squared.</li>
</ul>
<p><strong>Required Implementations</strong>:</p>
<div class="codehilite"><pre><span></span><span class="c1"># If mat_based_loss is True, then this takes in mat, pos_mask, and neg_mask</span>
<span class="c1"># If False, this takes in pos_pairs and neg_pairs</span>
<span class="k">def</span> <span class="nf">_compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</pre></div>


<h2 id="generalizedliftedstructureloss">GeneralizedLiftedStructureLoss<a class="headerlink" href="#generalizedliftedstructureloss" title="Permanent link">&para;</a></h2>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">GeneralizedLiftedStructureLoss</span><span class="p">(</span><span class="n">neg_margin</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>neg_margin</strong>: The margin in the expression <code>e^(margin - negative_distance)</code></li>
</ul>
<h2 id="largemarginsoftmaxloss">LargeMarginSoftmaxLoss<a class="headerlink" href="#largemarginsoftmaxloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1612.02295.pdf">Large-Margin Softmax Loss for Convolutional Neural Networks</a></p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">LargeMarginSoftmaxLoss</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">normalize_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>margin</strong>: An integer which dictates the size of the angular margin. Specifically, it multiplies the angle between the embeddings and weights: <code>cos(margin*theta)</code>. </li>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>scale</strong>: The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.)</li>
<li><strong>normalize_weights</strong>: If True, the learned weights will be normalized to have Euclidean norm of 1, before the loss is computed. Note that when this parameter is True, it becomes equivalent to <a href="./#spherefaceloss">SphereFaceLoss</a>.</li>
</ul>
<p><strong>Other info</strong>: </p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts a <code>regularizer</code> and <code>reg_weight</code> as optional init arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">LargeMarginSoftmaxLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>


<h2 id="marginloss">MarginLoss<a class="headerlink" href="#marginloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1706.07567.pdf">Sampling Matters in Deep Embedding Learning</a></p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">MarginLoss</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">triplets_per_anchor</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>margin</strong>: The radius of the minimalbuffer between positive and negative pairs.</li>
<li><strong>nu</strong>: The regularization weight for the magnitude of beta.</li>
<li><strong>beta</strong>: The center of the minimal buffer between positive and negative pairs.</li>
<li><strong>triplets_per_anchor</strong>: The number of triplets per element to sample within a batch. Can be an integer or the string "all". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is "all", then all possible triplets in the batch will be used.</li>
</ul>
<p>To make beta a learnable parameter (as done in the paper), pass in the keyword argument:</p>
<div class="codehilite"><pre><span></span><span class="n">learnable_param_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;beta&quot;</span><span class="p">]</span>
</pre></div>


<p>You can then pass the loss function's parameters() to any PyTorch optimizer.</p>
<h2 id="multiplelosses">MultipleLosses<a class="headerlink" href="#multiplelosses" title="Permanent link">&para;</a></h2>
<p>This is a simple wrapper for multiple losses. Pass in a list of already-initialized loss functions. Then, when you call forward on this object, it will return the average loss across the wrapped loss functions.</p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">MultipleLosses</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>losses</strong>: A list of initialized loss functions. On the forward call of MultipleLosses, each wrapped loss will be computed, and then the average will be returned.</li>
</ul>
<h2 id="multisimilarityloss">MultiSimilarityLoss<a class="headerlink" href="#multisimilarityloss" title="Permanent link">&para;</a></h2>
<p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Multi-Similarity_Loss_With_General_Pair_Weighting_for_Deep_Metric_Learning_CVPR_2019_paper.pdf">Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning</a></p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">MultiSimilarityLoss</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>alpha</strong>: The weight applied to positive pairs.</li>
<li><strong>beta</strong>: The weight applied to negative pairs.</li>
<li><strong>base</strong>: The offset applied to the exponent in the loss.</li>
</ul>
<h2 id="ncaloss">NCALoss<a class="headerlink" href="#ncaloss" title="Permanent link">&para;</a></h2>
<p><a href="https://www.cs.toronto.edu/~hinton/absps/nca.pdf">Neighbourhood Components Analysis</a></p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">NCALoss</span><span class="p">(</span><span class="n">softmax_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>softmax_scale</strong>: The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.)</li>
</ul>
<h2 id="normalizedsoftmaxloss">NormalizedSoftmaxLoss<a class="headerlink" href="#normalizedsoftmaxloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1811.12649.pdf">Classification is a Strong Baseline for Deep Metric Learning</a></p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">NormalizedSoftmaxLoss</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>temperature</strong>: The exponent divisor in the softmax funtion.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
</ul>
<p><strong>Other info</strong></p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts a <code>regularizer</code> and <code>reg_weight</code> as optional init arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">NormalizedSoftmaxLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>


<h2 id="npairsloss">NPairsLoss<a class="headerlink" href="#npairsloss" title="Permanent link">&para;</a></h2>
<p><a href="http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf">Improved Deep Metric Learning with Multi-class N-pair Loss Objective</a></p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">NPairsLoss</span><span class="p">(</span><span class="n">l2_reg_weight</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>l2_reg_weight</strong>: The regularization weight for the L2 norm of the embeddings.</li>
</ul>
<h2 id="ntxentloss">NTXentLoss<a class="headerlink" href="#ntxentloss" title="Permanent link">&para;</a></h2>
<p>This is the normalized temperature-scaled cross entropy loss used in <a href="https://arxiv.org/abs/2002.05709">A Simple Framework for Contrastive Learning of Visual Representations</a>.</p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">NTXentLoss</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>temperature</strong>: The exponent divisor in the softmax funtion.</li>
</ul>
<h2 id="proxyncaloss">ProxyNCALoss<a class="headerlink" href="#proxyncaloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1703.07464.pdf">No Fuss Distance Metric Learning using Proxies</a></p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">ProxyNCALoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>softmax_scale</strong>: See <a href="./#ncaloss">NCALoss</a></li>
</ul>
<p><strong>Other info</strong></p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts a <code>regularizer</code> and <code>reg_weight</code> as optional init arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">ProxyNCALoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>


<h2 id="signaltonoiseratiocontrastiveloss">SignalToNoiseRatioContrastiveLoss<a class="headerlink" href="#signaltonoiseratiocontrastiveloss" title="Permanent link">&para;</a></h2>
<p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yuan_Signal-To-Noise_Ratio_A_Robust_Distance_Metric_for_Deep_Metric_Learning_CVPR_2019_paper.pdf">Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning</a></p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">SignalToNoiseRatioContrastiveLoss</span><span class="p">(</span><span class="n">pos_margin</span><span class="p">,</span> 
                                        <span class="n">neg_margin</span><span class="p">,</span> 
                                        <span class="n">regularizer_weight</span><span class="p">,</span> 
                                        <span class="n">avg_non_zero_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                        <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>pos_margin</strong>: The noise-to-signal ratio over which positive pairs will contribute to the loss.</li>
<li><strong>neg_margin</strong>: The noise-to-signal ratio under which negative pairs will contribute to the loss.</li>
<li><strong>regularizer_weight</strong>: The regularizer encourages the embeddings to have zero-mean distributions. </li>
<li><strong>avg_non_zero_only</strong>: Only pairs that contribute non-zero loss will be used in the final loss. </li>
</ul>
<h2 id="softtripleloss">SoftTripleLoss<a class="headerlink" href="#softtripleloss" title="Permanent link">&para;</a></h2>
<p><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Qian_SoftTriple_Loss_Deep_Metric_Learning_Without_Triplet_Sampling_ICCV_2019_paper.pdf">SoftTriple Loss: Deep Metric Learning Without Triplet Sampling</a></p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">SoftTripleLoss</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> 
                    <span class="n">num_classes</span><span class="p">,</span> 
                    <span class="n">centers_per_class</span><span class="p">,</span> 
                    <span class="n">la</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
                    <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
                    <span class="n">reg_weight</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> 
                    <span class="n">margin</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> 
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>centers_per_class</strong>: The number of weight vectors per class. (The regular cross entropy loss has 1 center per class.)</li>
<li><strong>la</strong>: The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.)</li>
<li><strong>gamma</strong>: The similarity-to-centers multiplier.</li>
<li><strong>reg_weight</strong>: The regularization weight which encourages class centers to be close to each other.</li>
<li><strong>margin</strong>: The margin in the expression e^(similarities - margin).</li>
</ul>
<p><strong>Other info</strong></p>
<ul>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SoftTripleLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>


<h2 id="spherefaceloss">SphereFaceLoss<a class="headerlink" href="#spherefaceloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1704.08063.pdf">SphereFace: Deep Hypersphere Embedding for Face Recognition</a></p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">SphereFaceLoss</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>margin</strong>: An integer which dictates the size of the angular margin. Specifically, it multiplies the angle between the embeddings and weights: <code>cos(margin*theta)</code>. </li>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>scale</strong>: The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.)</li>
</ul>
<p><strong>Other info</strong></p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts a <code>regularizer</code> and <code>reg_weight</code> as optional init arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:</li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SphereFaceLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>


<h2 id="tripletmarginloss">TripletMarginLoss<a class="headerlink" href="#tripletmarginloss" title="Permanent link">&para;</a></h2>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">TripletMarginLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> 
                        <span class="n">distance_norm</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">power</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                        <span class="n">swap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                        <span class="n">smooth_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                        <span class="n">avg_non_zero_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                        <span class="n">triplets_per_anchor</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
                        <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>margin</strong>: The desired difference between the anchor-positive distance and the anchor-negative distance.</li>
<li><strong>distance_norm</strong>: The norm used when calculating distance between embeddings</li>
<li><strong>power</strong>: Each pair's loss will be raised to this power.</li>
<li><strong>swap</strong>: Use the positive-negative distance instead of anchor-negative distance, if it violates the margin more.</li>
<li><strong>smooth_loss</strong>: Use the log-exp version of the triplet loss</li>
<li><strong>avg_non_zero_only</strong>: Only triplets that contribute non-zero loss will be used in the final loss.</li>
<li><strong>triplets_per_anchor</strong>: The number of triplets per element to sample within a batch. Can be an integer or the string "all". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is "all", then all possible triplets in the batch will be used.</li>
</ul>
<h2 id="weightregularizermixin">WeightRegularizerMixin<a class="headerlink" href="#weightregularizermixin" title="Permanent link">&para;</a></h2>
<p>Losses can extend this class in addition to BaseMetricLossFunction. You should extend this class if your loss function can make use of a <a href="../regularizers/">weight regularizer</a>.</p>
<div class="codehilite"><pre><span></span><span class="n">losses</span><span class="o">.</span><span class="n">WeightRegularizerMixin</span><span class="p">(</span><span class="n">regularizer</span><span class="p">,</span> <span class="n">reg_weight</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>regularizer</strong>: The <a href="../regularizers/">regularizer</a> to apply to the loss's learned weights.</li>
<li><strong>reg_weight</strong>: The amount the regularization loss will be multiplied by.</li>
</ul>
<p>Extended by:</p>
<ul>
<li><a href="./#arcfaceloss">ArcFaceLoss</a></li>
<li><a href="./#cosfaceloss">CosFaceLoss</a></li>
<li><a href="./#largemarginsoftmaxloss">LargeMarginSoftmaxLoss</a></li>
<li><a href="./#normalizedsoftmaxloss">NormalizedSoftmaxLoss</a></li>
<li><a href="./#proxyncaloss">ProxyNCALoss</a></li>
<li><a href="./#spherefaceloss">SphereFaceLoss</a></li>
</ul>
                
                  
                
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href=".." title="Home" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Home
              </span>
            </div>
          </a>
        
        
          <a href="../miners/" title="Miners" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Miners
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.808e90bb.js"></script>
      
      <script>app.initialize({version:"1.1.dev0",url:{base:".."}})</script>
      
    
  </body>
</html>

<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../distances/">
      
      
        <link rel="next" href="../miners/">
      
      
      <link rel="icon" href="../imgs/Favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.43">
    
    
      
        <title>Losses - PyTorch Metric Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="red">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#losses" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="PyTorch Metric Learning" class="md-header__button md-logo" aria-label="PyTorch Metric Learning" data-md-component="logo">
      
  <img src="../imgs/TinyLogo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            PyTorch Metric Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Losses
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/KevinMusgrave/pytorch-metric-learning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    KevinMusgrave/pytorch-metric-learning
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="PyTorch Metric Learning" class="md-nav__button md-logo" aria-label="PyTorch Metric Learning" data-md-component="logo">
      
  <img src="../imgs/TinyLogo.png" alt="logo">

    </a>
    PyTorch Metric Learning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/KevinMusgrave/pytorch-metric-learning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    KevinMusgrave/pytorch-metric-learning
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../distances/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distances
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Losses
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Losses
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#angularloss" class="md-nav__link">
    <span class="md-ellipsis">
      AngularLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#arcfaceloss" class="md-nav__link">
    <span class="md-ellipsis">
      ArcFaceLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basemetriclossfunction" class="md-nav__link">
    <span class="md-ellipsis">
      BaseMetricLossFunction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#circleloss" class="md-nav__link">
    <span class="md-ellipsis">
      CircleLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contrastiveloss" class="md-nav__link">
    <span class="md-ellipsis">
      ContrastiveLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cosfaceloss" class="md-nav__link">
    <span class="md-ellipsis">
      CosFaceLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#crossbatchmemory" class="md-nav__link">
    <span class="md-ellipsis">
      CrossBatchMemory
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dynamicsoftmarginloss" class="md-nav__link">
    <span class="md-ellipsis">
      DynamicSoftMarginLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fastaploss" class="md-nav__link">
    <span class="md-ellipsis">
      FastAPLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#genericpairloss" class="md-nav__link">
    <span class="md-ellipsis">
      GenericPairLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generalizedliftedstructureloss" class="md-nav__link">
    <span class="md-ellipsis">
      GeneralizedLiftedStructureLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#instanceloss" class="md-nav__link">
    <span class="md-ellipsis">
      InstanceLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#histogramloss" class="md-nav__link">
    <span class="md-ellipsis">
      HistogramLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intrapairvarianceloss" class="md-nav__link">
    <span class="md-ellipsis">
      IntraPairVarianceLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#largemarginsoftmaxloss" class="md-nav__link">
    <span class="md-ellipsis">
      LargeMarginSoftmaxLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#liftedstructureloss" class="md-nav__link">
    <span class="md-ellipsis">
      LiftedStructureLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#manifoldloss" class="md-nav__link">
    <span class="md-ellipsis">
      ManifoldLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#marginloss" class="md-nav__link">
    <span class="md-ellipsis">
      MarginLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multisimilarityloss" class="md-nav__link">
    <span class="md-ellipsis">
      MultiSimilarityLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multiplelosses" class="md-nav__link">
    <span class="md-ellipsis">
      MultipleLosses
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ncaloss" class="md-nav__link">
    <span class="md-ellipsis">
      NCALoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#normalizedsoftmaxloss" class="md-nav__link">
    <span class="md-ellipsis">
      NormalizedSoftmaxLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#npairsloss" class="md-nav__link">
    <span class="md-ellipsis">
      NPairsLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ntxentloss" class="md-nav__link">
    <span class="md-ellipsis">
      NTXentLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#p2sgradloss" class="md-nav__link">
    <span class="md-ellipsis">
      P2SGradLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pnploss" class="md-nav__link">
    <span class="md-ellipsis">
      PNPLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#proxyanchorloss" class="md-nav__link">
    <span class="md-ellipsis">
      ProxyAnchorLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#proxyncaloss" class="md-nav__link">
    <span class="md-ellipsis">
      ProxyNCALoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rankedlistloss" class="md-nav__link">
    <span class="md-ellipsis">
      RankedListLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#selfsupervisedloss" class="md-nav__link">
    <span class="md-ellipsis">
      SelfSupervisedLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#signaltonoiseratiocontrastiveloss" class="md-nav__link">
    <span class="md-ellipsis">
      SignalToNoiseRatioContrastiveLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#softtripleloss" class="md-nav__link">
    <span class="md-ellipsis">
      SoftTripleLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spherefaceloss" class="md-nav__link">
    <span class="md-ellipsis">
      SphereFaceLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#subcenterarcfaceloss" class="md-nav__link">
    <span class="md-ellipsis">
      SubCenterArcFaceLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#supconloss" class="md-nav__link">
    <span class="md-ellipsis">
      SupConLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#thresholdconsistentmarginloss" class="md-nav__link">
    <span class="md-ellipsis">
      ThresholdConsistentMarginLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tripletmarginloss" class="md-nav__link">
    <span class="md-ellipsis">
      TripletMarginLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tupletmarginloss" class="md-nav__link">
    <span class="md-ellipsis">
      TupletMarginLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#weightregularizermixin" class="md-nav__link">
    <span class="md-ellipsis">
      WeightRegularizerMixin
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vicregloss" class="md-nav__link">
    <span class="md-ellipsis">
      VICRegLoss
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../miners/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Miners
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../reducers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reducers
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../regularizers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Regularizers
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../samplers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Samplers
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../trainers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Trainers
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../testers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Testers
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10" >
        
          
          <label class="md-nav__link" for="__nav_10" id="__nav_10_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10">
            <span class="md-nav__icon md-icon"></span>
            Utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../accuracy_calculation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Accuracy Calculation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../inference_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Inference Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../logging_presets/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Logging Presets
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../common_functions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Common Functions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../distributed/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11" >
        
          
          <label class="md-nav__link" for="__nav_11" id="__nav_11_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    How to extend this library
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11">
            <span class="md-nav__icon md-icon"></span>
            How to extend this library
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../extend/losses/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Custom losses
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../extend/miners/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Custom miners
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Frequently Asked Questions
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#angularloss" class="md-nav__link">
    <span class="md-ellipsis">
      AngularLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#arcfaceloss" class="md-nav__link">
    <span class="md-ellipsis">
      ArcFaceLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basemetriclossfunction" class="md-nav__link">
    <span class="md-ellipsis">
      BaseMetricLossFunction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#circleloss" class="md-nav__link">
    <span class="md-ellipsis">
      CircleLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contrastiveloss" class="md-nav__link">
    <span class="md-ellipsis">
      ContrastiveLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cosfaceloss" class="md-nav__link">
    <span class="md-ellipsis">
      CosFaceLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#crossbatchmemory" class="md-nav__link">
    <span class="md-ellipsis">
      CrossBatchMemory
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dynamicsoftmarginloss" class="md-nav__link">
    <span class="md-ellipsis">
      DynamicSoftMarginLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fastaploss" class="md-nav__link">
    <span class="md-ellipsis">
      FastAPLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#genericpairloss" class="md-nav__link">
    <span class="md-ellipsis">
      GenericPairLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generalizedliftedstructureloss" class="md-nav__link">
    <span class="md-ellipsis">
      GeneralizedLiftedStructureLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#instanceloss" class="md-nav__link">
    <span class="md-ellipsis">
      InstanceLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#histogramloss" class="md-nav__link">
    <span class="md-ellipsis">
      HistogramLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intrapairvarianceloss" class="md-nav__link">
    <span class="md-ellipsis">
      IntraPairVarianceLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#largemarginsoftmaxloss" class="md-nav__link">
    <span class="md-ellipsis">
      LargeMarginSoftmaxLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#liftedstructureloss" class="md-nav__link">
    <span class="md-ellipsis">
      LiftedStructureLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#manifoldloss" class="md-nav__link">
    <span class="md-ellipsis">
      ManifoldLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#marginloss" class="md-nav__link">
    <span class="md-ellipsis">
      MarginLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multisimilarityloss" class="md-nav__link">
    <span class="md-ellipsis">
      MultiSimilarityLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multiplelosses" class="md-nav__link">
    <span class="md-ellipsis">
      MultipleLosses
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ncaloss" class="md-nav__link">
    <span class="md-ellipsis">
      NCALoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#normalizedsoftmaxloss" class="md-nav__link">
    <span class="md-ellipsis">
      NormalizedSoftmaxLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#npairsloss" class="md-nav__link">
    <span class="md-ellipsis">
      NPairsLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ntxentloss" class="md-nav__link">
    <span class="md-ellipsis">
      NTXentLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#p2sgradloss" class="md-nav__link">
    <span class="md-ellipsis">
      P2SGradLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pnploss" class="md-nav__link">
    <span class="md-ellipsis">
      PNPLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#proxyanchorloss" class="md-nav__link">
    <span class="md-ellipsis">
      ProxyAnchorLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#proxyncaloss" class="md-nav__link">
    <span class="md-ellipsis">
      ProxyNCALoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rankedlistloss" class="md-nav__link">
    <span class="md-ellipsis">
      RankedListLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#selfsupervisedloss" class="md-nav__link">
    <span class="md-ellipsis">
      SelfSupervisedLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#signaltonoiseratiocontrastiveloss" class="md-nav__link">
    <span class="md-ellipsis">
      SignalToNoiseRatioContrastiveLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#softtripleloss" class="md-nav__link">
    <span class="md-ellipsis">
      SoftTripleLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spherefaceloss" class="md-nav__link">
    <span class="md-ellipsis">
      SphereFaceLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#subcenterarcfaceloss" class="md-nav__link">
    <span class="md-ellipsis">
      SubCenterArcFaceLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#supconloss" class="md-nav__link">
    <span class="md-ellipsis">
      SupConLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#thresholdconsistentmarginloss" class="md-nav__link">
    <span class="md-ellipsis">
      ThresholdConsistentMarginLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tripletmarginloss" class="md-nav__link">
    <span class="md-ellipsis">
      TripletMarginLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tupletmarginloss" class="md-nav__link">
    <span class="md-ellipsis">
      TupletMarginLoss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#weightregularizermixin" class="md-nav__link">
    <span class="md-ellipsis">
      WeightRegularizerMixin
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vicregloss" class="md-nav__link">
    <span class="md-ellipsis">
      VICRegLoss
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="losses">Losses<a class="headerlink" href="#losses" title="Permanent link">&para;</a></h1>
<p>All loss functions are used as follows:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">pytorch_metric_learning</span> <span class="kn">import</span> <span class="n">losses</span>
<span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SomeLoss</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="c1"># in your training for-loop</span>
</code></pre></div>
<p>Or if you are using a loss in conjunction with a <a href="../miners/">miner</a>:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">pytorch_metric_learning</span> <span class="kn">import</span> <span class="n">miners</span>
<span class="n">miner_func</span> <span class="o">=</span> <span class="n">miners</span><span class="o">.</span><span class="n">SomeMiner</span><span class="p">()</span>
<span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SomeLoss</span><span class="p">()</span>
<span class="n">miner_output</span> <span class="o">=</span> <span class="n">miner_func</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="c1"># in your training for-loop</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">miner_output</span><span class="p">)</span>
</code></pre></div>
<p>For some losses, you don't need to pass in labels if you are already passing in pair/triplet indices:
<div class="highlight"><pre><span></span><code><span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">indices_tuple</span><span class="o">=</span><span class="n">pairs</span><span class="p">)</span>
<span class="c1"># it also works with ref_emb</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">indices_tuple</span><span class="o">=</span><span class="n">pairs</span><span class="p">,</span> <span class="n">ref_emb</span><span class="o">=</span><span class="n">ref_emb</span><span class="p">)</span>
</code></pre></div></p>
<details class="note">
<summary>Losses for which you can pass in <code>indices_tuple</code> without <code>labels</code></summary>
<ul>
<li>CircleLoss</li>
<li>ContrastiveLoss</li>
<li>IntraPairVarianceLoss</li>
<li>GeneralizedLiftedStructureLoss</li>
<li>LiftedStructureLoss</li>
<li>MarginLoss</li>
<li>MultiSimilarityLoss</li>
<li>NTXentLoss</li>
<li>SignalToNoiseRatioContrastiveLoss</li>
<li>SupConLoss</li>
<li>TripletMarginLoss</li>
<li>TupletMarginLoss</li>
</ul>
</details>
<p>You can specify how losses get reduced to a single value by using a <a href="../reducers/">reducer</a>:
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">pytorch_metric_learning</span> <span class="kn">import</span> <span class="n">reducers</span>
<span class="n">reducer</span> <span class="o">=</span> <span class="n">reducers</span><span class="o">.</span><span class="n">SomeReducer</span><span class="p">()</span>
<span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SomeLoss</span><span class="p">(</span><span class="n">reducer</span><span class="o">=</span><span class="n">reducer</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="c1"># in your training for-loop</span>
</code></pre></div></p>
<p>For tuple losses, you can separate the source of anchors and positives/negatives:
<div class="highlight"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SomeLoss</span><span class="p">()</span>
<span class="c1"># anchors will come from embeddings</span>
<span class="c1"># positives/negatives will come from ref_emb</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">ref_emb</span><span class="o">=</span><span class="n">ref_emb</span><span class="p">,</span> <span class="n">ref_labels</span><span class="o">=</span><span class="n">ref_labels</span><span class="p">)</span>
</code></pre></div></p>
<p>For classification losses, you can get logits using the <code>get_logits</code> function:
<div class="highlight"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SomeClassificationLoss</span><span class="p">()</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">loss_func</span><span class="o">.</span><span class="n">get_logits</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
</code></pre></div></p>
<h2 id="angularloss">AngularLoss<a class="headerlink" href="#angularloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1708.01682.pdf" target="_blank">Deep Metric Learning with Angular Loss</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">AngularLoss</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<strong>Equation</strong>:</p>
<p><img alt="angular_loss_equation" src="../imgs/angular_loss_equation.png" style="height:200px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>alpha</strong>: The angle specified in degrees. The paper uses values between 36 and 55.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li>
<p><a href="../distances/#lpdistance"><code>LpDistance(p=2, power=1, normalize_embeddings=True)</code></a></p>
<ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss for every <code>a1</code>, where <code>(a1,p)</code> represents every positive pair in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="arcfaceloss">ArcFaceLoss<a class="headerlink" href="#arcfaceloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1801.07698.pdf" target="_blank">ArcFace: Additive Angular Margin Loss for Deep Face Recognition</a></p>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">ArcFaceLoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">28.6</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<p><strong>Equation</strong>:</p>
<p><img alt="arcface_loss_equation" src="../imgs/arcface_loss_equation.png" style="height:80px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>margin</strong>: The angular margin penalty in degrees. In the above equation, <code>m = radians(margin)</code>. The paper uses 0.5 radians, which is 28.6 degrees.</li>
<li><strong>scale</strong>: This is <code>s</code> in the above equation. The paper uses 64.</li>
</ul>
<p><strong>Other info</strong>: </p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts <code>weight_regularizer</code>, <code>weight_reg_weight</code>, and <code>weight_init_func</code> as optional arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:
<div class="highlight"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">ArcFaceLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a><ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="basemetriclossfunction">BaseMetricLossFunction<a class="headerlink" href="#basemetriclossfunction" title="Permanent link">&para;</a></h2>
<p>All loss functions extend this class and therefore inherit its <code>__init__</code> parameters.</p>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">BaseMetricLossFunction</span><span class="p">(</span><span class="n">collect_stats</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
                            <span class="n">reducer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                            <span class="n">distance</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                            <span class="n">embedding_regularizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                            <span class="n">embedding_reg_weight</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>collect_stats</strong>: If True, will collect various statistics that may be useful to analyze during experiments. If False, these computations will be skipped. Want to make <code>True</code> the default? Set the global <a href="../common_functions/#collect_stats">COLLECT_STATS</a> flag.</li>
<li><strong>reducer</strong>: A <a href="../reducers/">reducer</a> object. If None, then the default reducer will be used.</li>
<li><strong>distance</strong>: A <a href="../distances/">distance</a> object. If None, then the default distance will be used.</li>
<li><strong>embedding_regularizer</strong>: A <a href="../regularizers/">regularizer</a> object that will be applied to embeddings. If None, then no embedding regularization will be used.</li>
<li><strong>embedding_reg_weight</strong>: If an embedding regularizer is used, then its loss will be multiplied by this amount before being added to the total loss.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=1)</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>embedding_reg_loss</strong>: Only exists if an embedding regularizer is used. It contains the loss per element in the batch. Reduction type is <code>"already_reduced"</code>. </li>
</ul>
<p><strong>Required Implementations</strong>:
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">indices_tuple</span><span class="p">,</span> <span class="n">ref_emb</span><span class="p">,</span> <span class="n">ref_labels</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></p>
<h2 id="circleloss">CircleLoss<a class="headerlink" href="#circleloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/2002.10857.pdf" target="_blank">Circle Loss: A Unified Perspective of Pair Similarity Optimization</a></p>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">CircleLoss</span><span class="p">(</span><span class="n">m</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<p><strong>Equations</strong>:</p>
<p><img alt="circle_loss_equation1" src="../imgs/circle_loss_equation1.png" style="height:60px" /></p>
<p>where</p>
<p><img alt="circle_loss_equation2" src="../imgs/circle_loss_equation2.png" style="height:70px" /></p>
<p><img alt="circle_loss_equation3" src="../imgs/circle_loss_equation3.png" style="height:25px" /></p>
<p><img alt="circle_loss_equation4" src="../imgs/circle_loss_equation4.png" style="height:25px" /></p>
<p><img alt="circle_loss_equation5" src="../imgs/circle_loss_equation5.png" style="height:25px" /></p>
<p><img alt="circle_loss_equation6" src="../imgs/circle_loss_equation6.png" style="height:25px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>m</strong>: The relaxation factor that controls the radius of the decision boundary. The paper uses 0.25 for face recognition, and 0.4 for fine-grained image retrieval (images of birds, cars, and online products).</li>
<li><strong>gamma</strong>: The scale factor that determines the largest scale of each similarity score. The paper uses 256 for face recognition, and 80 for fine-grained image retrieval.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li>
<p><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></p>
<ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#avgnonzeroreducer">AvgNonZeroReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="contrastiveloss">ContrastiveLoss<a class="headerlink" href="#contrastiveloss" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">ContrastiveLoss</span><span class="p">(</span><span class="n">pos_margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">neg_margin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</code></pre></div>
<p><strong>Equation</strong>:</p>
<p>If using a distance metric like <a href="../distances/#lpdistance">LpDistance</a>, the loss is:</p>
<p><img alt="contrastive_loss_equation" src="../imgs/contrastive_loss_equation.png" style="height:37px" /></p>
<p>If using a similarity metric like <a href="../distances/#cosinesimilarity">CosineSimilarity</a>, the loss is:</p>
<p><img alt="contrastive_loss_similarity_equation" src="../imgs/contrastive_loss_similarity_equation.png" style="height:35px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>pos_margin</strong>: The distance (or similarity) over (under) which positive pairs will contribute to the loss.</li>
<li><strong>neg_margin</strong>: The distance (or similarity) under (over) which negative pairs will contribute to the loss.  </li>
</ul>
<p>Note that the default values for <code>pos_margin</code> and <code>neg_margin</code> are suitable if you are using a non-inverted distance measure, like <a href="../distances/#lpdistance">LpDistance</a>. If you use an inverted distance measure like <a href="../distances/#cosinesimilarity">CosineSimilarity</a>, then more appropriate values would be <code>pos_margin = 1</code> and <code>neg_margin = 0</code>.</p>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=1)</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#avgnonzeroreducer">AvgNonZeroReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>pos_loss</strong>: The loss per positive pair in the batch. Reduction type is <code>"pos_pair"</code>.</li>
<li><strong>neg_loss</strong>: The loss per negative pair in the batch. Reduction type is <code>"neg_pair"</code>.</li>
</ul>
<h2 id="cosfaceloss">CosFaceLoss<a class="headerlink" href="#cosfaceloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1801.09414.pdf" target="_blank">CosFace: Large Margin Cosine Loss for Deep Face Recognition</a></p>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">CosFaceLoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">0.35</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<p><strong>Equation</strong>:</p>
<p><img alt="cosface_loss_equation" src="../imgs/cosface_loss_equation.png" style="height:80px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>margin</strong>: The cosine margin penalty (m in the above equation). The paper used values between 0.25 and 0.45.</li>
<li><strong>scale</strong>: This is <code>s</code> in the above equation. The paper uses 64.</li>
</ul>
<p><strong>Other info</strong>: </p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts <code>weight_regularizer</code>, <code>weight_reg_weight</code>, and <code>weight_init_func</code> as optional arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:
<div class="highlight"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">CosFaceLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li>
<p><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></p>
<ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="crossbatchmemory">CrossBatchMemory<a class="headerlink" href="#crossbatchmemory" title="Permanent link">&para;</a></h2>
<p>This wraps a loss function, and implements <a href="https://arxiv.org/pdf/1912.06798.pdf" target="_blank">Cross-Batch Memory for Embedding Learning</a>. It stores embeddings from previous iterations in a queue, and uses them to form more pairs/triplets with the current iteration's embeddings.</p>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">CrossBatchMemory</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">memory_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">miner</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss function to be wrapped. For example, you could pass in <code>ContrastiveLoss()</code>.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>memory_size</strong>: The size of the memory queue.</li>
<li><strong>miner</strong>: An optional <a href="../miners/">tuple miner</a>, which will be used to mine pairs/triplets from the memory queue.</li>
</ul>
<p><strong>Forward function</strong>
<div class="highlight"><pre><span></span><code><span class="n">loss_fn</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">indices_tuple</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">enqueue_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div></p>
<p>As shown above, CrossBatchMemory comes with a 4th argument in its <code>forward</code> function:</p>
<ul>
<li><strong>enqueue_mask</strong>: A boolean tensor where <code>enqueue_mask[i]</code> is True if <code>embeddings[i]</code> should be added to the memory queue. This enables CrossBatchMemory to be used in self-supervision frameworks like <a href="https://arxiv.org/pdf/1911.05722.pdf">MoCo</a>. Check out the <a href="https://github.com/KevinMusgrave/pytorch-metric-learning/tree/master/examples#simple-examples">MoCo on CIFAR100</a> notebook to see how this works.</li>
</ul>
<details class="note">
<summary>Supported Loss Functions</summary>
<ul>
<li><a href="./#angularloss">AngularLoss</a></li>
<li><a href="./#circleloss">CircleLoss</a></li>
<li><a href="./#contrastiveloss">ContrastiveLoss</a></li>
<li><a href="./#generalizedliftedstructureloss">GeneralizedLiftedStructureLoss</a></li>
<li><a href="./#intrapairvarianceloss">IntraPairVarianceLoss</a></li>
<li><a href="./#liftedstructureloss">LiftedStructureLoss</a></li>
<li><a href="./#marginloss">MarginLoss</a></li>
<li><a href="./#multisimilarityloss">MultiSimilarityLoss</a></li>
<li><a href="./#ncaloss">NCALoss</a></li>
<li><a href="./#ntxentloss">NTXentLoss</a></li>
<li><a href="./#signaltonoiseratiocontrastiveloss">SignalToNoiseRatioContrastiveLoss</a></li>
<li><a href="./#supconloss">SupConLoss</a></li>
<li><a href="./#tripletmarginloss">TripletMarginLoss</a></li>
<li><a href="./#tupletmarginloss">TupletMarginLoss</a></li>
</ul>
</details>
<p><strong>Reset queue</strong></p>
<p>The queue can be cleared like this:
<div class="highlight"><pre><span></span><code><span class="n">loss_fn</span><span class="o">.</span><span class="n">reset_queue</span><span class="p">()</span>
</code></pre></div></p>
<h2 id="dynamicsoftmarginloss">DynamicSoftMarginLoss<a class="headerlink" href="#dynamicsoftmarginloss" title="Permanent link">&para;</a></h2>
<p><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Learning_Local_Descriptors_With_a_CDF-Based_Dynamic_Soft_Margin_ICCV_2019_paper.pdf">Learning Local Descriptors With a CDF-Based Dynamic Soft Margin</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">DynamicSoftMarginLoss</span><span class="p">(</span><span class="n">min_val</span><span class="o">=-</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">num_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>min_val</strong>: minimum significative value for <code>d_pos - d_neg</code></li>
<li><strong>num_bins</strong>: number of equally spaced bins for the partition of the interval <code>[min_val, ]</code></li>
<li><strong>momentum</strong>: weight assigned to the histogram computed from the current batch</li>
</ul>
<h2 id="fastaploss">FastAPLoss<a class="headerlink" href="#fastaploss" title="Permanent link">&para;</a></h2>
<p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Cakir_Deep_Metric_Learning_to_Rank_CVPR_2019_paper.pdf" target="_blank">Deep Metric Learning to Rank</a></p>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">FastAPLoss</span><span class="p">(</span><span class="n">num_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>num_bins</strong>: The number of soft histogram bins for calculating average precision. The paper suggests using 10.</li>
</ul>
<p><strong>Default distance</strong>:</p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=2)</code></a><ul>
<li>The only compatible distance is <code>LpDistance(normalize_embeddings=True, p=2)</code>. However, the <code>power</code> value can be changed.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element that has at least 1 positive in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="genericpairloss">GenericPairLoss<a class="headerlink" href="#genericpairloss" title="Permanent link">&para;</a></h2>
<p><div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">GenericPairLoss</span><span class="p">(</span><span class="n">mat_based_loss</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<strong>Parameters</strong>:</p>
<ul>
<li><strong>mat_based_loss</strong>: See required implementations.</li>
</ul>
<p><strong>Required Implementations</strong>:
<div class="highlight"><pre><span></span><code><span class="c1"># If mat_based_loss is True, then this takes in mat, pos_mask, neg_mask</span>
<span class="c1"># If False, this takes in pos_pair, neg_pair, indices_tuple</span>
<span class="k">def</span> <span class="nf">_compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></p>
<h2 id="generalizedliftedstructureloss">GeneralizedLiftedStructureLoss<a class="headerlink" href="#generalizedliftedstructureloss" title="Permanent link">&para;</a></h2>
<p>This was presented in <a href="https://arxiv.org/pdf/1703.07737.pdf" target="_blank">In Defense of the Triplet Loss for Person Re-Identification</a>. It is a modification of the original <a href="./#liftedstructureloss">LiftedStructureLoss</a></p>
<p><div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">GeneralizedLiftedStructureLoss</span><span class="p">(</span><span class="n">neg_margin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pos_margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<strong>Equation</strong>:</p>
<p><img alt="generalized_lifted_structure_loss_equation" src="../imgs/generalized_lifted_structure_loss_equation.png" style="height:250px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>pos_margin</strong>: The margin in the expression <code>e^(D - margin)</code>. The paper uses <code>pos_margin = 0</code>, which is why this margin does not appear in the above equation.</li>
<li><strong>neg_margin</strong>: This is <code>m</code> in the above equation. The paper used values between 0.1 and 1.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=1)</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="instanceloss">InstanceLoss<a class="headerlink" href="#instanceloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1711.05535.pdf">Dual-Path Convolutional Image-Text Embeddings with Instance Loss</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">InstanceLoss</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>gamma</strong>: The cosine similarity matrix is scaled by this amount.</li>
</ul>
<h2 id="histogramloss">HistogramLoss<a class="headerlink" href="#histogramloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1611.00822.pdf">Learning Deep Embeddings with Histogram Loss</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">HistogramLoss</span><span class="p">(</span><span class="n">n_bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>n_bins</strong>: The number of bins used to construct the histogram. Default is 100 when both <code>n_bins</code> and <code>delta</code> are <code>None</code>.</li>
<li><strong>delta</strong>: The mesh of the uniform partition of the interval [-1, 1] used to construct the histogram. If not set the value of n_bins will be used.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li>This loss returns an <strong>already reduced</strong> loss.</li>
</ul>
<h2 id="intrapairvarianceloss">IntraPairVarianceLoss<a class="headerlink" href="#intrapairvarianceloss" title="Permanent link">&para;</a></h2>
<p><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Deep_Metric_Learning_With_Tuplet_Margin_Loss_ICCV_2019_paper.pdf" target="_blank">Deep Metric Learning with Tuplet Margin Loss</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">IntraPairVarianceLoss</span><span class="p">(</span><span class="n">pos_eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">neg_eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Equations</strong>:</p>
<p><img alt="intra_pair_variance_loss_equation1" src="../imgs/intra_pair_variance_loss_equation1.png" style="height:39px" /></p>
<p><img alt="intra_pair_variance_loss_equation2" src="../imgs/intra_pair_variance_loss_equation2.png" style="height:34px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>pos_eps</strong>: The epsilon in the L<sub>pos</sub> equation. The paper uses 0.01.</li>
<li><strong>neg_eps</strong>: The epsilon in the L<sub>neg</sub> equation. The paper uses 0.01.</li>
</ul>
<p>You should probably use this in conjunction with another loss, as described in the paper. You can accomplish this by using <a href="./#multiplelosses">MultipleLosses</a>:
<div class="highlight"><pre><span></span><code><span class="n">main_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">TupletMarginLoss</span><span class="p">()</span>
<span class="n">var_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">IntraPairVarianceLoss</span><span class="p">()</span>
<span class="n">complete_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">MultipleLosses</span><span class="p">([</span><span class="n">main_loss</span><span class="p">,</span> <span class="n">var_loss</span><span class="p">],</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
</code></pre></div></p>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=1)</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>pos_loss</strong>: The loss per positive pair in the batch. Reduction type is <code>"pos_pair"</code>.</li>
<li><strong>neg_loss</strong>: The loss per negative pair in the batch. Reduction type is <code>"neg_pair"</code>.</li>
</ul>
<h2 id="largemarginsoftmaxloss">LargeMarginSoftmaxLoss<a class="headerlink" href="#largemarginsoftmaxloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1612.02295.pdf" target="_blank">Large-Margin Softmax Loss for Convolutional Neural Networks</a></p>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">LargeMarginSoftmaxLoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> 
                            <span class="n">embedding_size</span><span class="p">,</span> 
                            <span class="n">margin</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> 
                            <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                            <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<p><strong>Equations</strong>:</p>
<p><img alt="large_margin_softmax_loss_equation1" src="../imgs/large_margin_softmax_loss_equation1.png" style="height:80px" /></p>
<p>where</p>
<p><img alt="large_margin_softmax_loss_equation2" src="../imgs/large_margin_softmax_loss_equation2.png" style="height:90px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>margin</strong>: An integer which dictates the size of the angular margin. This is <code>m</code> in the above equation. The paper finds <code>m=4</code> works best.</li>
<li><strong>scale</strong>: The exponent multiplier in the loss's softmax expression. The paper uses <code>scale = 1</code>, which is why it does not appear in the above equation.</li>
</ul>
<p><strong>Other info</strong>: </p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts <code>weight_regularizer</code>, <code>weight_reg_weight</code>, and <code>weight_init_func</code> as optional arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:
<div class="highlight"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">LargeMarginSoftmaxLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li>
<p><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></p>
<ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="liftedstructureloss">LiftedStructureLoss<a class="headerlink" href="#liftedstructureloss" title="Permanent link">&para;</a></h2>
<p>The original lifted structure loss as presented in <a href="https://arxiv.org/pdf/1511.06452.pdf" target="_blank">Deep Metric Learning via Lifted Structured Feature Embedding</a></p>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">LiftedStructureLoss</span><span class="p">(</span><span class="n">neg_margin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pos_margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</code></pre></div>
<p><strong>Equation</strong>:</p>
<p><img alt="lifted_structure_loss_equation" src="../imgs/lifted_structure_loss_equation.png" style="height:150px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>pos_margin</strong>: The margin in the expression <code>D_(i,j) - margin</code>. The paper uses <code>pos_margin = 0</code>, which is why it does not appear in the above equation.</li>
<li><strong>neg_margin</strong>: This is <code>alpha</code> in the above equation. The paper uses 1.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=1)</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per positive pair in the batch. Reduction type is <code>"pos_pair"</code>.</li>
</ul>
<h2 id="manifoldloss">ManifoldLoss<a class="headerlink" href="#manifoldloss" title="Permanent link">&para;</a></h2>
<p><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Aziere_Ensemble_Deep_Manifold_Similarity_Learning_Using_Hard_Proxies_CVPR_2019_paper.pdf">Ensemble Deep Manifold Similarity Learning using Hard Proxies</a></p>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">ManifoldLoss</span><span class="p">(</span>
        <span class="n">l</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">K</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
        <span class="n">lambdaC</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span>
        <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span>
</code></pre></div>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><strong>l</strong>: embedding size.</p>
</li>
<li>
<p><strong>K</strong>: number of proxies.</p>
</li>
<li>
<p><strong>lambdaC</strong>: regularization weight. Used in the formula <code>loss = intrinsic_loss + lambdaC*context_loss</code>.
    If <code>lambdaC=0</code>, then it uses only the intrinsic loss. If <code>lambdaC=np.inf</code>, then it uses only the context loss.</p>
</li>
<li>
<p><strong>alpha</strong>: parameter of the Random Walk. Must be in the range <code>(0,1)</code>. It specifies the amount of similarity between neighboring nodes.</p>
</li>
<li>
<p><strong>margin</strong>: margin used in the calculation of the loss.</p>
</li>
</ul>
<p>Example usage:
<div class="highlight"><pre><span></span><code><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">ManifoldLoss</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>

<span class="c1"># use random cluster centers</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
<span class="c1"># or specify indices of embeddings to use as cluster centers</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">indices_tuple</span><span class="o">=</span><span class="n">indices</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Important notes</strong></p>
<p><code>labels</code>, <code>ref_emb</code>, and <code>ref_labels</code> are not supported for this loss function.</p>
<p>In addition, <code>indices_tuple</code> is <strong>not</strong> for the output of miners. Instead, it is for a list of indices of embeddings to be used as cluster centers.</p>
<p><strong>Default reducer</strong>: </p>
<ul>
<li>This loss returns an <strong>already reduced</strong> loss.</li>
</ul>
<h2 id="marginloss">MarginLoss<a class="headerlink" href="#marginloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1706.07567.pdf" target="_blank">Sampling Matters in Deep Embedding Learning</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">MarginLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> 
                <span class="n">nu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> 
                <span class="n">beta</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> 
                <span class="n">triplets_per_anchor</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> 
                <span class="n">learn_beta</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                <span class="n">num_classes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Equations</strong>:</p>
<p><img alt="margin_loss_equation2" src="../imgs/margin_loss_equation2.png" style="height:60px" /></p>
<p>where</p>
<p><img alt="margin_loss_equation1" src="../imgs/margin_loss_equation1.png" style="height:40px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>margin</strong>: This is alpha in the above equation. The paper uses 0.2.</li>
<li><strong>nu</strong>: The regularization weight for the magnitude of beta.</li>
<li><strong>beta</strong>: This is beta in the above equation. The paper uses 1.2 as the initial value.</li>
<li><strong>triplets_per_anchor</strong>: The number of triplets per element to sample within a batch. Can be an integer or the string "all". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is "all", then all possible triplets in the batch will be used.</li>
<li><strong>learn_beta</strong>: If True, beta will be a torch.nn.Parameter, which can be optimized using any PyTorch optimizer.</li>
<li><strong>num_classes</strong>: If not None, then beta will be of size <code>num_classes</code>, so that a separate beta is used for each class during training.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=1)</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#divisorreducer">DivisorReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>margin_loss</strong>: The loss per triplet in the batch. Reduction type is <code>"triplet"</code>.</li>
<li><strong>beta_reg_loss</strong>: The regularization loss per element in <code>self.beta</code>. Reduction type is <code>"already_reduced"</code> if <code>self.num_classes = None</code>. Otherwise it is <code>"element"</code>.</li>
</ul>
<h2 id="multisimilarityloss">MultiSimilarityLoss<a class="headerlink" href="#multisimilarityloss" title="Permanent link">&para;</a></h2>
<p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Multi-Similarity_Loss_With_General_Pair_Weighting_for_Deep_Metric_Learning_CVPR_2019_paper.pdf" target="_blank">Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">MultiSimilarityLoss</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Equation</strong>:</p>
<p><img alt="multi_similarity_loss_equation" src="../imgs/multi_similarity_loss_equation.png" style="height:150px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>alpha</strong>: The weight applied to positive pairs. The paper uses 2.</li>
<li><strong>beta</strong>: The weight applied to negative pairs. The paper uses 50.</li>
<li><strong>base</strong>: The offset applied to the exponent in the loss. This is lambda in the above equation. The paper uses 1. </li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="multiplelosses">MultipleLosses<a class="headerlink" href="#multiplelosses" title="Permanent link">&para;</a></h2>
<p>This is a simple wrapper for multiple losses. Pass in a list of already-initialized loss functions. Then, when you call forward on this object, it will return the sum of all wrapped losses.
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">MultipleLosses</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">miners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
<strong>Parameters</strong>:</p>
<ul>
<li><strong>losses</strong>: A list or dictionary of initialized loss functions. On the forward call of MultipleLosses, each wrapped loss will be computed, and then the average will be returned.</li>
<li><strong>miners</strong>: Optional. A list or dictionary of mining functions. This allows you to pair mining functions with loss functions. For example, if <code>losses = [loss_A, loss_B]</code>, and <code>miners = [None, miner_B]</code> then no mining will be done for <code>loss_A</code>, but the output of <code>miner_B</code> will be passed to <code>loss_B</code>. The same logic applies if <code>losses = {"loss_A": loss_A, "loss_B": loss_B}</code> and <code>miners = {"loss_B": miner_B}</code>.</li>
<li><strong>weights</strong>: Optional. A list or dictionary of loss weights, which will be multiplied by the corresponding losses obtained by the loss functions. The default is to multiply each loss by 1. If <code>losses</code> is a list, then <code>weights</code> must be a list. If <code>losses</code> is a dictionary, <code>weights</code> must contain the same keys as <code>losses</code>. </li>
</ul>
<h2 id="ncaloss">NCALoss<a class="headerlink" href="#ncaloss" title="Permanent link">&para;</a></h2>
<p><a href="https://www.cs.toronto.edu/~hinton/absps/nca.pdf" target="_blank">Neighbourhood Components Analysis</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">NCALoss</span><span class="p">(</span><span class="n">softmax_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Equations</strong>:</p>
<p><img alt="nca_loss_equation1" src="../imgs/nca_loss_equation1.png" style="height:50px" /></p>
<p>where</p>
<p><img alt="nca_loss_equation2" src="../imgs/nca_loss_equation2.png" style="height:60px" /></p>
<p><img alt="nca_loss_equation3" src="../imgs/nca_loss_equation3.png" style="height:60px" /></p>
<p>In this implementation, we use <code>-g(A)</code> as the loss.</p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>softmax_scale</strong>: The exponent multiplier in the loss's softmax expression. The paper uses <code>softmax_scale = 1</code>, which is why it does not appear in the above equations.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=2)</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch, that results in a non zero exponent in the cross entropy expression. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="normalizedsoftmaxloss">NormalizedSoftmaxLoss<a class="headerlink" href="#normalizedsoftmaxloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1811.12649.pdf" target="_blank">Classification is a Strong Baseline for Deep Metric Learning</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">NormalizedSoftmaxLoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Equation</strong>:</p>
<p><img alt="normalized_softmax_loss_equation" src="../imgs/normalized_softmax_loss_equation.png" style="height:80px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>temperature</strong>: This is sigma in the above equation. The paper uses 0.05.</li>
</ul>
<p><strong>Other info</strong></p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts <code>weight_regularizer</code>, <code>weight_reg_weight</code>, and <code>weight_init_func</code> as optional arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:
<div class="highlight"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">NormalizedSoftmaxLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#dotproductsimilarity"><code>DotProductSimilarity()</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="npairsloss">NPairsLoss<a class="headerlink" href="#npairsloss" title="Permanent link">&para;</a></h2>
<p><a href="http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf" target="_blank">Improved Deep Metric Learning with Multi-class N-pair Loss Objective</a></p>
<p>If your batch has more than 2 samples per label, then you should use <a href="#ntxentloss">NTXentLoss</a>.</p>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">NPairsLoss</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#dotproductsimilarity"><code>DotProductSimilarity()</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="ntxentloss">NTXentLoss<a class="headerlink" href="#ntxentloss" title="Permanent link">&para;</a></h2>
<p>This is also known as InfoNCE, and is a generalization of the <a href="./#npairsloss">NPairsLoss</a>. It has been used in self-supervision papers such as: </p>
<ul>
<li><a href="https://arxiv.org/pdf/1807.03748.pdf" target="_blank">Representation Learning with Contrastive Predictive Coding</a></li>
<li><a href="https://arxiv.org/pdf/1911.05722.pdf" target="_blank">Momentum Contrast for Unsupervised Visual Representation Learning</a></li>
<li><a href="https://arxiv.org/pdf/2002.05709.pdf" target="_blank">A Simple Framework for Contrastive Learning of Visual Representations</a></li>
</ul>
<details>
<summary>How exactly is the NTXentLoss computed?</summary>
<p>In the equation below, a loss is computed for each positive pair (<code>k_+</code>) in a batch, normalized by itself and all negative pairs in the batch that have the same "anchor" embedding (<code>k_i in K</code>). </p>
<ul>
<li>What does "anchor" mean? Let's say we have 3 pairs specified by batch indices: (0, 1), (0, 2), (1, 0). The first two pairs start with 0, so they have the same anchor. The third pair has the same indices as the first pair, but the order is different, so it does not have the same anchor.</li>
</ul>
<p>Given <code>embeddings</code> with corresponding <code>labels</code>, positive pairs <code>(embeddings[i], embeddings[j])</code> are defined when <code>labels[i] == labels[j]</code>. Now let's look at an example loss calculation:</p>
<p>Consider <code>labels = [0, 0, 1, 2]</code>. Two losses will be computed:</p>
<ul>
<li>
<p>A positive pair of indices <code>[0, 1]</code>, with negative pairs of indices <code>[0, 2], [0, 3]</code>.</p>
</li>
<li>
<p>A positive pair of indices <code>[1, 0]</code>, with negative pairs of indices <code>[1, 2], [1, 3]</code>.</p>
</li>
</ul>
<p>Labels <code>1</code>, and <code>2</code> do not have positive pairs, and therefore the negative pair of indices <code>[2, 3]</code> will not be used.</p>
<p>Note that an anchor can belong to multiple positive pairs if its label is present multiple times in <code>labels</code>.</p>
<p>Are you trying to use <code>NTXentLoss</code> for self-supervised learning? Specifically, do you have two sets of embeddings which are derived from data that are augmented versions of each other? If so, you can skip the step of creating the <code>labels</code> array, by wrapping <code>NTXentLoss</code> with <a href="./#selfsupervisedloss"><code>SelfSupervisedLoss</code></a>.</p>
</details>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">NTXentLoss</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.07</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<p><strong>Equation</strong>:</p>
<p><img alt="ntxent_loss_equation" src="../imgs/ntxent_loss_equation.png" style="height:70px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>temperature</strong>: This is tau in the above equation. The MoCo paper uses 0.07, while SimCLR uses 0.5.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per positive pair in the batch. Reduction type is <code>"pos_pair"</code>.</li>
</ul>
<h2 id="p2sgradloss">P2SGradLoss<a class="headerlink" href="#p2sgradloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/abs/1905.02479">P2SGrad: Refined Gradients for Optimizing Deep Face Models</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">P2SGradLoss</span><span class="p">(</span><span class="n">descriptors_dim</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><strong>descriptors_dim</strong>: The embedding size.</p>
</li>
<li>
<p><strong>num_classes</strong>: The number of classes in your training dataset.</p>
</li>
</ul>
<p>Example usage:
<div class="highlight"><pre><span></span><code><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">P2SGradLoss</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Important notes</strong></p>
<p><code>indices_tuple</code>, <code>ref_emb</code>, and <code>ref_labels</code> are not supported for this loss function.</p>
<p><strong>Default reducer</strong>: </p>
<ul>
<li>This loss returns an <strong>already reduced</strong> loss.</li>
</ul>
<h2 id="pnploss">PNPLoss<a class="headerlink" href="#pnploss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/2102.04640.pdf" target="_blank">Rethinking the Optimization of Average Precision: Only Penalizing Negative Instances before Positive Ones is Enough</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">PNPLoss</span><span class="p">(</span><span class="n">b</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">anneal</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">variant</span><span class="o">=</span><span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<strong>Equation</strong>:</p>
<p><img alt="PNP_loss_equation" src="../imgs/PNP_loss_equation.png" style="height:300px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>b</strong>: The boundary of PNP-Ib (see equation 9 above). The paper uses 2.</li>
<li><strong>alpha</strong>: The power of PNP-Dq (see equation 13 above). The paper uses 8.</li>
<li><strong>anneal</strong>: The temperature of the sigmoid function. (The sigmoid function is used for <code>R</code> in the equations above.) The paper uses 0.01.</li>
<li><strong>variant</strong>: The name of the variant. The options are {"Ds", "Dq", "Iu", "Ib", "O"}. The paper uses "Dq".</li>
</ul>
<p><strong>Default distance</strong>:</p>
<ul>
<li><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a><ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element that has at least 1 positive in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="proxyanchorloss">ProxyAnchorLoss<a class="headerlink" href="#proxyanchorloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/2003.13911.pdf" target="_blank">Proxy Anchor Loss for Deep Metric Learning</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">ProxyAnchorLoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">margin</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Equation</strong>:</p>
<p><img alt="proxy_anchor_loss_equation" src="../imgs/proxy_anchor_loss_equation.png" style="height:150px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>margin</strong>: This is delta in the above equation. The paper uses 0.1.</li>
<li><strong>alpha</strong>: This is alpha in the above equation. The paper uses 32.</li>
</ul>
<p><strong>Other info</strong></p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts <code>weight_regularizer</code>, <code>weight_reg_weight</code>, and <code>weight_init_func</code> as optional arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:
<div class="highlight"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">ProxyAnchorLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#divisorreducer">DivisorReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>pos_loss</strong>: The positive pair loss per proxy. Reduction type is <code>"element"</code>.</li>
<li><strong>neg_loss</strong>: The negative pair loss per proxy. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="proxyncaloss">ProxyNCALoss<a class="headerlink" href="#proxyncaloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1703.07464.pdf" target="_blank">No Fuss Distance Metric Learning using Proxies</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">ProxyNCALoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">softmax_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>softmax_scale</strong>: See <a href="./#ncaloss">NCALoss</a></li>
</ul>
<p><strong>Other info</strong></p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts <code>weight_regularizer</code>, <code>weight_reg_weight</code>, and <code>weight_init_func</code> as optional arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:
<div class="highlight"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">ProxyNCALoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=2)</code></a></li>
</ul>
<p><strong>Default reducer</strong>:</p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch, that results in a non zero exponent in the cross entropy expression. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="rankedlistloss">RankedListLoss<a class="headerlink" href="#rankedlistloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/abs/1903.03238">Ranked List Loss for Deep Metric Learning</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">RankedListLoss</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="n">Tn</span><span class="p">,</span> <span class="n">imbalance</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tp</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>margin</strong> (float): margin between positive and negative set</li>
<li><strong>imbalance</strong> (float): tradeoff between positive and negative sets. As the name suggests this takes into account
                    the imbalance between positive and negative samples in the dataset</li>
<li><strong>alpha</strong> (float): smallest distance between negative points</li>
<li><strong>Tp &amp; Tn</strong> (float): temperatures for, respectively, positive and negative pairs weighting.</li>
</ul>
<h2 id="selfsupervisedloss">SelfSupervisedLoss<a class="headerlink" href="#selfsupervisedloss" title="Permanent link">&para;</a></h2>
<p>A common use case is to have <code>embeddings</code> and <code>ref_emb</code> be augmented versions of each other. For most losses, you have to create labels to indicate which <code>embeddings</code> correspond with which <code>ref_emb</code>. </p>
<p><code>SelfSupervisedLoss</code> is a wrapper that takes care of this by creating labels internally. It assumes that:</p>
<ul>
<li><code>ref_emb[i]</code> is an augmented version of <code>embeddings[i]</code>.</li>
<li><code>ref_emb[i]</code> is the only augmented version of <code>embeddings[i]</code> in the batch.</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">SelfSupervisedLoss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss function to be wrapped.</li>
<li><strong>symmetric</strong>: If <code>True</code>, then the embeddings in both <code>embeddings</code> and <code>ref_emb</code> are used as anchors. If <code>False</code>, then only the embeddings in <code>embeddings</code> are used as anchors.</li>
</ul>
<p>Example usage:</p>
<div class="highlight"><pre><span></span><code>loss_fn = losses.TripletMarginLoss()
loss_fn = SelfSupervisedLoss(loss_fn)
loss = loss_fn(embeddings, ref_emb)
</code></pre></div>
<details>
<summary>Supported Loss Functions</summary>
<ul>
<li><a href="./#angularloss">AngularLoss</a></li>
<li><a href="./#circleloss">CircleLoss</a></li>
<li><a href="./#contrastiveloss">ContrastiveLoss</a></li>
<li><a href="./#intrapairvarianceloss">IntraPairVarianceLoss</a></li>
<li><a href="./#multisimilarityloss">MultiSimilarityLoss</a></li>
<li><a href="./#ntxentloss">NTXentLoss</a></li>
<li><a href="./#signaltonoiseratiocontrastiveloss">SignalToNoiseRatioContrastiveLoss</a></li>
<li><a href="./#supconloss">SupConLoss</a></li>
<li><a href="./#tripletmarginloss">TripletMarginLoss</a></li>
<li><a href="./#tupletmarginloss">TupletMarginLoss</a></li>
</ul>
</details>
<h2 id="signaltonoiseratiocontrastiveloss">SignalToNoiseRatioContrastiveLoss<a class="headerlink" href="#signaltonoiseratiocontrastiveloss" title="Permanent link">&para;</a></h2>
<p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yuan_Signal-To-Noise_Ratio_A_Robust_Distance_Metric_for_Deep_Metric_Learning_CVPR_2019_paper.pdf" target="_blank">Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">SignalToNoiseRatioContrastiveLoss</span><span class="p">(</span><span class="n">pos_margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">neg_margin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</code></pre></div></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>pos_margin</strong>: The noise-to-signal ratio over which positive pairs will contribute to the loss.</li>
<li><strong>neg_margin</strong>: The noise-to-signal ratio under which negative pairs will contribute to the loss.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#snrdistance"><code>SNRDistance()</code></a><ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#avgnonzeroreducer">AvgNonZeroReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>pos_loss</strong>: The loss per positive pair in the batch. Reduction type is <code>"pos_pair"</code>.</li>
<li><strong>neg_loss</strong>: The loss per negative pair in the batch. Reduction type is <code>"neg_pair"</code>.</li>
</ul>
<h2 id="softtripleloss">SoftTripleLoss<a class="headerlink" href="#softtripleloss" title="Permanent link">&para;</a></h2>
<p><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Qian_SoftTriple_Loss_Deep_Metric_Learning_Without_Triplet_Sampling_ICCV_2019_paper.pdf" target="_blank">SoftTriple Loss: Deep Metric Learning Without Triplet Sampling</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">SoftTripleLoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> 
                    <span class="n">embedding_size</span><span class="p">,</span> 
                    <span class="n">centers_per_class</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                    <span class="n">la</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
                    <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
                    <span class="n">margin</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Equations</strong>:</p>
<p><img alt="soft_triple_loss_equation1" src="../imgs/soft_triple_loss_equation1.png" style="height:100px" /></p>
<p>where</p>
<p><img alt="soft_triple_loss_equation2" src="../imgs/soft_triple_loss_equation2.png" style="height:80px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>centers_per_class</strong>: The number of weight vectors per class. (The regular cross entropy loss has 1 center per class.) The paper uses 10.</li>
<li><strong>la</strong>: This is lambda in the above equation.</li>
<li><strong>gamma</strong>: This is gamma in the above equation. The paper uses 0.1.</li>
<li><strong>margin</strong>: The is delta in the above equations. The paper uses 0.01.</li>
</ul>
<p><strong>Other info</strong></p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts <code>weight_regularizer</code>, <code>weight_reg_weight</code>, and <code>weight_init_func</code> as optional arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:
<div class="highlight"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SoftTripleLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a><ul>
<li>The distance measure must be inverted. For example, <a href="../distances/#dotproductsimilarity"><code>DotProductSimilarity(normalize_embeddings=False)</code></a> is also compatible.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="spherefaceloss">SphereFaceLoss<a class="headerlink" href="#spherefaceloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1704.08063.pdf" target="_blank">SphereFace: Deep Hypersphere Embedding for Face Recognition</a></p>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">SphereFaceLoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> 
                    <span class="n">embedding_size</span><span class="p">,</span> 
                    <span class="n">margin</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> 
                    <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<p><strong>Parameters</strong>:</p>
<p>See <a href="./#largemarginsoftmaxloss">LargeMarginSoftmaxLoss</a></p>
<p><strong>Other info</strong></p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts <code>weight_regularizer</code>, <code>weight_reg_weight</code>, and <code>weight_init_func</code> as optional arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:
<div class="highlight"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SphereFaceLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li>
<p><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></p>
<ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="subcenterarcfaceloss">SubCenterArcFaceLoss<a class="headerlink" href="#subcenterarcfaceloss" title="Permanent link">&para;</a></h2>
<p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560715.pdf" target="_blank">Sub-center ArcFace: Boosting Face Recognition by Large-scale Noisy Web Faces</a></p>
<p>This loss extends <a href="./#arcfaceloss">ArcFaceLoss</a>. It uses multiple sub centers per class, instead of just a single center, hence the name Sub-center ArcFace.</p>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">SubCenterArcFaceLoss</span><span class="p">(</span>
    <span class="n">num_classes</span><span class="p">,</span> 
    <span class="n">embedding_size</span><span class="p">,</span> 
    <span class="n">margin</span><span class="o">=</span><span class="mf">28.6</span><span class="p">,</span> 
    <span class="n">scale</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> 
    <span class="n">sub_centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>sub_centers</strong>: The number of sub centers per class.</li>
</ul>
<p>See <a href="./#arcfaceloss">ArcFaceLoss</a> for a description of the other parameters.</p>
<p><strong>Other info</strong>: </p>
<ul>
<li>This loss <strong>requires an optimizer</strong>. See <a href="./#arcfaceloss">ArcFaceLoss</a> for details.</li>
<li>See <a href="./#arcfaceloss">ArcFaceLoss</a> for default distance, reducer, and reducer input.</li>
</ul>
<p><strong>Getting outliers and dominant centers</strong></p>
<p>Outliers and dominant centers can be computed as described in the paper.
<div class="highlight"><pre><span></span><code><span class="n">outliers</span><span class="p">,</span> <span class="n">dominant_centers</span> <span class="o">=</span> <span class="n">loss_func</span><span class="o">.</span><span class="n">get_outliers</span><span class="p">(</span>
    <span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">75</span><span class="p">,</span> <span class="n">return_dominant_centers</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</code></pre></div></p>
<h2 id="supconloss">SupConLoss<a class="headerlink" href="#supconloss" title="Permanent link">&para;</a></h2>
<p>Described in <a href="https://arxiv.org/abs/2004.11362" target="_blank">Supervised Contrastive Learning</a>.
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">SupConLoss</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Equation</strong>:</p>
<p><img alt="supcon_loss_equation" src="../imgs/supcon_loss_equation.png" style="height:90px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>temperature</strong>: This is tau in the above equation. The paper uses 0.1.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#avgnonzeroreducer">AvgNonZeroReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. If an element has only negative pairs or no pairs, it's ignored thanks to <code>AvgNonZeroReducer</code>. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="thresholdconsistentmarginloss">ThresholdConsistentMarginLoss<a class="headerlink" href="#thresholdconsistentmarginloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/2307.04047" target="_blank">Threshold-Consistent Margin Loss for Open-World Deep Metric Learning</a></p>
<p>This loss acts as a form of regularization and is usually combined with another metric loss function.</p>
<p><div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">ThresholdConsistentMarginLoss</span><span class="p">(</span>
    <span class="n">lambda_plus</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
    <span class="n">lambda_minus</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
    <span class="n">margin_plus</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> 
    <span class="n">margin_minus</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<strong>Equation</strong>:
<img alt="threshold_consistent_margin_loss" src="../imgs/tcm_loss_equation.png" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>lambda_plus</strong>: The scaling coefficient for the anchor-positive part of the loss. This is $\lambda^+$ in the above equation. </li>
<li><strong>lambda_minus</strong>: The scaling coefficient for the anchor-negative part of the loss. This is $\lambda^-$ in the above equation. </li>
<li><strong>margin_plus</strong>: The minimum anchor-positive similarity to be included in the loss. This is $m^+$ in the above equation.</li>
<li><strong>margin_minus</strong>: The maximum anchor-negative similarity to be included in the loss. This is $m^-$ in the above equation.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a><ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<h2 id="tripletmarginloss">TripletMarginLoss<a class="headerlink" href="#tripletmarginloss" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">TripletMarginLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                        <span class="n">swap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">smooth_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">triplets_per_anchor</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span>
                        <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<p><strong>Equation</strong>:</p>
<p><img alt="triplet_margin_loss_equation" src="../imgs/triplet_margin_loss_equation.png" style="height:35px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>margin</strong>: The desired difference between the anchor-positive distance and the anchor-negative distance. This is <code>m</code> in the above equation.</li>
<li><strong>swap</strong>: Use the positive-negative distance instead of anchor-negative distance, if it violates the margin more.</li>
<li><strong>smooth_loss</strong>: Use the log-exp version of the triplet loss</li>
<li><strong>triplets_per_anchor</strong>: The number of triplets per element to sample within a batch. Can be an integer or the string "all". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is "all", then all possible triplets in the batch will be used.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=1)</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#avgnonzeroreducer">AvgNonZeroReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per triplet in the batch. Reduction type is <code>"triplet"</code>.</li>
</ul>
<h2 id="tupletmarginloss">TupletMarginLoss<a class="headerlink" href="#tupletmarginloss" title="Permanent link">&para;</a></h2>
<p><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Deep_Metric_Learning_With_Tuplet_Margin_Loss_ICCV_2019_paper.pdf" target="_blank">Deep Metric Learning with Tuplet Margin Loss</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">TupletMarginLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">5.73</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Equation</strong>:</p>
<p><img alt="tuplet_margin_loss_equation" src="../imgs/tuplet_margin_loss_equation.png" style="height:80px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>margin</strong>: The angular margin (in degrees) applied to positive pairs. This is beta in the above equation. The paper uses a value of 5.73 degrees (0.1 radians).</li>
<li><strong>scale</strong>: This is <code>s</code> in the above equation.</li>
</ul>
<p>The paper combines this loss with <a href="./#intrapairvarianceloss">IntraPairVarianceLoss</a>. You can accomplish this by using <a href="./#multiplelosses">MultipleLosses</a>:
<div class="highlight"><pre><span></span><code><span class="n">main_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">TupletMarginLoss</span><span class="p">()</span>
<span class="n">var_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">IntraPairVarianceLoss</span><span class="p">()</span>
<span class="n">complete_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">MultipleLosses</span><span class="p">([</span><span class="n">main_loss</span><span class="p">,</span> <span class="n">var_loss</span><span class="p">],</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
</code></pre></div></p>
<p><strong>Default distance</strong>: </p>
<ul>
<li>
<p><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></p>
<ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per positive pair in the batch. Reduction type is <code>"pos_pair"</code>.</li>
</ul>
<h2 id="weightregularizermixin">WeightRegularizerMixin<a class="headerlink" href="#weightregularizermixin" title="Permanent link">&para;</a></h2>
<p>Losses can extend this class in addition to BaseMetricLossFunction. You should extend this class if your loss function contains a learnable weight matrix.
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">WeightRegularizerMixin</span><span class="p">(</span><span class="n">weight_init_func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weight_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weight_reg_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>weight_init_func</strong>: An <a href="../common_functions/#torchinitwrapper">TorchInitWrapper</a> object, which will be used to initialize the weights of the loss function.</li>
<li><strong>weight_regularizer</strong>: The <a href="../regularizers/">regularizer</a> to apply to the loss's learned weights.</li>
<li><strong>weight_reg_weight</strong>: The amount the regularization loss will be multiplied by.</li>
</ul>
<p>Extended by:</p>
<ul>
<li><a href="./#arcfaceloss">ArcFaceLoss</a></li>
<li><a href="./#cosfaceloss">CosFaceLoss</a></li>
<li><a href="./#largemarginsoftmaxloss">LargeMarginSoftmaxLoss</a></li>
<li><a href="./#normalizedsoftmaxloss">NormalizedSoftmaxLoss</a></li>
<li><a href="./#proxyanchorloss">ProxyAnchorLoss</a></li>
<li><a href="./#proxyncaloss">ProxyNCALoss</a></li>
<li><a href="./#softtripleloss">SoftTripleLoss</a></li>
<li><a href="./#spherefaceloss">SphereFaceLoss</a></li>
</ul>
<h2 id="vicregloss">VICRegLoss<a class="headerlink" href="#vicregloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/2105.04906.pdf" target="_blank">VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning</a>
<div class="highlight"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">VICRegLoss</span><span class="p">(</span><span class="n">invariance_lambda</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> 
                <span class="n">variance_mu</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> 
                <span class="n">covariance_v</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                <span class="n">eps</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> 
                <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Usage</strong>:</p>
<p>Unlike other loss functions, <code>VICRegLoss</code> does not accept <code>labels</code> or <code>indices_tuple</code>:</p>
<div class="highlight"><pre><span></span><code><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">VICRegLoss</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">ref_emb</span><span class="o">=</span><span class="n">ref_emb</span><span class="p">)</span>
</code></pre></div>
<p><strong>Equations</strong>:</p>
<p><img alt="vicreg_total" src="../imgs/vicreg_total.png" style="height:40px" /></p>
<p>where</p>
<p><img alt="vicreg_total" src="../imgs/vicreg_invariance.png" style="height:70px" /></p>
<p><img alt="vicreg_total" src="../imgs/vicreg_variance.png" style="height:90px" /></p>
<p><img alt="vicreg_total" src="../imgs/vicreg_variance_detail.png" style="height:40px" /></p>
<p><img alt="vicreg_total" src="../imgs/vicreg_covariance.png" style="height:70px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>invariance_lambda</strong>: The weight of the invariance term.</li>
<li><strong>variance_mu</strong>: The weight of the variance term.</li>
<li><strong>covariance_v</strong>: The weight of the covariance term.</li>
<li><strong>eps</strong>: Small scalar to prevent numerical instability.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li>Not applicable. You cannot pass in a distance function.</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>invariance_loss</strong>: The MSE loss between <code>embeddings[i]</code> and <code>ref_emb[i]</code>. Reduction type is <code>"element"</code>.</li>
<li><strong>variance_loss1</strong>: The variance loss for <code>embeddings</code>. Reduction type is <code>"element"</code>.</li>
<li><strong>variance_loss2</strong>: The variance loss for <code>ref_emb</code>. Reduction type is <code>"element"</code>.</li>
<li><strong>covariance_loss</strong>: The covariance loss. This loss is already reduced to a single value.</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="shortcut icon" href="../imgs/Favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-5.5.0">
    
    
      
        <title>Losses - PyTorch Metric Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.b5d04df8.min.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/palette.9ab2c1f8.min.css">
      
      
        
        
        <meta name="theme-color" content="#ef5350">
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
    
    
  </head>
  
  
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="red" data-md-color-accent="red">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#losses" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href=".." title="PyTorch Metric Learning" class="md-header-nav__button md-logo" aria-label="PyTorch Metric Learning">
      
  <img src="../imgs/TinyLogo.png" alt="logo">

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            PyTorch Metric Learning
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              Losses
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/KevinMusgrave/pytorch-metric-learning/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    KevinMusgrave/pytorch-metric-learning
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="PyTorch Metric Learning" class="md-nav__button md-logo" aria-label="PyTorch Metric Learning">
      
  <img src="../imgs/TinyLogo.png" alt="logo">

    </a>
    PyTorch Metric Learning
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/KevinMusgrave/pytorch-metric-learning/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    KevinMusgrave/pytorch-metric-learning
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../distances/" title="Distances" class="md-nav__link">
      Distances
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Losses
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 9h14V7H3v2m0 4h14v-2H3v2m0 4h14v-2H3v2m16 0h2v-2h-2v2m0-10v2h2V7h-2m0 6h2v-2h-2v2z"/></svg>
        </span>
      </label>
    
    <a href="./" title="Losses" class="md-nav__link md-nav__link--active">
      Losses
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#angularloss" class="md-nav__link">
    AngularLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#arcfaceloss" class="md-nav__link">
    ArcFaceLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basemetriclossfunction" class="md-nav__link">
    BaseMetricLossFunction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#circleloss" class="md-nav__link">
    CircleLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contrastiveloss" class="md-nav__link">
    ContrastiveLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cosfaceloss" class="md-nav__link">
    CosFaceLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#crossbatchmemory" class="md-nav__link">
    CrossBatchMemory
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fastaploss" class="md-nav__link">
    FastAPLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#genericpairloss" class="md-nav__link">
    GenericPairLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generalizedliftedstructureloss" class="md-nav__link">
    GeneralizedLiftedStructureLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intrapairvarianceloss" class="md-nav__link">
    IntraPairVarianceLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#largemarginsoftmaxloss" class="md-nav__link">
    LargeMarginSoftmaxLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#liftedstructureloss" class="md-nav__link">
    LiftedStructureLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#marginloss" class="md-nav__link">
    MarginLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multiplelosses" class="md-nav__link">
    MultipleLosses
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multisimilarityloss" class="md-nav__link">
    MultiSimilarityLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ncaloss" class="md-nav__link">
    NCALoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#normalizedsoftmaxloss" class="md-nav__link">
    NormalizedSoftmaxLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#npairsloss" class="md-nav__link">
    NPairsLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ntxentloss" class="md-nav__link">
    NTXentLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#proxyanchorloss" class="md-nav__link">
    ProxyAnchorLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#proxyncaloss" class="md-nav__link">
    ProxyNCALoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#signaltonoiseratiocontrastiveloss" class="md-nav__link">
    SignalToNoiseRatioContrastiveLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#softtripleloss" class="md-nav__link">
    SoftTripleLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spherefaceloss" class="md-nav__link">
    SphereFaceLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#supconloss" class="md-nav__link">
    SupConLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tripletmarginloss" class="md-nav__link">
    TripletMarginLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tupletmarginloss" class="md-nav__link">
    TupletMarginLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#weightregularizermixin" class="md-nav__link">
    WeightRegularizerMixin
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../miners/" title="Miners" class="md-nav__link">
      Miners
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../reducers/" title="Reducers" class="md-nav__link">
      Reducers
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../regularizers/" title="Regularizers" class="md-nav__link">
      Regularizers
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../samplers/" title="Samplers" class="md-nav__link">
      Samplers
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../trainers/" title="Trainers" class="md-nav__link">
      Trainers
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../testers/" title="Testers" class="md-nav__link">
      Testers
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-10" type="checkbox" id="nav-10">
    
    <label class="md-nav__link" for="nav-10">
      Utils
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Utils" data-md-level="1">
      <label class="md-nav__title" for="nav-10">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
        </span>
        Utils
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../accuracy_calculation/" title="Accuracy Calculation" class="md-nav__link">
      Accuracy Calculation
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../inference_models/" title="Inference Models" class="md-nav__link">
      Inference Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../logging_presets/" title="Logging Presets" class="md-nav__link">
      Logging Presets
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../common_functions/" title="Common Functions" class="md-nav__link">
      Common Functions
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../distributed/" title="Distributed" class="md-nav__link">
      Distributed
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-11" type="checkbox" id="nav-11">
    
    <label class="md-nav__link" for="nav-11">
      How to extend this library
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="How to extend this library" data-md-level="1">
      <label class="md-nav__title" for="nav-11">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
        </span>
        How to extend this library
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../extend/losses/" title="Custom losses" class="md-nav__link">
      Custom losses
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../extend/miners/" title="Custom miners" class="md-nav__link">
      Custom miners
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#angularloss" class="md-nav__link">
    AngularLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#arcfaceloss" class="md-nav__link">
    ArcFaceLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basemetriclossfunction" class="md-nav__link">
    BaseMetricLossFunction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#circleloss" class="md-nav__link">
    CircleLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contrastiveloss" class="md-nav__link">
    ContrastiveLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cosfaceloss" class="md-nav__link">
    CosFaceLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#crossbatchmemory" class="md-nav__link">
    CrossBatchMemory
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fastaploss" class="md-nav__link">
    FastAPLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#genericpairloss" class="md-nav__link">
    GenericPairLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generalizedliftedstructureloss" class="md-nav__link">
    GeneralizedLiftedStructureLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intrapairvarianceloss" class="md-nav__link">
    IntraPairVarianceLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#largemarginsoftmaxloss" class="md-nav__link">
    LargeMarginSoftmaxLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#liftedstructureloss" class="md-nav__link">
    LiftedStructureLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#marginloss" class="md-nav__link">
    MarginLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multiplelosses" class="md-nav__link">
    MultipleLosses
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multisimilarityloss" class="md-nav__link">
    MultiSimilarityLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ncaloss" class="md-nav__link">
    NCALoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#normalizedsoftmaxloss" class="md-nav__link">
    NormalizedSoftmaxLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#npairsloss" class="md-nav__link">
    NPairsLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ntxentloss" class="md-nav__link">
    NTXentLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#proxyanchorloss" class="md-nav__link">
    ProxyAnchorLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#proxyncaloss" class="md-nav__link">
    ProxyNCALoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#signaltonoiseratiocontrastiveloss" class="md-nav__link">
    SignalToNoiseRatioContrastiveLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#softtripleloss" class="md-nav__link">
    SoftTripleLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spherefaceloss" class="md-nav__link">
    SphereFaceLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#supconloss" class="md-nav__link">
    SupConLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tripletmarginloss" class="md-nav__link">
    TripletMarginLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tupletmarginloss" class="md-nav__link">
    TupletMarginLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#weightregularizermixin" class="md-nav__link">
    WeightRegularizerMixin
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/KevinMusgrave/pytorch-metric-learning/edit/master/docs/losses.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                  
                
                
                <h1 id="losses">Losses<a class="headerlink" href="#losses" title="Permanent link">&para;</a></h1>
<p>All loss functions are used as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">pytorch_metric_learning</span> <span class="kn">import</span> <span class="n">losses</span>
<span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SomeLoss</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="c1"># in your training for-loop</span>
</code></pre></div>


<p>Or if you are using a loss in conjunction with a <a href="../miners/">miner</a>:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">pytorch_metric_learning</span> <span class="kn">import</span> <span class="n">miners</span><span class="p">,</span> <span class="n">losses</span>
<span class="n">miner_func</span> <span class="o">=</span> <span class="n">miners</span><span class="o">.</span><span class="n">SomeMiner</span><span class="p">()</span>
<span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SomeLoss</span><span class="p">()</span>
<span class="n">miner_output</span> <span class="o">=</span> <span class="n">miner_func</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="c1"># in your training for-loop</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">miner_output</span><span class="p">)</span>
</code></pre></div>


<p>You can also specify how losses get reduced to a single value by using a <a href="../reducers/">reducer</a>:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">pytorch_metric_learning</span> <span class="kn">import</span> <span class="n">losses</span><span class="p">,</span> <span class="n">reducers</span>
<span class="n">reducer</span> <span class="o">=</span> <span class="n">reducers</span><span class="o">.</span><span class="n">SomeReducer</span><span class="p">()</span>
<span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SomeLoss</span><span class="p">(</span><span class="n">reducer</span><span class="o">=</span><span class="n">reducer</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="c1"># in your training for-loop</span>
</code></pre></div>


<h2 id="angularloss">AngularLoss<a class="headerlink" href="#angularloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1708.01682.pdf" target="_blank">Deep Metric Learning with Angular Loss</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">AngularLoss</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equation</strong>:</p>
<p><img alt="angular_loss_equation" src="../imgs/angular_loss_equation.png" style="height:200px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>alpha</strong>: The angle specified in degrees. The paper uses values between 36 and 55.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li>
<p><a href="../distances/#lpdistance"><code>LpDistance(p=2, power=1, normalize_embeddings=True)</code></a></p>
<ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss for every <code>a1</code>, where <code>(a1,p)</code> represents every positive pair in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="arcfaceloss">ArcFaceLoss<a class="headerlink" href="#arcfaceloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1801.07698.pdf" target="_blank">ArcFace: Additive Angular Margin Loss for Deep Face Recognition</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">ArcFaceLoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">28.6</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equation</strong>:</p>
<p><img alt="arcface_loss_equation" src="../imgs/arcface_loss_equation.png" style="height:80px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>margin</strong>: The angular margin penalty in degrees. In the above equation, <code>m = radians(margin)</code>. The paper uses 0.5 radians, which is 28.6 degrees.</li>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>scale</strong>: This is <code>s</code> in the above equation. The paper uses 64.</li>
</ul>
<p><strong>Other info</strong>: </p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts <code>weight_regularizer</code>, <code>weight_reg_weight</code>, and <code>weight_init_func</code> as optional arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">ArcFaceLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>


<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a><ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="basemetriclossfunction">BaseMetricLossFunction<a class="headerlink" href="#basemetriclossfunction" title="Permanent link">&para;</a></h2>
<p>All loss functions extend this class and therefore inherit its <code>__init__</code> parameters.</p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">BaseMetricLossFunction</span><span class="p">(</span><span class="n">collect_stats</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> 
                            <span class="n">reducer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                            <span class="n">distance</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                            <span class="n">embedding_regularizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                            <span class="n">embedding_reg_weight</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>collect_stats</strong>: If True, will collect various statistics that may be useful to analyze during experiments. If False, these computations will be skipped. Want to make <code>False</code> the default? Set the global <a href="../common_functions/#collect_stats">COLLECT_STATS</a> flag.</li>
<li><strong>reducer</strong>: A <a href="../reducers/">reducer</a> object. If None, then the default reducer will be used.</li>
<li><strong>distance</strong>: A <a href="../distances/">distance</a> object. If None, then the default distance will be used.</li>
<li><strong>embedding_regularizer</strong>: A <a href="../regularizers/">regularizer</a> object that will be applied to embeddings. If None, then no embedding regularization will be used.</li>
<li><strong>embedding_reg_weight</strong>: If an embedding regularizer is used, then its loss will be multiplied by this amount before being added to the total loss.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=1)</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>embedding_reg_loss</strong>: Only exists if an embedding regularizer is used. It contains the loss per element in the batch. Reduction type is <code>"already_reduced"</code>. </li>
</ul>
<p><strong>Required Implementations</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">indices_tuple</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div>


<h2 id="circleloss">CircleLoss<a class="headerlink" href="#circleloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/2002.10857.pdf" target="_blank">Circle Loss: A Unified Perspective of Pair Similarity Optimization</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">CircleLoss</span><span class="p">(</span><span class="n">m</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equations</strong>:</p>
<p><img alt="circle_loss_equation1" src="../imgs/circle_loss_equation1.png" style="height:150px" /></p>
<p>where</p>
<p><img alt="circle_loss_equation2" src="../imgs/circle_loss_equation2.png" style="height:70px" /></p>
<p><img alt="circle_loss_equation7" src="../imgs/circle_loss_equation7.png" style="height:25px" /></p>
<p><img alt="circle_loss_equation8" src="../imgs/circle_loss_equation8.png" style="height:25px" /></p>
<p><img alt="circle_loss_equation3" src="../imgs/circle_loss_equation3.png" style="height:25px" /></p>
<p><img alt="circle_loss_equation4" src="../imgs/circle_loss_equation4.png" style="height:25px" /></p>
<p><img alt="circle_loss_equation5" src="../imgs/circle_loss_equation5.png" style="height:25px" /></p>
<p><img alt="circle_loss_equation6" src="../imgs/circle_loss_equation6.png" style="height:25px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>m</strong>: The relaxation factor that controls the radius of the decision boundary. The paper uses 0.25 for face recognition, and 0.4 for fine-grained image retrieval (images of birds, cars, and online products).</li>
<li><strong>gamma</strong>: The scale factor that determines the largest scale of each similarity score. The paper uses 256 for face recognition, and 80 for fine-grained image retrieval.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li>
<p><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></p>
<ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#avgnonzeroreducer">AvgNonZeroReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="contrastiveloss">ContrastiveLoss<a class="headerlink" href="#contrastiveloss" title="Permanent link">&para;</a></h2>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">ContrastiveLoss</span><span class="p">(</span><span class="n">pos_margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">neg_margin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</code></pre></div>


<p><strong>Equation</strong>:</p>
<p>If using a distance metric like <a href="../distances/#lpdistance">LpDistance</a>, the loss is:</p>
<p><img alt="contrastive_loss_equation" src="../imgs/contrastive_loss_equation.png" style="height:37px" /></p>
<p>If using a similarity metric like <a href="../distances/#cosinesimilarity">CosineSimilarity</a>, the loss is:</p>
<p><img alt="contrastive_loss_similarity_equation" src="../imgs/contrastive_loss_similarity_equation.png" style="height:35px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>pos_margin</strong>: The distance (or similarity) over (under) which positive pairs will contribute to the loss.</li>
<li><strong>neg_margin</strong>: The distance (or similarity) under (over) which negative pairs will contribute to the loss.  </li>
</ul>
<p>Note that the default values for <code>pos_margin</code> and <code>neg_margin</code> are suitable if you are using a non-inverted distance measure, like <a href="../distances/#lpdistance">LpDistance</a>. If you use an inverted distance measure like <a href="../distances/#cosinesimilarity">CosineSimilarity</a>, then more appropriate values would be <code>pos_margin = 1</code> and <code>neg_margin = 0</code>.</p>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=1)</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#avgnonzeroreducer">AvgNonZeroReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>pos_loss</strong>: The loss per positive pair in the batch. Reduction type is <code>"pos_pair"</code>.</li>
<li><strong>neg_loss</strong>: The loss per negative pair in the batch. Reduction type is <code>"neg_pair"</code>.</li>
</ul>
<h2 id="cosfaceloss">CosFaceLoss<a class="headerlink" href="#cosfaceloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1801.09414.pdf" target="_blank">CosFace: Large Margin Cosine Loss for Deep Face Recognition</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">CosFaceLoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">0.35</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equation</strong>:</p>
<p><img alt="cosface_loss_equation" src="../imgs/cosface_loss_equation.png" style="height:80px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>margin</strong>: The cosine margin penalty (m in the above equation). The paper used values between 0.25 and 0.45.</li>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>scale</strong>: This is <code>s</code> in the above equation. The paper uses 64.</li>
</ul>
<p><strong>Other info</strong>: </p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts <code>weight_regularizer</code>, <code>weight_reg_weight</code>, and <code>weight_init_func</code> as optional arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">CosFaceLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>


<p><strong>Default distance</strong>: </p>
<ul>
<li>
<p><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></p>
<ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="crossbatchmemory">CrossBatchMemory<a class="headerlink" href="#crossbatchmemory" title="Permanent link">&para;</a></h2>
<p>This wraps a loss function, and implements <a href="https://arxiv.org/pdf/1912.06798.pdf" target="_blank">Cross-Batch Memory for Embedding Learning</a>. It stores embeddings from previous iterations in a queue, and uses them to form more pairs/triplets with the current iteration's embeddings.</p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">CrossBatchMemory</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">memory_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">miner</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss function to be wrapped. For example, you could pass in <code>ContrastiveLoss()</code>.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>memory_size</strong>: The size of the memory queue.</li>
<li><strong>miner</strong>: An optional <a href="../miners/">tuple miner</a>, which will be used to mine pairs/triplets from the memory queue.</li>
</ul>
<p><strong>Forward function</strong></p>
<div class="codehilite"><pre><span></span><code><span class="n">loss_fn</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">indices_tuple</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">enqueue_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>


<p>As shown above, CrossBatchMemory comes with a 4th argument in its <code>forward</code> function:</p>
<ul>
<li><strong>enqueue_idx</strong>: The indices of <code>embeddings</code> that will be added to the memory queue. In other words, only <code>embeddings[enqueue_idx]</code> will be added to memory. This enables CrossBatchMemory to be used in self-supervision frameworks like <a href="https://arxiv.org/pdf/1911.05722.pdf">MoCo</a>. Check out the <a href="https://github.com/KevinMusgrave/pytorch-metric-learning/tree/master/examples#simple-examples">MoCo on CIFAR100</a> notebook to see how this works.</li>
</ul>
<h2 id="fastaploss">FastAPLoss<a class="headerlink" href="#fastaploss" title="Permanent link">&para;</a></h2>
<p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Cakir_Deep_Metric_Learning_to_Rank_CVPR_2019_paper.pdf" target="_blank">Deep Metric Learning to Rank</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">FastAPLoss</span><span class="p">(</span><span class="n">num_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>num_bins</strong>: The number of soft histogram bins for calculating average precision. The paper suggests using 10.</li>
</ul>
<p><strong>Default distance</strong>:</p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=2)</code></a><ul>
<li>The only compatible distance is <code>LpDistance(normalize_embeddings=True, p=2)</code>. However, the <code>power</code> value can be changed.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element that has at least 1 positive in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="genericpairloss">GenericPairLoss<a class="headerlink" href="#genericpairloss" title="Permanent link">&para;</a></h2>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">GenericPairLoss</span><span class="p">(</span><span class="n">mat_based_loss</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>mat_based_loss</strong>: See required implementations.</li>
</ul>
<p><strong>Required Implementations</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># If mat_based_loss is True, then this takes in mat, pos_mask, neg_mask</span>
<span class="c1"># If False, this takes in pos_pair, neg_pair, indices_tuple</span>
<span class="k">def</span> <span class="nf">_compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div>


<h2 id="generalizedliftedstructureloss">GeneralizedLiftedStructureLoss<a class="headerlink" href="#generalizedliftedstructureloss" title="Permanent link">&para;</a></h2>
<p>This was presented in <a href="https://arxiv.org/pdf/1703.07737.pdf" target="_blank">In Defense of the Triplet Loss for Person Re-Identification</a>. It is a modification of the original <a href="./#liftedstructureloss">LiftedStructureLoss</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">GeneralizedLiftedStructureLoss</span><span class="p">(</span><span class="n">neg_margin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pos_margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equation</strong>:</p>
<p><img alt="generalized_lifted_structure_loss_equation" src="../imgs/generalized_lifted_structure_loss_equation.png" style="height:250px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>pos_margin</strong>: The margin in the expression <code>e^(D - margin)</code>. The paper uses <code>pos_margin = 0</code>, which is why this margin does not appear in the above equation.</li>
<li><strong>neg_margin</strong>: This is <code>m</code> in the above equation. The paper used values between 0.1 and 1.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=1)</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="intrapairvarianceloss">IntraPairVarianceLoss<a class="headerlink" href="#intrapairvarianceloss" title="Permanent link">&para;</a></h2>
<p><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Deep_Metric_Learning_With_Tuplet_Margin_Loss_ICCV_2019_paper.pdf" target="_blank">Deep Metric Learning with Tuplet Margin Loss</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">IntraPairVarianceLoss</span><span class="p">(</span><span class="n">pos_eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">neg_eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equations</strong>:</p>
<p><img alt="intra_pair_variance_loss_equation1" src="../imgs/intra_pair_variance_loss_equation1.png" style="height:39px" /></p>
<p><img alt="intra_pair_variance_loss_equation2" src="../imgs/intra_pair_variance_loss_equation2.png" style="height:34px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>pos_eps</strong>: The epsilon in the L<sub>pos</sub> equation. The paper uses 0.01.</li>
<li><strong>neg_eps</strong>: The epsilon in the L<sub>neg</sub> equation. The paper uses 0.01.</li>
</ul>
<p>You should probably use this in conjunction with another loss, as described in the paper. You can accomplish this by using <a href="./#multiplelosses">MultipleLosses</a>:</p>
<div class="codehilite"><pre><span></span><code><span class="n">main_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">TupletMarginLoss</span><span class="p">()</span>
<span class="n">var_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">IntraPairVarianceLoss</span><span class="p">()</span>
<span class="n">complete_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">MultipleLosses</span><span class="p">([</span><span class="n">main_loss</span><span class="p">,</span> <span class="n">var_loss</span><span class="p">],</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
</code></pre></div>


<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=1)</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>pos_loss</strong>: The loss per positive pair in the batch. Reduction type is <code>"pos_pair"</code>.</li>
<li><strong>neg_loss</strong>: The loss per negative pair in the batch. Reduction type is <code>"neg_pair"</code>.</li>
</ul>
<h2 id="largemarginsoftmaxloss">LargeMarginSoftmaxLoss<a class="headerlink" href="#largemarginsoftmaxloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1612.02295.pdf" target="_blank">Large-Margin Softmax Loss for Convolutional Neural Networks</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">LargeMarginSoftmaxLoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> 
                            <span class="n">embedding_size</span><span class="p">,</span> 
                            <span class="n">margin</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> 
                            <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                            <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equations</strong>:</p>
<p><img alt="large_margin_softmax_loss_equation1" src="../imgs/large_margin_softmax_loss_equation1.png" style="height:80px" /></p>
<p>where</p>
<p><img alt="large_margin_softmax_loss_equation2" src="../imgs/large_margin_softmax_loss_equation2.png" style="height:90px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>margin</strong>: An integer which dictates the size of the angular margin. This is <code>m</code> in the above equation. The paper finds <code>m=4</code> works best.</li>
<li><strong>scale</strong>: The exponent multiplier in the loss's softmax expression. The paper uses <code>scale = 1</code>, which is why it does not appear in the above equation.</li>
</ul>
<p><strong>Other info</strong>: </p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts <code>weight_regularizer</code>, <code>weight_reg_weight</code>, and <code>weight_init_func</code> as optional arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">LargeMarginSoftmaxLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>


<p><strong>Default distance</strong>: </p>
<ul>
<li>
<p><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></p>
<ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="liftedstructureloss">LiftedStructureLoss<a class="headerlink" href="#liftedstructureloss" title="Permanent link">&para;</a></h2>
<p>The original lifted structure loss as presented in <a href="https://arxiv.org/pdf/1511.06452.pdf" target="_blank">Deep Metric Learning via Lifted Structured Feature Embedding</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">LiftedStructureLoss</span><span class="p">(</span><span class="n">neg_margin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pos_margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</code></pre></div>


<p><strong>Equation</strong>:</p>
<p><img alt="lifted_structure_loss_equation" src="../imgs/lifted_structure_loss_equation.png" style="height:150px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>pos_margin</strong>: The margin in the expression <code>D_(i,j) - margin</code>. The paper uses <code>pos_margin = 0</code>, which is why it does not appear in the above equation.</li>
<li><strong>neg_margin</strong>: This is <code>alpha</code> in the above equation. The paper uses 1.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=1)</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per positive pair in the batch. Reduction type is <code>"pos_pair"</code>.</li>
</ul>
<h2 id="marginloss">MarginLoss<a class="headerlink" href="#marginloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1706.07567.pdf" target="_blank">Sampling Matters in Deep Embedding Learning</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">MarginLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> 
                <span class="n">nu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> 
                <span class="n">beta</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> 
                <span class="n">triplets_per_anchor</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> 
                <span class="n">learn_beta</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                <span class="n">num_classes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equations</strong>:</p>
<p><img alt="margin_loss_equation2" src="../imgs/margin_loss_equation2.png" style="height:60px" /></p>
<p>where</p>
<p><img alt="margin_loss_equation1" src="../imgs/margin_loss_equation1.png" style="height:40px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>margin</strong>: This is alpha in the above equation. The paper uses 0.2.</li>
<li><strong>nu</strong>: The regularization weight for the magnitude of beta.</li>
<li><strong>beta</strong>: This is beta in the above equation. The paper uses 1.2 as the initial value.</li>
<li><strong>triplets_per_anchor</strong>: The number of triplets per element to sample within a batch. Can be an integer or the string "all". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is "all", then all possible triplets in the batch will be used.</li>
<li><strong>learn_beta</strong>: If True, beta will be a torch.nn.Parameter, which can be optimized using any PyTorch optimizer.</li>
<li><strong>num_classes</strong>: If not None, then beta will be of size <code>num_classes</code>, so that a separate beta is used for each class during training.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=1)</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#divisorreducer">DivisorReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>margin_loss</strong>: The loss per triplet in the batch. Reduction type is <code>"triplet"</code>.</li>
<li><strong>beta_reg_loss</strong>: The regularization loss per element in <code>self.beta</code>. Reduction type is <code>"already_reduced"</code> if <code>self.num_classes = None</code>. Otherwise it is <code>"element"</code>.</li>
</ul>
<h2 id="multiplelosses">MultipleLosses<a class="headerlink" href="#multiplelosses" title="Permanent link">&para;</a></h2>
<p>This is a simple wrapper for multiple losses. Pass in a list of already-initialized loss functions. Then, when you call forward on this object, it will return the sum of all wrapped losses.</p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">MultipleLosses</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">miners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>losses</strong>: A list or dictionary of initialized loss functions. On the forward call of MultipleLosses, each wrapped loss will be computed, and then the average will be returned.</li>
<li><strong>miners</strong>: Optional. A list or dictionary of mining functions. This allows you to pair mining functions with loss functions. For example, if <code>losses = [loss_A, loss_B]</code>, and <code>miners = [None, miner_B]</code> then no mining will be done for <code>loss_A</code>, but the output of <code>miner_B</code> will be passed to <code>loss_B</code>. The same logic applies if <code>losses = {"loss_A": loss_A, "loss_B": loss_B}</code> and <code>miners = {"loss_B": miner_B}</code>.</li>
<li><strong>weights</strong>: Optional. A list or dictionary of loss weights, which will be multiplied by the corresponding losses obtained by the loss functions. The default is to multiply each loss by 1. If <code>losses</code> is a list, then <code>weights</code> must be a list. If <code>losses</code> is a dictionary, <code>weights</code> must contain the same keys as <code>losses</code>. </li>
</ul>
<h2 id="multisimilarityloss">MultiSimilarityLoss<a class="headerlink" href="#multisimilarityloss" title="Permanent link">&para;</a></h2>
<p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Multi-Similarity_Loss_With_General_Pair_Weighting_for_Deep_Metric_Learning_CVPR_2019_paper.pdf" target="_blank">Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">MultiSimilarityLoss</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equation</strong>:</p>
<p><img alt="multi_similarity_loss_equation" src="../imgs/multi_similarity_loss_equation.png" style="height:150px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>alpha</strong>: The weight applied to positive pairs. The paper uses 2.</li>
<li><strong>beta</strong>: The weight applied to negative pairs. The paper uses 50.</li>
<li><strong>base</strong>: The offset applied to the exponent in the loss. This is lambda in the above equation. The paper uses 1. </li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="ncaloss">NCALoss<a class="headerlink" href="#ncaloss" title="Permanent link">&para;</a></h2>
<p><a href="https://www.cs.toronto.edu/~hinton/absps/nca.pdf" target="_blank">Neighbourhood Components Analysis</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">NCALoss</span><span class="p">(</span><span class="n">softmax_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equations</strong>:</p>
<p><img alt="nca_loss_equation1" src="../imgs/nca_loss_equation1.png" style="height:50px" /></p>
<p>where</p>
<p><img alt="nca_loss_equation2" src="../imgs/nca_loss_equation2.png" style="height:60px" /></p>
<p><img alt="nca_loss_equation3" src="../imgs/nca_loss_equation3.png" style="height:60px" /></p>
<p>In this implementation, we use <code>-g(A)</code> as the loss.</p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>softmax_scale</strong>: The exponent multiplier in the loss's softmax expression. The paper uses <code>softmax_scale = 1</code>, which is why it does not appear in the above equations.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=2)</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch, that results in a non zero exponent in the cross entropy expression. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="normalizedsoftmaxloss">NormalizedSoftmaxLoss<a class="headerlink" href="#normalizedsoftmaxloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1811.12649.pdf" target="_blank">Classification is a Strong Baseline for Deep Metric Learning</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">NormalizedSoftmaxLoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equation</strong>:</p>
<p><img alt="normalized_softmax_loss_equation" src="../imgs/normalized_softmax_loss_equation.png" style="height:80px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>temperature</strong>: This is sigma in the above equation. The paper uses 0.05.</li>
</ul>
<p><strong>Other info</strong></p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts <code>weight_regularizer</code>, <code>weight_reg_weight</code>, and <code>weight_init_func</code> as optional arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">NormalizedSoftmaxLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>


<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#dotproductsimilarity"><code>DotProductSimilarity()</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="npairsloss">NPairsLoss<a class="headerlink" href="#npairsloss" title="Permanent link">&para;</a></h2>
<p><a href="http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf" target="_blank">Improved Deep Metric Learning with Multi-class N-pair Loss Objective</a></p>
<p>If your batch has more than 2 samples per label, then you should use <a href="#ntxentloss">NTXentLoss</a>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">NPairsLoss</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#dotproductsimilarity"><code>DotProductSimilarity()</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="ntxentloss">NTXentLoss<a class="headerlink" href="#ntxentloss" title="Permanent link">&para;</a></h2>
<p>This is also known as InfoNCE, and is a generalization of the <a href="./#npairsloss">NPairsLoss</a>. It has been used in self-supervision papers such as: </p>
<ul>
<li><a href="https://arxiv.org/pdf/1807.03748.pdf" target="_blank">Representation Learning with Contrastive Predictive Coding</a></li>
<li><a href="https://arxiv.org/pdf/1911.05722.pdf" target="_blank">Momentum Contrast for Unsupervised Visual Representation Learning</a></li>
<li><a href="https://arxiv.org/pdf/2002.05709.pdf" target="_blank">A Simple Framework for Contrastive Learning of Visual Representations</a></li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">NTXentLoss</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.07</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equation</strong>:</p>
<p><img alt="ntxent_loss_equation" src="../imgs/ntxent_loss_equation.png" style="height:70px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>temperature</strong>: This is tau in the above equation. The MoCo paper uses 0.07, while SimCLR uses 0.5.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per positive pair in the batch. Reduction type is <code>"pos_pair"</code>.</li>
</ul>
<h2 id="proxyanchorloss">ProxyAnchorLoss<a class="headerlink" href="#proxyanchorloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/2003.13911.pdf" target="_blank">Proxy Anchor Loss for Deep Metric Learning</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">ProxyAnchorLoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">margin</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equation</strong>:</p>
<p><img alt="proxy_anchor_loss_equation" src="../imgs/proxy_anchor_loss_equation.png" style="height:150px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>margin</strong>: This is delta in the above equation. The paper uses 0.1.</li>
<li><strong>alpha</strong>: This is alpha in the above equation. The paper uses 32.</li>
</ul>
<p><strong>Other info</strong></p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts <code>weight_regularizer</code>, <code>weight_reg_weight</code>, and <code>weight_init_func</code> as optional arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">ProxyAnchorLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>


<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#divisorreducer">DivisorReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>pos_loss</strong>: The positive pair loss per proxy. Reduction type is <code>"element"</code>.</li>
<li><strong>neg_loss</strong>: The negative pair loss per proxy. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="proxyncaloss">ProxyNCALoss<a class="headerlink" href="#proxyncaloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1703.07464.pdf" target="_blank">No Fuss Distance Metric Learning using Proxies</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">ProxyNCALoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">softmax_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>softmax_scale</strong>: See <a href="./#ncaloss">NCALoss</a></li>
</ul>
<p><strong>Other info</strong></p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts <code>weight_regularizer</code>, <code>weight_reg_weight</code>, and <code>weight_init_func</code> as optional arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">ProxyNCALoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>


<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=2)</code></a></li>
</ul>
<p><strong>Default reducer</strong>:</p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch, that results in a non zero exponent in the cross entropy expression. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="signaltonoiseratiocontrastiveloss">SignalToNoiseRatioContrastiveLoss<a class="headerlink" href="#signaltonoiseratiocontrastiveloss" title="Permanent link">&para;</a></h2>
<p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yuan_Signal-To-Noise_Ratio_A_Robust_Distance_Metric_for_Deep_Metric_Learning_CVPR_2019_paper.pdf" target="_blank">Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">SignalToNoiseRatioContrastiveLoss</span><span class="p">(</span><span class="n">pos_margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">neg_margin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</code></pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>pos_margin</strong>: The noise-to-signal ratio over which positive pairs will contribute to the loss.</li>
<li><strong>neg_margin</strong>: The noise-to-signal ratio under which negative pairs will contribute to the loss.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#snrdistance"><code>SNRDistance()</code></a><ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#avgnonzeroreducer">AvgNonZeroReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>pos_loss</strong>: The loss per positive pair in the batch. Reduction type is <code>"pos_pair"</code>.</li>
<li><strong>neg_loss</strong>: The loss per negative pair in the batch. Reduction type is <code>"neg_pair"</code>.</li>
</ul>
<h2 id="softtripleloss">SoftTripleLoss<a class="headerlink" href="#softtripleloss" title="Permanent link">&para;</a></h2>
<p><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Qian_SoftTriple_Loss_Deep_Metric_Learning_Without_Triplet_Sampling_ICCV_2019_paper.pdf" target="_blank">SoftTriple Loss: Deep Metric Learning Without Triplet Sampling</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">SoftTripleLoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> 
                    <span class="n">embedding_size</span><span class="p">,</span> 
                    <span class="n">centers_per_class</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                    <span class="n">la</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
                    <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
                    <span class="n">margin</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equations</strong>:</p>
<p><img alt="soft_triple_loss_equation1" src="../imgs/soft_triple_loss_equation1.png" style="height:100px" /></p>
<p>where</p>
<p><img alt="soft_triple_loss_equation2" src="../imgs/soft_triple_loss_equation2.png" style="height:80px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>num_classes</strong>: The number of classes in your training dataset.</li>
<li><strong>embedding_size</strong>: The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set <code>embedding_size</code> to 512.</li>
<li><strong>centers_per_class</strong>: The number of weight vectors per class. (The regular cross entropy loss has 1 center per class.) The paper uses 10.</li>
<li><strong>la</strong>: This is lambda in the above equation.</li>
<li><strong>gamma</strong>: This is gamma in the above equation. The paper uses 0.1.</li>
<li><strong>margin</strong>: The is delta in the above equations. The paper uses 0.01.</li>
</ul>
<p><strong>Other info</strong></p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts <code>weight_regularizer</code>, <code>weight_reg_weight</code>, and <code>weight_init_func</code> as optional arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SoftTripleLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>


<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a><ul>
<li>The distance measure must be inverted. For example, <a href="../distances/#dotproductsimilarity"><code>DotProductSimilarity(normalize_embeddings=False)</code></a> is also compatible.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="spherefaceloss">SphereFaceLoss<a class="headerlink" href="#spherefaceloss" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/pdf/1704.08063.pdf" target="_blank">SphereFace: Deep Hypersphere Embedding for Face Recognition</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">SphereFaceLoss</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> 
                    <span class="n">embedding_size</span><span class="p">,</span> 
                    <span class="n">margin</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> 
                    <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Parameters</strong>:</p>
<p>See <a href="./#largemarginsoftmaxloss">LargeMarginSoftmaxLoss</a></p>
<p><strong>Other info</strong></p>
<ul>
<li>This also extends <a href="./#weightregularizermixin">WeightRegularizerMixin</a>, so it accepts <code>weight_regularizer</code>, <code>weight_reg_weight</code>, and <code>weight_init_func</code> as optional arguments.</li>
<li>This loss <strong>requires an optimizer</strong>. You need to create an optimizer and pass this loss's parameters to that optimizer. For example:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">loss_func</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SphereFaceLoss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="n">loss_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss_func</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># then during training:</span>
<span class="n">loss_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>


<p><strong>Default distance</strong>: </p>
<ul>
<li>
<p><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></p>
<ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="supconloss">SupConLoss<a class="headerlink" href="#supconloss" title="Permanent link">&para;</a></h2>
<p>Described in <a href="https://arxiv.org/abs/2004.11362" target="_blank">Supervised Contrastive Learning</a>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">SupConLoss</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equation</strong>:</p>
<p><img alt="supcon_loss_equation" src="../imgs/supcon_loss_equation.png" style="height:90px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>temperature</strong>: This is tau in the above equation. The paper uses 0.1.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#avgnonzeroreducer">AvgNonZeroReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per element in the batch. If an element has only negative pairs or no pairs, it's ignored thanks to <code>AvgNonZeroReducer</code>. Reduction type is <code>"element"</code>.</li>
</ul>
<h2 id="tripletmarginloss">TripletMarginLoss<a class="headerlink" href="#tripletmarginloss" title="Permanent link">&para;</a></h2>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">TripletMarginLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                        <span class="n">swap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">smooth_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">triplets_per_anchor</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span>
                        <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equation</strong>:</p>
<p><img alt="triplet_margin_loss_equation" src="../imgs/triplet_margin_loss_equation.png" style="height:35px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>margin</strong>: The desired difference between the anchor-positive distance and the anchor-negative distance. This is <code>m</code> in the above equation.</li>
<li><strong>swap</strong>: Use the positive-negative distance instead of anchor-negative distance, if it violates the margin more.</li>
<li><strong>smooth_loss</strong>: Use the log-exp version of the triplet loss</li>
<li><strong>triplets_per_anchor</strong>: The number of triplets per element to sample within a batch. Can be an integer or the string "all". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is "all", then all possible triplets in the batch will be used.</li>
</ul>
<p><strong>Default distance</strong>: </p>
<ul>
<li><a href="../distances/#lpdistance"><code>LpDistance(normalize_embeddings=True, p=2, power=1)</code></a></li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#avgnonzeroreducer">AvgNonZeroReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per triplet in the batch. Reduction type is <code>"triplet"</code>.</li>
</ul>
<h2 id="tupletmarginloss">TupletMarginLoss<a class="headerlink" href="#tupletmarginloss" title="Permanent link">&para;</a></h2>
<p><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Deep_Metric_Learning_With_Tuplet_Margin_Loss_ICCV_2019_paper.pdf" target="_blank">Deep Metric Learning with Tuplet Margin Loss</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">TupletMarginLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">5.73</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Equation</strong>:</p>
<p><img alt="tuplet_margin_loss_equation" src="../imgs/tuplet_margin_loss_equation.png" style="height:80px" /></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>margin</strong>: The angular margin (in degrees) applied to positive pairs. This is beta in the above equation. The paper uses a value of 5.73 degrees (0.1 radians).</li>
<li><strong>scale</strong>: This is <code>s</code> in the above equation.</li>
</ul>
<p>The paper combines this loss with <a href="./#intrapairvarianceloss">IntraPairVarianceLoss</a>. You can accomplish this by using <a href="./#multiplelosses">MultipleLosses</a>:</p>
<div class="codehilite"><pre><span></span><code><span class="n">main_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">TupletMarginLoss</span><span class="p">()</span>
<span class="n">var_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">IntraPairVarianceLoss</span><span class="p">()</span>
<span class="n">complete_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">MultipleLosses</span><span class="p">([</span><span class="n">main_loss</span><span class="p">,</span> <span class="n">var_loss</span><span class="p">],</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
</code></pre></div>


<p><strong>Default distance</strong>: </p>
<ul>
<li>
<p><a href="../distances/#cosinesimilarity"><code>CosineSimilarity()</code></a></p>
<ul>
<li>This is the only compatible distance.</li>
</ul>
</li>
</ul>
<p><strong>Default reducer</strong>: </p>
<ul>
<li><a href="../reducers/#meanreducer">MeanReducer</a></li>
</ul>
<p><strong>Reducer input</strong>:</p>
<ul>
<li><strong>loss</strong>: The loss per positive pair in the batch. Reduction type is <code>"pos_pair"</code>.</li>
</ul>
<h2 id="weightregularizermixin">WeightRegularizerMixin<a class="headerlink" href="#weightregularizermixin" title="Permanent link">&para;</a></h2>
<p>Losses can extend this class in addition to BaseMetricLossFunction. You should extend this class if your loss function contains a learnable weight matrix.</p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span><span class="o">.</span><span class="n">WeightRegularizerMixin</span><span class="p">(</span><span class="n">weight_init_func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weight_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weight_reg_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>weight_init_func</strong>: An <a href="../common_functions/#torchinitwrapper">TorchInitWrapper</a> object, which will be used to initialize the weights of the loss function.</li>
<li><strong>weight_regularizer</strong>: The <a href="../regularizers/">regularizer</a> to apply to the loss's learned weights.</li>
<li><strong>weight_reg_weight</strong>: The amount the regularization loss will be multiplied by.</li>
</ul>
<p>Extended by:</p>
<ul>
<li><a href="./#arcfaceloss">ArcFaceLoss</a></li>
<li><a href="./#cosfaceloss">CosFaceLoss</a></li>
<li><a href="./#largemarginsoftmaxloss">LargeMarginSoftmaxLoss</a></li>
<li><a href="./#normalizedsoftmaxloss">NormalizedSoftmaxLoss</a></li>
<li><a href="./#proxyanchorloss">ProxyAnchorLoss</a></li>
<li><a href="./#proxyncaloss">ProxyNCALoss</a></li>
<li><a href="./#softtripleloss">SoftTripleLoss</a></li>
<li><a href="./#spherefaceloss">SphereFaceLoss</a></li>
</ul>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../distances/" title="Distances" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Distances
              </div>
            </div>
          </a>
        
        
          <a href="../miners/" title="Miners" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Miners
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.92ffa368.min.js"></script>
      <script src="../assets/javascripts/bundle.5123e3d4.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: [],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.a68abb33.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
    
  </body>
</html>
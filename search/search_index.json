{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyTorch Metric Learning \u00b6 Google Colab Examples \u00b6 See the examples folder for notebooks you can download or run on Google Colab. Overview \u00b6 This library contains 9 modules, each of which can be used independently within your existing codebase, or combined together for a complete train/test workflow. How loss functions work \u00b6 Using losses and miners in your training loop \u00b6 Let\u2019s initialize a plain TripletMarginLoss : from pytorch_metric_learning import losses loss_func = losses . TripletMarginLoss () To compute the loss in your training loop, pass in the embeddings computed by your model, and the corresponding labels. The embeddings should have size (N, embedding_size), and the labels should have size (N), where N is the batch size. # your training loop for i , ( data , labels ) in enumerate ( dataloader ): optimizer . zero_grad () embeddings = model ( data ) loss = loss_func ( embeddings , labels ) loss . backward () optimizer . step () The TripletMarginLoss computes all possible triplets within the batch, based on the labels you pass into it. Anchor-positive pairs are formed by embeddings that share the same label, and anchor-negative pairs are formed by embeddings that have different labels. Sometimes it can help to add a mining function: from pytorch_metric_learning import miners , losses miner = miners . MultiSimilarityMiner () loss_func = losses . TripletMarginLoss () # your training loop for i , ( data , labels ) in enumerate ( dataloader ): optimizer . zero_grad () embeddings = model ( data ) hard_pairs = miner ( embeddings , labels ) loss = loss_func ( embeddings , labels , hard_pairs ) loss . backward () optimizer . step () In the above code, the miner finds positive and negative pairs that it thinks are particularly difficult. Note that even though the TripletMarginLoss operates on triplets, it\u2019s still possible to pass in pairs. This is because the library automatically converts pairs to triplets and triplets to pairs, when necessary. Customizing loss functions \u00b6 Loss functions can be customized using distances , reducers , and regularizers . In the diagram below, a miner finds the indices of hard pairs within a batch. These are used to index into the distance matrix, computed by the distance object. For this diagram, the loss function is pair-based, so it computes a loss per pair. In addition, a regularizer has been supplied, so a regularization loss is computed for each embedding in the batch. The per-pair and per-element losses are passed to the reducer, which (in this diagram) only keeps losses with a high value. The averages are computed for the high-valued pair and element losses, and are then added together to obtain the final loss. Now here's an example of a customized TripletMarginLoss: from pytorch_metric_learning.distances import CosineSimilarity from pytorch_metric_learning.reducers import ThresholdReducer from pytorch_metric_learning.regularizers import LpRegularizer from pytorch_metric_learning import losses loss_func = losses . TripletMarginLoss ( distance = CosineSimilarity (), reducer = ThresholdReducer ( high = 0.3 ), embedding_regularizer = LpRegularizer ()) This customized triplet loss has the following properties: The loss will be computed using cosine similarity instead of Euclidean distance. All triplet losses that are higher than 0.3 will be discarded. The embeddings will be L2 regularized. Using loss functions for unsupervised / self-supervised learning \u00b6 The TripletMarginLoss is an embedding-based or tuple-based loss. This means that internally, there is no real notion of \"classes\". Tuples (pairs or triplets) are formed at each iteration, based on the labels it receives. The labels don't have to represent classes. They simply need to indicate the positive and negative relationships between the embeddings. Thus, it is easy to use these loss functions for unsupervised or self-supervised learning. For example, the code below is a simplified version of the augmentation strategy commonly used in self-supervision. The dataset does not come with any labels. Instead, the labels are created in the training loop, solely to indicate which embeddings are positive pairs. # your training for-loop for i , data in enumerate ( dataloader ): optimizer . zero_grad () embeddings = your_model ( data ) augmented = your_model ( your_augmentation ( data )) labels = torch . arange ( embeddings . size ( 0 )) embeddings = torch . cat ([ embeddings , augmented ], dim = 0 ) labels = torch . cat ([ labels , labels ], dim = 0 ) loss = loss_func ( embeddings , labels ) loss . backward () optimizer . step () If you're interested in MoCo -style self-supervision, take a look at the MoCo on CIFAR10 notebook. It uses CrossBatchMemory to implement the momentum encoder queue, which means you can use any tuple loss, and any tuple miner to extract hard samples from the queue. Highlights of the rest of the library \u00b6 For a convenient way to train your model, take a look at the trainers . Want to test your model's accuracy on a dataset? Try the testers . To compute the accuracy of an embedding space directly, use AccuracyCalculator . If you're short of time and want a complete train/test workflow, check out the example Google Colab notebooks . Installation \u00b6 Required PyTorch version \u00b6 pytorch-metric-learning >= v0.9.90 requires torch >= 1.6 pytorch-metric-learning < v0.9.90 doesn't have a version requirement, but was tested with torch >= 1.2 Pip \u00b6 pip install pytorch-metric-learning To get the latest dev version : pip install pytorch-metric-learning --pre To install on Windows : pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html pip install pytorch-metric-learning To install with evaluation and logging capabilities (This will install the unofficial pypi version of faiss-gpu) : pip install pytorch-metric-learning[with-hooks] To install with evaluation and logging capabilities (CPU) (This will install the unofficial pypi version of faiss-cpu) : pip install pytorch-metric-learning[with-hooks-cpu] Conda \u00b6 conda install pytorch-metric-learning -c metric-learning -c pytorch To use the testing module, you'll need faiss, which can be installed via conda as well. See the installation instructions for faiss .","title":"Home"},{"location":"#pytorch-metric-learning","text":"","title":"PyTorch Metric Learning"},{"location":"#google-colab-examples","text":"See the examples folder for notebooks you can download or run on Google Colab.","title":"Google Colab Examples"},{"location":"#overview","text":"This library contains 9 modules, each of which can be used independently within your existing codebase, or combined together for a complete train/test workflow.","title":"Overview"},{"location":"#how-loss-functions-work","text":"","title":"How loss functions work"},{"location":"#using-losses-and-miners-in-your-training-loop","text":"Let\u2019s initialize a plain TripletMarginLoss : from pytorch_metric_learning import losses loss_func = losses . TripletMarginLoss () To compute the loss in your training loop, pass in the embeddings computed by your model, and the corresponding labels. The embeddings should have size (N, embedding_size), and the labels should have size (N), where N is the batch size. # your training loop for i , ( data , labels ) in enumerate ( dataloader ): optimizer . zero_grad () embeddings = model ( data ) loss = loss_func ( embeddings , labels ) loss . backward () optimizer . step () The TripletMarginLoss computes all possible triplets within the batch, based on the labels you pass into it. Anchor-positive pairs are formed by embeddings that share the same label, and anchor-negative pairs are formed by embeddings that have different labels. Sometimes it can help to add a mining function: from pytorch_metric_learning import miners , losses miner = miners . MultiSimilarityMiner () loss_func = losses . TripletMarginLoss () # your training loop for i , ( data , labels ) in enumerate ( dataloader ): optimizer . zero_grad () embeddings = model ( data ) hard_pairs = miner ( embeddings , labels ) loss = loss_func ( embeddings , labels , hard_pairs ) loss . backward () optimizer . step () In the above code, the miner finds positive and negative pairs that it thinks are particularly difficult. Note that even though the TripletMarginLoss operates on triplets, it\u2019s still possible to pass in pairs. This is because the library automatically converts pairs to triplets and triplets to pairs, when necessary.","title":"Using losses and miners in your training loop"},{"location":"#customizing-loss-functions","text":"Loss functions can be customized using distances , reducers , and regularizers . In the diagram below, a miner finds the indices of hard pairs within a batch. These are used to index into the distance matrix, computed by the distance object. For this diagram, the loss function is pair-based, so it computes a loss per pair. In addition, a regularizer has been supplied, so a regularization loss is computed for each embedding in the batch. The per-pair and per-element losses are passed to the reducer, which (in this diagram) only keeps losses with a high value. The averages are computed for the high-valued pair and element losses, and are then added together to obtain the final loss. Now here's an example of a customized TripletMarginLoss: from pytorch_metric_learning.distances import CosineSimilarity from pytorch_metric_learning.reducers import ThresholdReducer from pytorch_metric_learning.regularizers import LpRegularizer from pytorch_metric_learning import losses loss_func = losses . TripletMarginLoss ( distance = CosineSimilarity (), reducer = ThresholdReducer ( high = 0.3 ), embedding_regularizer = LpRegularizer ()) This customized triplet loss has the following properties: The loss will be computed using cosine similarity instead of Euclidean distance. All triplet losses that are higher than 0.3 will be discarded. The embeddings will be L2 regularized.","title":"Customizing loss functions"},{"location":"#using-loss-functions-for-unsupervised-self-supervised-learning","text":"The TripletMarginLoss is an embedding-based or tuple-based loss. This means that internally, there is no real notion of \"classes\". Tuples (pairs or triplets) are formed at each iteration, based on the labels it receives. The labels don't have to represent classes. They simply need to indicate the positive and negative relationships between the embeddings. Thus, it is easy to use these loss functions for unsupervised or self-supervised learning. For example, the code below is a simplified version of the augmentation strategy commonly used in self-supervision. The dataset does not come with any labels. Instead, the labels are created in the training loop, solely to indicate which embeddings are positive pairs. # your training for-loop for i , data in enumerate ( dataloader ): optimizer . zero_grad () embeddings = your_model ( data ) augmented = your_model ( your_augmentation ( data )) labels = torch . arange ( embeddings . size ( 0 )) embeddings = torch . cat ([ embeddings , augmented ], dim = 0 ) labels = torch . cat ([ labels , labels ], dim = 0 ) loss = loss_func ( embeddings , labels ) loss . backward () optimizer . step () If you're interested in MoCo -style self-supervision, take a look at the MoCo on CIFAR10 notebook. It uses CrossBatchMemory to implement the momentum encoder queue, which means you can use any tuple loss, and any tuple miner to extract hard samples from the queue.","title":"Using loss functions for unsupervised / self-supervised learning"},{"location":"#highlights-of-the-rest-of-the-library","text":"For a convenient way to train your model, take a look at the trainers . Want to test your model's accuracy on a dataset? Try the testers . To compute the accuracy of an embedding space directly, use AccuracyCalculator . If you're short of time and want a complete train/test workflow, check out the example Google Colab notebooks .","title":"Highlights of the rest of the library"},{"location":"#installation","text":"","title":"Installation"},{"location":"#required-pytorch-version","text":"pytorch-metric-learning >= v0.9.90 requires torch >= 1.6 pytorch-metric-learning < v0.9.90 doesn't have a version requirement, but was tested with torch >= 1.2","title":"Required PyTorch version"},{"location":"#pip","text":"pip install pytorch-metric-learning To get the latest dev version : pip install pytorch-metric-learning --pre To install on Windows : pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html pip install pytorch-metric-learning To install with evaluation and logging capabilities (This will install the unofficial pypi version of faiss-gpu) : pip install pytorch-metric-learning[with-hooks] To install with evaluation and logging capabilities (CPU) (This will install the unofficial pypi version of faiss-cpu) : pip install pytorch-metric-learning[with-hooks-cpu]","title":"Pip"},{"location":"#conda","text":"conda install pytorch-metric-learning -c metric-learning -c pytorch To use the testing module, you'll need faiss, which can be installed via conda as well. See the installation instructions for faiss .","title":"Conda"},{"location":"accuracy_calculation/","text":"Accuracy Calculation \u00b6 The AccuracyCalculator class computes several accuracy metrics given a query and reference embeddings. It can be easily extended to create custom accuracy metrics. from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator AccuracyCalculator ( include = (), exclude = (), avg_of_avgs = False , return_per_class = False , k = None , label_comparison_fn = None , device = None , knn_func = None , kmeans_func = None ) Parameters \u00b6 include : Optional. A list or tuple of strings, which are the names of metrics you want to calculate. If left empty, all default metrics will be calculated. exclude : Optional. A list or tuple of strings, which are the names of metrics you do not want to calculate. avg_of_avgs : If True, the average accuracy per class is computed, and then the average of those averages is returned. This can be useful if your dataset has unbalanced classes. If False, the global average will be returned. return_per_class : If True, the average accuracy per class is computed and returned. k : The number of nearest neighbors that will be retrieved for metrics that require k-nearest neighbors. The allowed values are: None . This means k will be set to the total number of reference embeddings. An integer greater than 0. This means k will be set to the input integer. \"max_bin_count\" . This means k will be set to max(bincount(reference_labels)) - self_count where self_count == 1 if the query and reference embeddings come from the same source. label_comparison_fn : A function that compares two torch arrays of labels and returns a boolean array. The default is torch.eq . If a custom function is used, then you must exclude clustering based metrics (\"NMI\" and \"AMI\"). The following is an example of a custom function for two-dimensional labels. It returns True if the 0th column matches, and the 1st column does not match: device : The device to move input tensors to. If None , will default to GPUs if available. knn_func : A callable that takes in 4 arguments ( query, k, reference, embeddings_come_from_same_source ) and returns distances, indices . Default is pytorch_metric_learning.utils.inference.FaissKNN . kmeans_func : A callable that takes in 2 arguments ( x, nmb_clusters ) and returns a 1-d tensor of cluster assignments. Default is pytorch_metric_learning.utils.inference.FaissKMeans . from pytorch_metric_learning.distances import SNRDistance from pytorch_metric_learning.utils.inference import CustomKNN def example_label_comparison_fn ( x , y ): return ( x [:, 0 ] == y [:, 0 ]) & ( x [:, 1 ] != y [:, 1 ]) knn_func = CustomKNN ( SNRDistance ()) AccuracyCalculator ( exclude = ( \"NMI\" , \"AMI\" ), label_comparison_fn = example_label_comparison_fn , knn_func = knn_func ) Getting accuracy \u00b6 Call the get_accuracy method to obtain a dictionary of accuracies. def get_accuracy ( self , query , reference , query_labels , reference_labels , embeddings_come_from_same_source , include = (), exclude = () ): # returns a dictionary mapping from metric names to accuracy values # The default metrics are: # \"NMI\" (Normalized Mutual Information) # \"AMI\" (Adjusted Mutual Information) # \"precision_at_1\" # \"r_precision\" # \"mean_average_precision_at_r\" query : A 2D torch or numpy array of size (Nq, D) , where Nq is the number of query samples. For each query sample, nearest neighbors are retrieved and accuracy is computed. reference : A 2D torch or numpy array of size (Nr, D) , where Nr is the number of reference samples. This is where nearest neighbors are retrieved from. query_labels : A 1D torch or numpy array of size (Nq) . Each element should be an integer representing the sample's label. reference_labels : A 1D torch or numpy array of size (Nr) . Each element should be an integer representing the sample's label. embeddings_come_from_same_source : Set to True if query is a subset of reference or if query is reference . Set to False otherwise. include : Optional. A list or tuple of strings, which are the names of metrics you want to calculate. If left empty, all metrics specified during initialization will be calculated. exclude : Optional. A list or tuple of strings, which are the names of metrics you do not want to calculate. Note that labels can be 2D if a custom label comparison function is used. Lone query labels \u00b6 If some query labels don't appear in the reference set, then it's impossible for those labels to have non-zero k-nn accuracy. Zero accuracy for these labels doesn't indicate anything about the quality of the embedding space. So these lone query labels are excluded from k-nn based accuracy calculations. For example, if the input query_labels is [0,0,1,1] and reference_labels is [1,1,1,2,2] , then 0 is considered a lone query label. CPU/GPU usage \u00b6 If you installed faiss-cpu then the CPU will always be used. If you installed faiss-gpu , then the GPU will be used if k <= 1024 for CUDA < 9.5, and k <= 2048 for CUDA >= 9.5. If this condition is not met, then the CPU will be used. If your dataset is large, you might find the k-nn search is very slow. This is because the default behavior is to set k to len(reference_embeddings) . To avoid this, you can set k to a number, like k = 1000 , or try k = \"max_bin_count\" . Explanations of the default accuracy metrics \u00b6 AMI : scikit-learn article Wikipedia NMI : scikit-learn article Slides from Northeastern University mean_average_precision : Slides from Stanford mean_average_precision_at_r : See section 3.2 of A Metric Learning Reality Check mean_reciprocal_rank : Slides from Stanford precision_at_1 : Fancy way of saying \"is the 1st nearest neighbor correct?\" r_precision : See chapter 8 (page 161) of Introduction to Information Retrieval Important note AccuracyCalculator's mean_average_precision_at_r and r_precision are correct only if k = None , or k = \"max_bin_count\" , or k >= max(bincount(reference_labels)) Adding custom accuracy metrics \u00b6 Let's say you want to use the existing metrics but also compute precision @ 2, and a fancy mutual info method. You can extend the existing class, and write methods that start with the keyword calculate_ from pytorch_metric_learning.utils import accuracy_calculator class YourCalculator ( accuracy_calculator . AccuracyCalculator ): def calculate_precision_at_2 ( self , knn_labels , query_labels , ** kwargs ): return accuracy_calculator . precision_at_k ( knn_labels , query_labels [:, None ], 2 ) def calculate_fancy_mutual_info ( self , query_labels , cluster_labels , ** kwargs ): return fancy_computations def requires_clustering ( self ): return super () . requires_clustering () + [ \"fancy_mutual_info\" ] def requires_knn ( self ): return super () . requires_knn () + [ \"precision_at_2\" ] Any method that starts with \"calculate_\" will be passed the following kwargs: kwargs = { \"query\" : query , # query embeddings \"reference\" : reference , # reference embeddings \"query_labels\" : query_labels , \"reference_labels\" : reference_labels , \"embeddings_come_from_same_source\" : e } # True if query is reference, or if query is a subset of reference. If your method requires a k-nearest neighbors search, then append your method's name to the requires_knn list, as shown in the above example. If any of your accuracy methods require k-nearest neighbors, they will also receive the following kwargs: { \"label_counts\" : label_counts , # A dictionary mapping from reference labels to the number of times they occur \"knn_labels\" : knn_labels , # A 2d array where each row is the labels of the nearest neighbors of each query. The neighbors are retrieved from the reference set \"knn_distances\" : knn_distances # The euclidean distance corresponding to each k-nearest neighbor in knn_labels \"lone_query_labels\" : lone_query_labels # The set of labels (in the form of a torch array) that have only 1 occurrence in reference_labels \"not_lone_query_mask\" : not_lone_query_mask } # A boolean mask, where True means that a query element has at least 1 possible neighbor in reference. If your method requires cluster labels, then append your method's name to the requires_clustering list, as shown in the above example. Then, if any of your methods need cluster labels, self.get_cluster_labels() will be called, and the kwargs will include: { \"cluster_labels\" : cluster_labels } # A 1D array with a cluster label for each element in the query embeddings. Now when get_accuracy is called, the returned dictionary will contain precision_at_2 and fancy_mutual_info : calculator = YourCalculator () acc_dict = calculator . get_accuracy ( query_embeddings , reference_embeddings , query_labels , reference_labels , embeddings_come_from_same_source = True ) # Now acc_dict contains the metrics \"precision_at_2\" and \"fancy_mutual_info\" # in addition to the original metrics from AccuracyCalculator You can use your custom calculator with the tester classes as well, by passing it in as an init argument. (By default, the testers use AccuracyCalculator.) from pytorch_metric_learning import testers t = testers . GlobalEmbeddingSpaceTester ( ... , accuracy_calculator = YourCalculator ()) Using a custom label comparison function \u00b6 If you define your own label_comparison_fn , then query_labels and reference_labels can be 1D or 2D, and consist of integers or floating point numbers, as long as your label_comparison_fn can process them. Example of 2D labels: def label_comparison_fn ( x , y ): return ( x [ ... , 0 ] == y [ ... , 0 ]) & ( x [ ... , 1 ] != y [ ... , 1 ]) # these are valid labels labels = torch . tensor ([ ( 1 , 3 ), ( 7 , 4 ), ( 1 , 4 ), ( 1 , 5 ), ( 1 , 6 ), ]) Example of floating point labels: def label_comparison_fn ( x , y ): return torch . abs ( x - y ) < 1 # these are valid labels labels = torch . tensor ([ 10.0 , 0.03 , 0.04 , 0.05 , ]) Warning for versions <= 0.9.97 \u00b6 The behavior of the k parameter described in the Parameters section is for versions >= 0.9.98. For versions <= 0.9.97, the behavior was: If k = None , then k = min(1023, max(bincount(reference_labels))) Otherwise k = k","title":"Accuracy Calculation"},{"location":"accuracy_calculation/#accuracy-calculation","text":"The AccuracyCalculator class computes several accuracy metrics given a query and reference embeddings. It can be easily extended to create custom accuracy metrics. from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator AccuracyCalculator ( include = (), exclude = (), avg_of_avgs = False , return_per_class = False , k = None , label_comparison_fn = None , device = None , knn_func = None , kmeans_func = None )","title":"Accuracy Calculation"},{"location":"accuracy_calculation/#parameters","text":"include : Optional. A list or tuple of strings, which are the names of metrics you want to calculate. If left empty, all default metrics will be calculated. exclude : Optional. A list or tuple of strings, which are the names of metrics you do not want to calculate. avg_of_avgs : If True, the average accuracy per class is computed, and then the average of those averages is returned. This can be useful if your dataset has unbalanced classes. If False, the global average will be returned. return_per_class : If True, the average accuracy per class is computed and returned. k : The number of nearest neighbors that will be retrieved for metrics that require k-nearest neighbors. The allowed values are: None . This means k will be set to the total number of reference embeddings. An integer greater than 0. This means k will be set to the input integer. \"max_bin_count\" . This means k will be set to max(bincount(reference_labels)) - self_count where self_count == 1 if the query and reference embeddings come from the same source. label_comparison_fn : A function that compares two torch arrays of labels and returns a boolean array. The default is torch.eq . If a custom function is used, then you must exclude clustering based metrics (\"NMI\" and \"AMI\"). The following is an example of a custom function for two-dimensional labels. It returns True if the 0th column matches, and the 1st column does not match: device : The device to move input tensors to. If None , will default to GPUs if available. knn_func : A callable that takes in 4 arguments ( query, k, reference, embeddings_come_from_same_source ) and returns distances, indices . Default is pytorch_metric_learning.utils.inference.FaissKNN . kmeans_func : A callable that takes in 2 arguments ( x, nmb_clusters ) and returns a 1-d tensor of cluster assignments. Default is pytorch_metric_learning.utils.inference.FaissKMeans . from pytorch_metric_learning.distances import SNRDistance from pytorch_metric_learning.utils.inference import CustomKNN def example_label_comparison_fn ( x , y ): return ( x [:, 0 ] == y [:, 0 ]) & ( x [:, 1 ] != y [:, 1 ]) knn_func = CustomKNN ( SNRDistance ()) AccuracyCalculator ( exclude = ( \"NMI\" , \"AMI\" ), label_comparison_fn = example_label_comparison_fn , knn_func = knn_func )","title":"Parameters"},{"location":"accuracy_calculation/#getting-accuracy","text":"Call the get_accuracy method to obtain a dictionary of accuracies. def get_accuracy ( self , query , reference , query_labels , reference_labels , embeddings_come_from_same_source , include = (), exclude = () ): # returns a dictionary mapping from metric names to accuracy values # The default metrics are: # \"NMI\" (Normalized Mutual Information) # \"AMI\" (Adjusted Mutual Information) # \"precision_at_1\" # \"r_precision\" # \"mean_average_precision_at_r\" query : A 2D torch or numpy array of size (Nq, D) , where Nq is the number of query samples. For each query sample, nearest neighbors are retrieved and accuracy is computed. reference : A 2D torch or numpy array of size (Nr, D) , where Nr is the number of reference samples. This is where nearest neighbors are retrieved from. query_labels : A 1D torch or numpy array of size (Nq) . Each element should be an integer representing the sample's label. reference_labels : A 1D torch or numpy array of size (Nr) . Each element should be an integer representing the sample's label. embeddings_come_from_same_source : Set to True if query is a subset of reference or if query is reference . Set to False otherwise. include : Optional. A list or tuple of strings, which are the names of metrics you want to calculate. If left empty, all metrics specified during initialization will be calculated. exclude : Optional. A list or tuple of strings, which are the names of metrics you do not want to calculate. Note that labels can be 2D if a custom label comparison function is used.","title":"Getting accuracy"},{"location":"accuracy_calculation/#lone-query-labels","text":"If some query labels don't appear in the reference set, then it's impossible for those labels to have non-zero k-nn accuracy. Zero accuracy for these labels doesn't indicate anything about the quality of the embedding space. So these lone query labels are excluded from k-nn based accuracy calculations. For example, if the input query_labels is [0,0,1,1] and reference_labels is [1,1,1,2,2] , then 0 is considered a lone query label.","title":"Lone query labels"},{"location":"accuracy_calculation/#cpugpu-usage","text":"If you installed faiss-cpu then the CPU will always be used. If you installed faiss-gpu , then the GPU will be used if k <= 1024 for CUDA < 9.5, and k <= 2048 for CUDA >= 9.5. If this condition is not met, then the CPU will be used. If your dataset is large, you might find the k-nn search is very slow. This is because the default behavior is to set k to len(reference_embeddings) . To avoid this, you can set k to a number, like k = 1000 , or try k = \"max_bin_count\" .","title":"CPU/GPU usage"},{"location":"accuracy_calculation/#explanations-of-the-default-accuracy-metrics","text":"AMI : scikit-learn article Wikipedia NMI : scikit-learn article Slides from Northeastern University mean_average_precision : Slides from Stanford mean_average_precision_at_r : See section 3.2 of A Metric Learning Reality Check mean_reciprocal_rank : Slides from Stanford precision_at_1 : Fancy way of saying \"is the 1st nearest neighbor correct?\" r_precision : See chapter 8 (page 161) of Introduction to Information Retrieval Important note AccuracyCalculator's mean_average_precision_at_r and r_precision are correct only if k = None , or k = \"max_bin_count\" , or k >= max(bincount(reference_labels))","title":"Explanations of the default accuracy metrics"},{"location":"accuracy_calculation/#adding-custom-accuracy-metrics","text":"Let's say you want to use the existing metrics but also compute precision @ 2, and a fancy mutual info method. You can extend the existing class, and write methods that start with the keyword calculate_ from pytorch_metric_learning.utils import accuracy_calculator class YourCalculator ( accuracy_calculator . AccuracyCalculator ): def calculate_precision_at_2 ( self , knn_labels , query_labels , ** kwargs ): return accuracy_calculator . precision_at_k ( knn_labels , query_labels [:, None ], 2 ) def calculate_fancy_mutual_info ( self , query_labels , cluster_labels , ** kwargs ): return fancy_computations def requires_clustering ( self ): return super () . requires_clustering () + [ \"fancy_mutual_info\" ] def requires_knn ( self ): return super () . requires_knn () + [ \"precision_at_2\" ] Any method that starts with \"calculate_\" will be passed the following kwargs: kwargs = { \"query\" : query , # query embeddings \"reference\" : reference , # reference embeddings \"query_labels\" : query_labels , \"reference_labels\" : reference_labels , \"embeddings_come_from_same_source\" : e } # True if query is reference, or if query is a subset of reference. If your method requires a k-nearest neighbors search, then append your method's name to the requires_knn list, as shown in the above example. If any of your accuracy methods require k-nearest neighbors, they will also receive the following kwargs: { \"label_counts\" : label_counts , # A dictionary mapping from reference labels to the number of times they occur \"knn_labels\" : knn_labels , # A 2d array where each row is the labels of the nearest neighbors of each query. The neighbors are retrieved from the reference set \"knn_distances\" : knn_distances # The euclidean distance corresponding to each k-nearest neighbor in knn_labels \"lone_query_labels\" : lone_query_labels # The set of labels (in the form of a torch array) that have only 1 occurrence in reference_labels \"not_lone_query_mask\" : not_lone_query_mask } # A boolean mask, where True means that a query element has at least 1 possible neighbor in reference. If your method requires cluster labels, then append your method's name to the requires_clustering list, as shown in the above example. Then, if any of your methods need cluster labels, self.get_cluster_labels() will be called, and the kwargs will include: { \"cluster_labels\" : cluster_labels } # A 1D array with a cluster label for each element in the query embeddings. Now when get_accuracy is called, the returned dictionary will contain precision_at_2 and fancy_mutual_info : calculator = YourCalculator () acc_dict = calculator . get_accuracy ( query_embeddings , reference_embeddings , query_labels , reference_labels , embeddings_come_from_same_source = True ) # Now acc_dict contains the metrics \"precision_at_2\" and \"fancy_mutual_info\" # in addition to the original metrics from AccuracyCalculator You can use your custom calculator with the tester classes as well, by passing it in as an init argument. (By default, the testers use AccuracyCalculator.) from pytorch_metric_learning import testers t = testers . GlobalEmbeddingSpaceTester ( ... , accuracy_calculator = YourCalculator ())","title":"Adding custom accuracy metrics"},{"location":"accuracy_calculation/#using-a-custom-label-comparison-function","text":"If you define your own label_comparison_fn , then query_labels and reference_labels can be 1D or 2D, and consist of integers or floating point numbers, as long as your label_comparison_fn can process them. Example of 2D labels: def label_comparison_fn ( x , y ): return ( x [ ... , 0 ] == y [ ... , 0 ]) & ( x [ ... , 1 ] != y [ ... , 1 ]) # these are valid labels labels = torch . tensor ([ ( 1 , 3 ), ( 7 , 4 ), ( 1 , 4 ), ( 1 , 5 ), ( 1 , 6 ), ]) Example of floating point labels: def label_comparison_fn ( x , y ): return torch . abs ( x - y ) < 1 # these are valid labels labels = torch . tensor ([ 10.0 , 0.03 , 0.04 , 0.05 , ])","title":"Using a custom label comparison function"},{"location":"accuracy_calculation/#warning-for-versions-0997","text":"The behavior of the k parameter described in the Parameters section is for versions >= 0.9.98. For versions <= 0.9.97, the behavior was: If k = None , then k = min(1023, max(bincount(reference_labels))) Otherwise k = k","title":"Warning for versions &lt;= 0.9.97"},{"location":"common_functions/","text":"Common Functions \u00b6 LOGGER \u00b6 This is the logger that is used everywhere in this library. from pytorch_metric_learning.utils import common_functions as c_f c_f . LOGGER . info ( \"Using the PML logger\" ) LOGGER_NAME \u00b6 The default logger name is \"PML\" . You can set the logging level for just this library: import logging from pytorch_metric_learning.utils import common_functions as c_f logging . basicConfig () logging . getLogger ( c_f . LOGGER_NAME ) . setLevel ( logging . INFO ) set_logger_name \u00b6 Allows you to change LOGGER_NAME from pytorch_metric_learning.utils import common_functions as c_f c_f . set_logger_name ( \"DOGS\" ) c_f . LOGGER . info ( \"Hello\" ) # prints INFO:DOGS:Hello COLLECT_STATS \u00b6 Default value is False . This is used by all distances, losses, miners, reducers, and regularizers. Set this to True if you want to turn on all statistics collection. from pytorch_metric_learning.utils import common_functions as c_f c_f . COLLECT_STATS = True NUMPY_RANDOM \u00b6 Default value is np.random . This is used anytime a numpy random function is needed. You can set it to something else if you want import numpy as np from pytorch_metric_learning.utils import common_functions as c_f c_f . NUMPY_RANDOM = np . random . RandomState ( 42 ) TorchInitWrapper \u00b6 A simpler wrapper to convert the torch weight initialization functions into class form, which can then be applied within loss functions. Example usage: from pytorch_metric_learning.utils import common_functions as c_f import torch # use kaiming_uniform, with a=1 and mode='fan_out' weight_init_func = c_f . TorchInitWrapper ( torch . nn . kaiming_uniform_ , a = 1 , mode = 'fan_out' ) loss_func = SomeClassificationLoss ( ... , weight_init_func = weight_init_func )","title":"Common Functions"},{"location":"common_functions/#common-functions","text":"","title":"Common Functions"},{"location":"common_functions/#logger","text":"This is the logger that is used everywhere in this library. from pytorch_metric_learning.utils import common_functions as c_f c_f . LOGGER . info ( \"Using the PML logger\" )","title":"LOGGER"},{"location":"common_functions/#logger_name","text":"The default logger name is \"PML\" . You can set the logging level for just this library: import logging from pytorch_metric_learning.utils import common_functions as c_f logging . basicConfig () logging . getLogger ( c_f . LOGGER_NAME ) . setLevel ( logging . INFO )","title":"LOGGER_NAME"},{"location":"common_functions/#set_logger_name","text":"Allows you to change LOGGER_NAME from pytorch_metric_learning.utils import common_functions as c_f c_f . set_logger_name ( \"DOGS\" ) c_f . LOGGER . info ( \"Hello\" ) # prints INFO:DOGS:Hello","title":"set_logger_name"},{"location":"common_functions/#collect_stats","text":"Default value is False . This is used by all distances, losses, miners, reducers, and regularizers. Set this to True if you want to turn on all statistics collection. from pytorch_metric_learning.utils import common_functions as c_f c_f . COLLECT_STATS = True","title":"COLLECT_STATS"},{"location":"common_functions/#numpy_random","text":"Default value is np.random . This is used anytime a numpy random function is needed. You can set it to something else if you want import numpy as np from pytorch_metric_learning.utils import common_functions as c_f c_f . NUMPY_RANDOM = np . random . RandomState ( 42 )","title":"NUMPY_RANDOM"},{"location":"common_functions/#torchinitwrapper","text":"A simpler wrapper to convert the torch weight initialization functions into class form, which can then be applied within loss functions. Example usage: from pytorch_metric_learning.utils import common_functions as c_f import torch # use kaiming_uniform, with a=1 and mode='fan_out' weight_init_func = c_f . TorchInitWrapper ( torch . nn . kaiming_uniform_ , a = 1 , mode = 'fan_out' ) loss_func = SomeClassificationLoss ( ... , weight_init_func = weight_init_func )","title":"TorchInitWrapper"},{"location":"distances/","text":"Distances \u00b6 Distance classes compute pairwise distances/similarities between input embeddings. Consider the TripletMarginLoss in its default form: from pytorch_metric_learning.losses import TripletMarginLoss loss_func = TripletMarginLoss ( margin = 0.2 ) This loss function attempts to minimize [d ap - d an + margin] + . Typically, d ap and d an represent Euclidean or L2 distances. But what if we want to use a squared L2 distance, or an unnormalized L1 distance, or a completely different distance measure like signal-to-noise ratio? With the distances module, you can try out these ideas easily: ### TripletMarginLoss with squared L2 distance ### from pytorch_metric_learning.distances import LpDistance loss_func = TripletMarginLoss ( margin = 0.2 , distance = LpDistance ( power = 2 )) ### TripletMarginLoss with unnormalized L1 distance ### loss_func = TripletMarginLoss ( margin = 0.2 , distance = LpDistance ( normalize_embeddings = False , p = 1 )) ### TripletMarginLoss with signal-to-noise ratio### from pytorch_metric_learning.distances import SNRDistance loss_func = TripletMarginLoss ( margin = 0.2 , distance = SNRDistance ()) You can also use similarity measures rather than distances, and the loss function will make the necessary adjustments: ### TripletMarginLoss with cosine similarity## from pytorch_metric_learning.distances import CosineSimilarity loss_func = TripletMarginLoss ( margin = 0.2 , distance = CosineSimilarity ()) With a similarity measure, the TripletMarginLoss internally swaps the anchor-positive and anchor-negative terms: [s an - s ap + margin] + . In other words, it will try to make the anchor-negative similarities smaller than the anchor-positive similarities. All losses, miners, and regularizers accept a distance argument. So you can try out the MultiSimilarityMiner using SNRDistance , or the NTXentLoss using LpDistance(p=1) and so on. Note that some losses/miners/regularizers have restrictions on the type of distances they can accept. For example, some classification losses only allow CosineSimilarity or DotProductSimilarity as their distance measure between embeddings and weights. To view restrictions for specific loss functions, see the losses page BaseDistance \u00b6 All distances extend this class and therefore inherit its __init__ parameters. distances . BaseDistance ( collect_stats = False , normalize_embeddings = True , p = 2 , power = 1 , is_inverted = False ) Parameters : collect_stats : If True, will collect various statistics that may be useful to analyze during experiments. If False, these computations will be skipped. Want to make True the default? Set the global COLLECT_STATS flag. normalize_embeddings : If True, embeddings will be normalized to have an Lp norm of 1, before the distance/similarity matrix is computed. p : The distance norm. power : If not 1, each element of the distance/similarity matrix will be raised to this power. is_inverted : Should be set by child classes. If False, then small values represent embeddings that are close together. If True, then large values represent embeddings that are similar to each other. Required Implementations : # Must return a matrix where mat[j,k] represents # the distance/similarity between query_emb[j] and ref_emb[k] def compute_mat ( self , query_emb , ref_emb ): raise NotImplementedError # Must return a tensor where output[j] represents # the distance/similarity between query_emb[j] and ref_emb[j] def pairwise_distance ( self , query_emb , ref_emb ): raise NotImplementedError CosineSimilarity \u00b6 distances . CosineSimilarity ( ** kwargs ) The returned mat[i,j] is the cosine similarity between query_emb[i] and ref_emb[j] . This class is equivalent to DotProductSimilarity(normalize_embeddings=True) . DotProductSimilarity \u00b6 distances . DotProductSimilarity ( ** kwargs ) The returned mat[i,j] is equal to torch.sum(query_emb[i] * ref_emb[j]) LpDistance \u00b6 distances . LpDistance ( ** kwargs ) The returned mat[i,j] is the Lp distance between query_emb[i] and ref_emb[j] . With default parameters, this is the Euclidean distance. SNRDistance \u00b6 Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning distances . SNRDistance ( ** kwargs ) The returned mat[i,j] is equal to: torch . var ( query_emb [ i ] - ref_emb [ j ]) / torch . var ( query_emb [ i ])","title":"Distances"},{"location":"distances/#distances","text":"Distance classes compute pairwise distances/similarities between input embeddings. Consider the TripletMarginLoss in its default form: from pytorch_metric_learning.losses import TripletMarginLoss loss_func = TripletMarginLoss ( margin = 0.2 ) This loss function attempts to minimize [d ap - d an + margin] + . Typically, d ap and d an represent Euclidean or L2 distances. But what if we want to use a squared L2 distance, or an unnormalized L1 distance, or a completely different distance measure like signal-to-noise ratio? With the distances module, you can try out these ideas easily: ### TripletMarginLoss with squared L2 distance ### from pytorch_metric_learning.distances import LpDistance loss_func = TripletMarginLoss ( margin = 0.2 , distance = LpDistance ( power = 2 )) ### TripletMarginLoss with unnormalized L1 distance ### loss_func = TripletMarginLoss ( margin = 0.2 , distance = LpDistance ( normalize_embeddings = False , p = 1 )) ### TripletMarginLoss with signal-to-noise ratio### from pytorch_metric_learning.distances import SNRDistance loss_func = TripletMarginLoss ( margin = 0.2 , distance = SNRDistance ()) You can also use similarity measures rather than distances, and the loss function will make the necessary adjustments: ### TripletMarginLoss with cosine similarity## from pytorch_metric_learning.distances import CosineSimilarity loss_func = TripletMarginLoss ( margin = 0.2 , distance = CosineSimilarity ()) With a similarity measure, the TripletMarginLoss internally swaps the anchor-positive and anchor-negative terms: [s an - s ap + margin] + . In other words, it will try to make the anchor-negative similarities smaller than the anchor-positive similarities. All losses, miners, and regularizers accept a distance argument. So you can try out the MultiSimilarityMiner using SNRDistance , or the NTXentLoss using LpDistance(p=1) and so on. Note that some losses/miners/regularizers have restrictions on the type of distances they can accept. For example, some classification losses only allow CosineSimilarity or DotProductSimilarity as their distance measure between embeddings and weights. To view restrictions for specific loss functions, see the losses page","title":"Distances"},{"location":"distances/#basedistance","text":"All distances extend this class and therefore inherit its __init__ parameters. distances . BaseDistance ( collect_stats = False , normalize_embeddings = True , p = 2 , power = 1 , is_inverted = False ) Parameters : collect_stats : If True, will collect various statistics that may be useful to analyze during experiments. If False, these computations will be skipped. Want to make True the default? Set the global COLLECT_STATS flag. normalize_embeddings : If True, embeddings will be normalized to have an Lp norm of 1, before the distance/similarity matrix is computed. p : The distance norm. power : If not 1, each element of the distance/similarity matrix will be raised to this power. is_inverted : Should be set by child classes. If False, then small values represent embeddings that are close together. If True, then large values represent embeddings that are similar to each other. Required Implementations : # Must return a matrix where mat[j,k] represents # the distance/similarity between query_emb[j] and ref_emb[k] def compute_mat ( self , query_emb , ref_emb ): raise NotImplementedError # Must return a tensor where output[j] represents # the distance/similarity between query_emb[j] and ref_emb[j] def pairwise_distance ( self , query_emb , ref_emb ): raise NotImplementedError","title":"BaseDistance"},{"location":"distances/#cosinesimilarity","text":"distances . CosineSimilarity ( ** kwargs ) The returned mat[i,j] is the cosine similarity between query_emb[i] and ref_emb[j] . This class is equivalent to DotProductSimilarity(normalize_embeddings=True) .","title":"CosineSimilarity"},{"location":"distances/#dotproductsimilarity","text":"distances . DotProductSimilarity ( ** kwargs ) The returned mat[i,j] is equal to torch.sum(query_emb[i] * ref_emb[j])","title":"DotProductSimilarity"},{"location":"distances/#lpdistance","text":"distances . LpDistance ( ** kwargs ) The returned mat[i,j] is the Lp distance between query_emb[i] and ref_emb[j] . With default parameters, this is the Euclidean distance.","title":"LpDistance"},{"location":"distances/#snrdistance","text":"Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning distances . SNRDistance ( ** kwargs ) The returned mat[i,j] is equal to: torch . var ( query_emb [ i ] - ref_emb [ j ]) / torch . var ( query_emb [ i ])","title":"SNRDistance"},{"location":"distributed/","text":"Distributed \u00b6 Wrap a tuple loss or miner with these when using PyTorch's DistributedDataParallel (i.e. multiprocessing). DistributedLossWrapper \u00b6 utils . distributed . DistributedLossWrapper ( loss , efficient = False ) Parameters : loss : The loss function to wrap efficient : True : each process uses its own embeddings for anchors, and the gathered embeddings for positives/negatives. Gradients will not be equal to those in non-distributed code, but the benefit is reduced memory and faster training. False : each process uses gathered embeddings for both anchors and positives/negatives. Gradients will be equal to those in non-distributed code, but at the cost of doing unnecessary operations (i.e. doing computations where both anchors and positives/negatives have no gradient). Example usage: from pytorch_metric_learning import losses from pytorch_metric_learning.utils import distributed as pml_dist loss_func = losses . ContrastiveLoss () loss_func = pml_dist . DistributedLossWrapper ( loss_func ) # in each process during training loss = loss_func ( embeddings , labels ) DistributedMinerWrapper \u00b6 utils . distributed . DistributedMinerWrapper ( miner , efficient = False ) Parameters : miner : The miner to wrap efficient : If your distributed loss function has efficient=True then you must also set the distributed miner's efficient to True. Example usage: from pytorch_metric_learning import miners from pytorch_metric_learning.utils import distributed as pml_dist miner = miners . MultiSimilarityMiner () miner = pml_dist . DistributedMinerWrapper ( miner ) # in each process tuples = miner ( embeddings , labels ) # pass into a DistributedLossWrapper loss = loss_func ( embeddings , labels , indices_tuple )","title":"Distributed"},{"location":"distributed/#distributed","text":"Wrap a tuple loss or miner with these when using PyTorch's DistributedDataParallel (i.e. multiprocessing).","title":"Distributed"},{"location":"distributed/#distributedlosswrapper","text":"utils . distributed . DistributedLossWrapper ( loss , efficient = False ) Parameters : loss : The loss function to wrap efficient : True : each process uses its own embeddings for anchors, and the gathered embeddings for positives/negatives. Gradients will not be equal to those in non-distributed code, but the benefit is reduced memory and faster training. False : each process uses gathered embeddings for both anchors and positives/negatives. Gradients will be equal to those in non-distributed code, but at the cost of doing unnecessary operations (i.e. doing computations where both anchors and positives/negatives have no gradient). Example usage: from pytorch_metric_learning import losses from pytorch_metric_learning.utils import distributed as pml_dist loss_func = losses . ContrastiveLoss () loss_func = pml_dist . DistributedLossWrapper ( loss_func ) # in each process during training loss = loss_func ( embeddings , labels )","title":"DistributedLossWrapper"},{"location":"distributed/#distributedminerwrapper","text":"utils . distributed . DistributedMinerWrapper ( miner , efficient = False ) Parameters : miner : The miner to wrap efficient : If your distributed loss function has efficient=True then you must also set the distributed miner's efficient to True. Example usage: from pytorch_metric_learning import miners from pytorch_metric_learning.utils import distributed as pml_dist miner = miners . MultiSimilarityMiner () miner = pml_dist . DistributedMinerWrapper ( miner ) # in each process tuples = miner ( embeddings , labels ) # pass into a DistributedLossWrapper loss = loss_func ( embeddings , labels , indices_tuple )","title":"DistributedMinerWrapper"},{"location":"inference_models/","text":"Inference Models \u00b6 utils.inference contains classes that make it convenient to find matching pairs within a batch, or from a set of pairs. Take a look at this notebook to see example usage. InferenceModel \u00b6 from pytorch_metric_learning.utils.inference import InferenceModel InferenceModel ( trunk , embedder = None , match_finder = None , normalize_embeddings = True , knn_func = None , data_device = None , dtype = None ) Parameters : trunk : Your trained model for computing embeddings. embedder : Optional. This is if your model is split into two components (trunk and embedder). If None, then the embedder will simply return the trunk's output. match_finder : A MatchFinder object. If None , it will be set to MatchFinder(distance=CosineSimilarity(), threshold=0.9) . normalize_embeddings : If True, embeddings will be normalized to have Euclidean norm of 1. knn_func : The function used for computing k-nearest-neighbors. If None , it will be set to FaissKNN() . data_device : The device that you want to put batches of data on. If not specified, GPUs will be used if available. dtype : The datatype to cast data to. If None, no casting will be done. Methods : # initialize with a model im = InferenceModel ( model ) # pass in a dataset to serve as the search space for k-nn im . train_knn ( dataset ) # add another dataset to the index im . add_to_knn ( dataset2 ) # get the 10 nearest neighbors of a query distances , indices = im . get_nearest_neighbors ( query , k = 10 ) # determine if inputs are close to each other is_match = im . is_match ( x , y ) # determine \"is_match\" pairwise for all elements in a batch match_matrix = im . get_matches ( x ) # save and load the knn function (which is a faiss index by default) im . save_knn_func ( \"filename.index\" ) im . load_knn_func ( \"filename.index\" ) MatchFinder \u00b6 from pytorch_metric_learning.utils.inference import MatchFinder MatchFinder ( distance = None , threshold = None ) Parameters : distance : A distance object. threshold : Optional. Pairs will be a match if they fall under this threshold for non-inverted distances, or over this value for inverted distances. If not provided, then a threshold must be provided during function calls. FaissKNN \u00b6 Uses the faiss library to compute k-nearest-neighbors from pytorch_metric_learning.utils.inference import FaissKNN FaissKNN ( reset_before = True , reset_after = True , index_init_fn = None , gpus = None ) Parameters : reset_before : Reset the faiss index before knn is computed. reset_after : Reset the faiss index after knn is computed (good for clearing memory). index_init_fn : A callable that takes in the embedding dimensionality and returns a faiss index. The default is faiss.IndexFlatL2 . gpus : A list of gpu indices to move the faiss index onto. The default is to use all available gpus, if the input tensors are also on gpus. Example: # use faiss.IndexFlatIP on 3 gpus knn_func = FaissKNN ( index_init_fn = faiss . IndexFlatIP , gpus = [ 0 , 1 , 2 ]) # query = query embeddings # k = the k in k-nearest-neighbors # reference = the embeddings to search # last argument is whether or not query and reference share datapoints distances , indices = knn_func ( query , k , references , False ) FaissKMeans \u00b6 Uses the faiss library to do k-means clustering. from pytorch_metric_learning.utils.inference import FaissKMeans FaissKMeans ( ** kwargs ) Parameters : kwargs : Keyword arguments that will be passed to the faiss.Kmeans constructor. Example: kmeans_func = FaissKMeans ( niter = 100 , verbose = True , gpu = True ) # cluster into 10 groups cluster_assignments = kmeans_func ( embeddings , 10 ) CustomKNN \u00b6 Uses a distance function to determine similarity between datapoints, and then computes k-nearest-neighbors. from pytorch_metric_learning.utils.inference import CustomKNN CustomKNN ( distance ) Parameters : distance : A distance function Example: from pytorch_metric_learning.distances import SNRDistance from pytorch_metric_learning.utils.inference import CustomKNN knn_func = CustomKNN ( SNRDistance ()) distances , indices = knn_func ( query , k , references , False )","title":"Inference Models"},{"location":"inference_models/#inference-models","text":"utils.inference contains classes that make it convenient to find matching pairs within a batch, or from a set of pairs. Take a look at this notebook to see example usage.","title":"Inference Models"},{"location":"inference_models/#inferencemodel","text":"from pytorch_metric_learning.utils.inference import InferenceModel InferenceModel ( trunk , embedder = None , match_finder = None , normalize_embeddings = True , knn_func = None , data_device = None , dtype = None ) Parameters : trunk : Your trained model for computing embeddings. embedder : Optional. This is if your model is split into two components (trunk and embedder). If None, then the embedder will simply return the trunk's output. match_finder : A MatchFinder object. If None , it will be set to MatchFinder(distance=CosineSimilarity(), threshold=0.9) . normalize_embeddings : If True, embeddings will be normalized to have Euclidean norm of 1. knn_func : The function used for computing k-nearest-neighbors. If None , it will be set to FaissKNN() . data_device : The device that you want to put batches of data on. If not specified, GPUs will be used if available. dtype : The datatype to cast data to. If None, no casting will be done. Methods : # initialize with a model im = InferenceModel ( model ) # pass in a dataset to serve as the search space for k-nn im . train_knn ( dataset ) # add another dataset to the index im . add_to_knn ( dataset2 ) # get the 10 nearest neighbors of a query distances , indices = im . get_nearest_neighbors ( query , k = 10 ) # determine if inputs are close to each other is_match = im . is_match ( x , y ) # determine \"is_match\" pairwise for all elements in a batch match_matrix = im . get_matches ( x ) # save and load the knn function (which is a faiss index by default) im . save_knn_func ( \"filename.index\" ) im . load_knn_func ( \"filename.index\" )","title":"InferenceModel"},{"location":"inference_models/#matchfinder","text":"from pytorch_metric_learning.utils.inference import MatchFinder MatchFinder ( distance = None , threshold = None ) Parameters : distance : A distance object. threshold : Optional. Pairs will be a match if they fall under this threshold for non-inverted distances, or over this value for inverted distances. If not provided, then a threshold must be provided during function calls.","title":"MatchFinder"},{"location":"inference_models/#faissknn","text":"Uses the faiss library to compute k-nearest-neighbors from pytorch_metric_learning.utils.inference import FaissKNN FaissKNN ( reset_before = True , reset_after = True , index_init_fn = None , gpus = None ) Parameters : reset_before : Reset the faiss index before knn is computed. reset_after : Reset the faiss index after knn is computed (good for clearing memory). index_init_fn : A callable that takes in the embedding dimensionality and returns a faiss index. The default is faiss.IndexFlatL2 . gpus : A list of gpu indices to move the faiss index onto. The default is to use all available gpus, if the input tensors are also on gpus. Example: # use faiss.IndexFlatIP on 3 gpus knn_func = FaissKNN ( index_init_fn = faiss . IndexFlatIP , gpus = [ 0 , 1 , 2 ]) # query = query embeddings # k = the k in k-nearest-neighbors # reference = the embeddings to search # last argument is whether or not query and reference share datapoints distances , indices = knn_func ( query , k , references , False )","title":"FaissKNN"},{"location":"inference_models/#faisskmeans","text":"Uses the faiss library to do k-means clustering. from pytorch_metric_learning.utils.inference import FaissKMeans FaissKMeans ( ** kwargs ) Parameters : kwargs : Keyword arguments that will be passed to the faiss.Kmeans constructor. Example: kmeans_func = FaissKMeans ( niter = 100 , verbose = True , gpu = True ) # cluster into 10 groups cluster_assignments = kmeans_func ( embeddings , 10 )","title":"FaissKMeans"},{"location":"inference_models/#customknn","text":"Uses a distance function to determine similarity between datapoints, and then computes k-nearest-neighbors. from pytorch_metric_learning.utils.inference import CustomKNN CustomKNN ( distance ) Parameters : distance : A distance function Example: from pytorch_metric_learning.distances import SNRDistance from pytorch_metric_learning.utils.inference import CustomKNN knn_func = CustomKNN ( SNRDistance ()) distances , indices = knn_func ( query , k , references , False )","title":"CustomKNN"},{"location":"logging_presets/","text":"Logging Presets \u00b6 The logging_presets module contains ready-to-use hooks for logging data, validating and saving your models, and early stoppage during training. It requires the record-keeper and tensorboard packages, which can be installed with pip: pip install record-keeper tensorboard Here's how you can use it in conjunction with a trainer and tester: import pytorch_metric_learning.utils.logging_presets as LP log_folder , tensorboard_folder = \"example_logs\" , \"example_tensorboard\" record_keeper , _ , _ = LP . get_record_keeper ( log_folder , tensorboard_folder ) hooks = LP . get_hook_container ( record_keeper ) dataset_dict = { \"val\" : val_dataset } model_folder = \"example_saved_models\" # Create the tester tester = testers . GlobalEmbeddingSpaceTester ( end_of_testing_hook = hooks . end_of_testing_hook ) end_of_epoch_hook = hooks . end_of_epoch_hook ( tester , dataset_dict , model_folder ) trainer = trainers . MetricLossOnly ( models , optimizers , batch_size , loss_funcs , mining_funcs , train_dataset , sampler = sampler , end_of_iteration_hook = hooks . end_of_iteration_hook , end_of_epoch_hook = end_of_epoch_hook ) trainer . train ( num_epochs = num_epochs ) With the provided hooks, data from both the training and validation stages will be saved in csv, sqlite, and tensorboard format, and models and optimizers will be saved in the specified model folder. See the example notebooks for complete examples. Read the next section to learn more about the provided hooks. HookContainer \u00b6 This class contains ready-to-use hooks to be used by trainers and testers. import pytorch_metric_learning.utils.logging_presets as LP LP . HookContainer ( record_keeper , record_group_name_prefix = None , primary_metric = \"mean_average_precision_at_r\" , validation_split_name = \"val\" , save_models = True , log_freq = 50 ) Parameters : record_keeper : A record-keeper object. Install: pip install record-keeper tensorboard . record_group_name_prefix : A string which will be prepended to all record names and tensorboard tags. primary_metric : A string that specifies the accuracy metric which will be used to determine the best checkpoint. Must be one of: mean_average_precision_at_r r_precision precision_at_1 NMI validation_split_name : Optional. Default value is \"val\". The name of your validation set in dataset_dict . save_models : Optional. Models will be saved if this is True . log_freq : Data will be logged every log_freq iterations. Important functions : end_of_iteration_hook : This function records data about models, optimizers, and loss and mining functions. You can pass this function directly into a trainer object. end_of_epoch_hook : This function runs validation and saves models. This function returns the actual hook, i.e. you must pass in the following arguments to obtain the hook. tester : A tester object. dataset_dict : A dictionary mapping from split names to PyTorch datasets. For example: {\"train\": train_dataset, \"val\": val_dataset} model_folder : A string which is the folder path where models, optimizers etc. will be saved. test_interval : Optional. Default value is 1. Validation will be run every test_interval epochs. patience : Optional. Default value is None. If not None, training will end early if epoch - best_epoch > patience . splits_to_eval : Optional. See splits_to_eval . test_collate_fn : Optional. Default value is None. This is the collate function used by the dataloader during testing. end_of_testing_hook : This function records accuracy metrics. You can pass this function directly into a tester object. Useful methods : Getting loss history: # Get a dictionary mapping from loss names to lists loss_histories = hooks . get_loss_history () # You can also specify which loss histories you want # It will still return a dictionary. In this case, the dictionary will contain only \"total_loss\" loss_histories = hooks . get_loss_history ( loss_names = [ \"total_loss\" ]) Getting accuracy history # The first argument is the tester object. The second is the split name. # Get a dictionary containing the keys \"epoch\" and the primary metric # The values are lists acc_histories = hooks . get_accuracy_history ( tester , \"val\" ) # Get all accuracy histories acc_histories = hooks . get_accuracy_history ( tester , \"val\" , return_all_metrics = True ) # Get a specific set of accuracy histories acc_histories = hooks . get_accuracy_history ( tester , \"val\" , metrics = [ \"AMI\" , \"NMI\" ])","title":"Logging Presets"},{"location":"logging_presets/#logging-presets","text":"The logging_presets module contains ready-to-use hooks for logging data, validating and saving your models, and early stoppage during training. It requires the record-keeper and tensorboard packages, which can be installed with pip: pip install record-keeper tensorboard Here's how you can use it in conjunction with a trainer and tester: import pytorch_metric_learning.utils.logging_presets as LP log_folder , tensorboard_folder = \"example_logs\" , \"example_tensorboard\" record_keeper , _ , _ = LP . get_record_keeper ( log_folder , tensorboard_folder ) hooks = LP . get_hook_container ( record_keeper ) dataset_dict = { \"val\" : val_dataset } model_folder = \"example_saved_models\" # Create the tester tester = testers . GlobalEmbeddingSpaceTester ( end_of_testing_hook = hooks . end_of_testing_hook ) end_of_epoch_hook = hooks . end_of_epoch_hook ( tester , dataset_dict , model_folder ) trainer = trainers . MetricLossOnly ( models , optimizers , batch_size , loss_funcs , mining_funcs , train_dataset , sampler = sampler , end_of_iteration_hook = hooks . end_of_iteration_hook , end_of_epoch_hook = end_of_epoch_hook ) trainer . train ( num_epochs = num_epochs ) With the provided hooks, data from both the training and validation stages will be saved in csv, sqlite, and tensorboard format, and models and optimizers will be saved in the specified model folder. See the example notebooks for complete examples. Read the next section to learn more about the provided hooks.","title":"Logging Presets"},{"location":"logging_presets/#hookcontainer","text":"This class contains ready-to-use hooks to be used by trainers and testers. import pytorch_metric_learning.utils.logging_presets as LP LP . HookContainer ( record_keeper , record_group_name_prefix = None , primary_metric = \"mean_average_precision_at_r\" , validation_split_name = \"val\" , save_models = True , log_freq = 50 ) Parameters : record_keeper : A record-keeper object. Install: pip install record-keeper tensorboard . record_group_name_prefix : A string which will be prepended to all record names and tensorboard tags. primary_metric : A string that specifies the accuracy metric which will be used to determine the best checkpoint. Must be one of: mean_average_precision_at_r r_precision precision_at_1 NMI validation_split_name : Optional. Default value is \"val\". The name of your validation set in dataset_dict . save_models : Optional. Models will be saved if this is True . log_freq : Data will be logged every log_freq iterations. Important functions : end_of_iteration_hook : This function records data about models, optimizers, and loss and mining functions. You can pass this function directly into a trainer object. end_of_epoch_hook : This function runs validation and saves models. This function returns the actual hook, i.e. you must pass in the following arguments to obtain the hook. tester : A tester object. dataset_dict : A dictionary mapping from split names to PyTorch datasets. For example: {\"train\": train_dataset, \"val\": val_dataset} model_folder : A string which is the folder path where models, optimizers etc. will be saved. test_interval : Optional. Default value is 1. Validation will be run every test_interval epochs. patience : Optional. Default value is None. If not None, training will end early if epoch - best_epoch > patience . splits_to_eval : Optional. See splits_to_eval . test_collate_fn : Optional. Default value is None. This is the collate function used by the dataloader during testing. end_of_testing_hook : This function records accuracy metrics. You can pass this function directly into a tester object. Useful methods : Getting loss history: # Get a dictionary mapping from loss names to lists loss_histories = hooks . get_loss_history () # You can also specify which loss histories you want # It will still return a dictionary. In this case, the dictionary will contain only \"total_loss\" loss_histories = hooks . get_loss_history ( loss_names = [ \"total_loss\" ]) Getting accuracy history # The first argument is the tester object. The second is the split name. # Get a dictionary containing the keys \"epoch\" and the primary metric # The values are lists acc_histories = hooks . get_accuracy_history ( tester , \"val\" ) # Get all accuracy histories acc_histories = hooks . get_accuracy_history ( tester , \"val\" , return_all_metrics = True ) # Get a specific set of accuracy histories acc_histories = hooks . get_accuracy_history ( tester , \"val\" , metrics = [ \"AMI\" , \"NMI\" ])","title":"HookContainer"},{"location":"losses/","text":"Losses \u00b6 All loss functions are used as follows: from pytorch_metric_learning import losses loss_func = losses . SomeLoss () loss = loss_func ( embeddings , labels ) # in your training for-loop Or if you are using a loss in conjunction with a miner : from pytorch_metric_learning import miners miner_func = miners . SomeMiner () loss_func = losses . SomeLoss () miner_output = miner_func ( embeddings , labels ) # in your training for-loop loss = loss_func ( embeddings , labels , miner_output ) You can specify how losses get reduced to a single value by using a reducer : from pytorch_metric_learning import reducers reducer = reducers . SomeReducer () loss_func = losses . SomeLoss ( reducer = reducer ) loss = loss_func ( embeddings , labels ) # in your training for-loop For tuple losses, can separate the source of anchors and positives/negatives: loss_func = losses . SomeLoss () # anchors will come from embeddings # positives/negatives will come from ref_emb loss = loss_func ( embeddings , labels , ref_emb = ref_emb , ref_labels = ref_labels ) For classification losses, you can get logits using the get_logits function: loss_func = losses . SomeClassificationLoss () logits = loss_func . get_logits ( embeddings ) AngularLoss \u00b6 Deep Metric Learning with Angular Loss losses . AngularLoss ( alpha = 40 , ** kwargs ) Equation : Parameters : alpha : The angle specified in degrees. The paper uses values between 36 and 55. Default distance : LpDistance(p=2, power=1, normalize_embeddings=True) This is the only compatible distance. Default reducer : MeanReducer Reducer input : loss : The loss for every a1 , where (a1,p) represents every positive pair in the batch. Reduction type is \"element\" . ArcFaceLoss \u00b6 ArcFace: Additive Angular Margin Loss for Deep Face Recognition losses . ArcFaceLoss ( num_classes , embedding_size , margin = 28.6 , scale = 64 , ** kwargs ) Equation : Parameters : margin : The angular margin penalty in degrees. In the above equation, m = radians(margin) . The paper uses 0.5 radians, which is 28.6 degrees. num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : This is s in the above equation. The paper uses 64. Other info : This also extends WeightRegularizerMixin , so it accepts weight_regularizer , weight_reg_weight , and weight_init_func as optional arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . ArcFaceLoss ( ... ) . to ( torch . device ( 'cuda' )) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () Default distance : CosineSimilarity() This is the only compatible distance. Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" . BaseMetricLossFunction \u00b6 All loss functions extend this class and therefore inherit its __init__ parameters. losses . BaseMetricLossFunction ( collect_stats = False , reducer = None , distance = None , embedding_regularizer = None , embedding_reg_weight = 1 ) Parameters : collect_stats : If True, will collect various statistics that may be useful to analyze during experiments. If False, these computations will be skipped. Want to make True the default? Set the global COLLECT_STATS flag. reducer : A reducer object. If None, then the default reducer will be used. distance : A distance object. If None, then the default distance will be used. embedding_regularizer : A regularizer object that will be applied to embeddings. If None, then no embedding regularization will be used. embedding_reg_weight : If an embedding regularizer is used, then its loss will be multiplied by this amount before being added to the total loss. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Default reducer : MeanReducer Reducer input : embedding_reg_loss : Only exists if an embedding regularizer is used. It contains the loss per element in the batch. Reduction type is \"already_reduced\" . Required Implementations : def compute_loss ( self , embeddings , labels , indices_tuple = None ): raise NotImplementedError CentroidTripletLoss \u00b6 On the Unreasonable Effectiveness of Centroids in Image Retrieval This is like TripletMarginLoss , except the positives and negatives are class centroids. losses . CentroidTripletLoss ( margin = 0.05 , swap = False , smooth_loss = False , triplets_per_anchor = \"all\" , ** kwargs ) Parameters : See TripletMarginLoss Default distance : See TripletMarginLoss Default reducer : AvgNonZeroReducer CircleLoss \u00b6 Circle Loss: A Unified Perspective of Pair Similarity Optimization losses . CircleLoss ( m = 0.4 , gamma = 80 , ** kwargs ) Equations : where Parameters : m : The relaxation factor that controls the radius of the decision boundary. The paper uses 0.25 for face recognition, and 0.4 for fine-grained image retrieval (images of birds, cars, and online products). gamma : The scale factor that determines the largest scale of each similarity score. The paper uses 256 for face recognition, and 80 for fine-grained image retrieval. Default distance : CosineSimilarity() This is the only compatible distance. Default reducer : AvgNonZeroReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" . ContrastiveLoss \u00b6 losses . ContrastiveLoss ( pos_margin = 0 , neg_margin = 1 , ** kwargs ): Equation : If using a distance metric like LpDistance , the loss is: If using a similarity metric like CosineSimilarity , the loss is: Parameters : pos_margin : The distance (or similarity) over (under) which positive pairs will contribute to the loss. neg_margin : The distance (or similarity) under (over) which negative pairs will contribute to the loss. Note that the default values for pos_margin and neg_margin are suitable if you are using a non-inverted distance measure, like LpDistance . If you use an inverted distance measure like CosineSimilarity , then more appropriate values would be pos_margin = 1 and neg_margin = 0 . Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Default reducer : AvgNonZeroReducer Reducer input : pos_loss : The loss per positive pair in the batch. Reduction type is \"pos_pair\" . neg_loss : The loss per negative pair in the batch. Reduction type is \"neg_pair\" . CosFaceLoss \u00b6 CosFace: Large Margin Cosine Loss for Deep Face Recognition losses . CosFaceLoss ( num_classes , embedding_size , margin = 0.35 , scale = 64 , ** kwargs ) Equation : Parameters : margin : The cosine margin penalty (m in the above equation). The paper used values between 0.25 and 0.45. num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : This is s in the above equation. The paper uses 64. Other info : This also extends WeightRegularizerMixin , so it accepts weight_regularizer , weight_reg_weight , and weight_init_func as optional arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . CosFaceLoss ( ... ) . to ( torch . device ( 'cuda' )) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () Default distance : CosineSimilarity() This is the only compatible distance. Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" . CrossBatchMemory \u00b6 This wraps a loss function, and implements Cross-Batch Memory for Embedding Learning . It stores embeddings from previous iterations in a queue, and uses them to form more pairs/triplets with the current iteration's embeddings. losses . CrossBatchMemory ( loss , embedding_size , memory_size = 1024 , miner = None ) Parameters : loss : The loss function to be wrapped. For example, you could pass in ContrastiveLoss() . embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. memory_size : The size of the memory queue. miner : An optional tuple miner , which will be used to mine pairs/triplets from the memory queue. Forward function loss_fn ( embeddings , labels , indices_tuple = None , enqueue_idx = None ) As shown above, CrossBatchMemory comes with a 4th argument in its forward function: enqueue_idx : The indices of embeddings that will be added to the memory queue. In other words, only embeddings[enqueue_idx] will be added to memory. This enables CrossBatchMemory to be used in self-supervision frameworks like MoCo . Check out the MoCo on CIFAR100 notebook to see how this works. FastAPLoss \u00b6 Deep Metric Learning to Rank losses . FastAPLoss ( num_bins = 10 , ** kwargs ) Parameters : num_bins : The number of soft histogram bins for calculating average precision. The paper suggests using 10. Default distance : LpDistance(normalize_embeddings=True, p=2, power=2) The only compatible distance is LpDistance(normalize_embeddings=True, p=2) . However, the power value can be changed. Default reducer : MeanReducer Reducer input : loss : The loss per element that has at least 1 positive in the batch. Reduction type is \"element\" . GenericPairLoss \u00b6 losses . GenericPairLoss ( mat_based_loss , ** kwargs ) Parameters : mat_based_loss : See required implementations. Required Implementations : # If mat_based_loss is True, then this takes in mat, pos_mask, neg_mask # If False, this takes in pos_pair, neg_pair, indices_tuple def _compute_loss ( self ): raise NotImplementedError GeneralizedLiftedStructureLoss \u00b6 This was presented in In Defense of the Triplet Loss for Person Re-Identification . It is a modification of the original LiftedStructureLoss losses . GeneralizedLiftedStructureLoss ( neg_margin = 1 , pos_margin = 0 , ** kwargs ) Equation : Parameters : pos_margin : The margin in the expression e^(D - margin) . The paper uses pos_margin = 0 , which is why this margin does not appear in the above equation. neg_margin : This is m in the above equation. The paper used values between 0.1 and 1. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" . IntraPairVarianceLoss \u00b6 Deep Metric Learning with Tuplet Margin Loss losses . IntraPairVarianceLoss ( pos_eps = 0.01 , neg_eps = 0.01 , ** kwargs ) Equations : Parameters : pos_eps : The epsilon in the L pos equation. The paper uses 0.01. neg_eps : The epsilon in the L neg equation. The paper uses 0.01. You should probably use this in conjunction with another loss, as described in the paper. You can accomplish this by using MultipleLosses : main_loss = losses . TupletMarginLoss () var_loss = losses . IntraPairVarianceLoss () complete_loss = losses . MultipleLosses ([ main_loss , var_loss ], weights = [ 1 , 0.5 ]) Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Default reducer : MeanReducer Reducer input : pos_loss : The loss per positive pair in the batch. Reduction type is \"pos_pair\" . neg_loss : The loss per negative pair in the batch. Reduction type is \"neg_pair\" . LargeMarginSoftmaxLoss \u00b6 Large-Margin Softmax Loss for Convolutional Neural Networks losses . LargeMarginSoftmaxLoss ( num_classes , embedding_size , margin = 4 , scale = 1 , ** kwargs ) Equations : where Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. margin : An integer which dictates the size of the angular margin. This is m in the above equation. The paper finds m=4 works best. scale : The exponent multiplier in the loss's softmax expression. The paper uses scale = 1 , which is why it does not appear in the above equation. Other info : This also extends WeightRegularizerMixin , so it accepts weight_regularizer , weight_reg_weight , and weight_init_func as optional arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . LargeMarginSoftmaxLoss ( ... ) . to ( torch . device ( 'cuda' )) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () Default distance : CosineSimilarity() This is the only compatible distance. Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" . LiftedStructureLoss \u00b6 The original lifted structure loss as presented in Deep Metric Learning via Lifted Structured Feature Embedding losses . LiftedStructureLoss ( neg_margin = 1 , pos_margin = 0 , ** kwargs ): Equation : Parameters : pos_margin : The margin in the expression D_(i,j) - margin . The paper uses pos_margin = 0 , which is why it does not appear in the above equation. neg_margin : This is alpha in the above equation. The paper uses 1. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Default reducer : MeanReducer Reducer input : loss : The loss per positive pair in the batch. Reduction type is \"pos_pair\" . MarginLoss \u00b6 Sampling Matters in Deep Embedding Learning losses . MarginLoss ( margin = 0.2 , nu = 0 , beta = 1.2 , triplets_per_anchor = \"all\" , learn_beta = False , num_classes = None , ** kwargs ) Equations : where Parameters : margin : This is alpha in the above equation. The paper uses 0.2. nu : The regularization weight for the magnitude of beta. beta : This is beta in the above equation. The paper uses 1.2 as the initial value. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used. learn_beta : If True, beta will be a torch.nn.Parameter, which can be optimized using any PyTorch optimizer. num_classes : If not None, then beta will be of size num_classes , so that a separate beta is used for each class during training. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Default reducer : DivisorReducer Reducer input : margin_loss : The loss per triplet in the batch. Reduction type is \"triplet\" . beta_reg_loss : The regularization loss per element in self.beta . Reduction type is \"already_reduced\" if self.num_classes = None . Otherwise it is \"element\" . MultipleLosses \u00b6 This is a simple wrapper for multiple losses. Pass in a list of already-initialized loss functions. Then, when you call forward on this object, it will return the sum of all wrapped losses. losses . MultipleLosses ( losses , miners = None , weights = None ) Parameters : losses : A list or dictionary of initialized loss functions. On the forward call of MultipleLosses, each wrapped loss will be computed, and then the average will be returned. miners : Optional. A list or dictionary of mining functions. This allows you to pair mining functions with loss functions. For example, if losses = [loss_A, loss_B] , and miners = [None, miner_B] then no mining will be done for loss_A , but the output of miner_B will be passed to loss_B . The same logic applies if losses = {\"loss_A\": loss_A, \"loss_B\": loss_B} and miners = {\"loss_B\": miner_B} . weights : Optional. A list or dictionary of loss weights, which will be multiplied by the corresponding losses obtained by the loss functions. The default is to multiply each loss by 1. If losses is a list, then weights must be a list. If losses is a dictionary, weights must contain the same keys as losses . MultiSimilarityLoss \u00b6 Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning losses . MultiSimilarityLoss ( alpha = 2 , beta = 50 , base = 0.5 , ** kwargs ) Equation : Parameters : alpha : The weight applied to positive pairs. The paper uses 2. beta : The weight applied to negative pairs. The paper uses 50. base : The offset applied to the exponent in the loss. This is lambda in the above equation. The paper uses 1. Default distance : CosineSimilarity() Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" . NCALoss \u00b6 Neighbourhood Components Analysis losses . NCALoss ( softmax_scale = 1 , ** kwargs ) Equations : where In this implementation, we use -g(A) as the loss. Parameters : softmax_scale : The exponent multiplier in the loss's softmax expression. The paper uses softmax_scale = 1 , which is why it does not appear in the above equations. Default distance : LpDistance(normalize_embeddings=True, p=2, power=2) Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch, that results in a non zero exponent in the cross entropy expression. Reduction type is \"element\" . NormalizedSoftmaxLoss \u00b6 Classification is a Strong Baseline for Deep Metric Learning losses . NormalizedSoftmaxLoss ( num_classes , embedding_size , temperature = 0.05 , ** kwargs ) Equation : Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. temperature : This is sigma in the above equation. The paper uses 0.05. Other info This also extends WeightRegularizerMixin , so it accepts weight_regularizer , weight_reg_weight , and weight_init_func as optional arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . NormalizedSoftmaxLoss ( ... ) . to ( torch . device ( 'cuda' )) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () Default distance : DotProductSimilarity() Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" . NPairsLoss \u00b6 Improved Deep Metric Learning with Multi-class N-pair Loss Objective If your batch has more than 2 samples per label, then you should use NTXentLoss . losses . NPairsLoss ( ** kwargs ) Default distance : DotProductSimilarity() Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" . NTXentLoss \u00b6 This is also known as InfoNCE, and is a generalization of the NPairsLoss . It has been used in self-supervision papers such as: Representation Learning with Contrastive Predictive Coding Momentum Contrast for Unsupervised Visual Representation Learning A Simple Framework for Contrastive Learning of Visual Representations losses . NTXentLoss ( temperature = 0.07 , ** kwargs ) Equation : Parameters : temperature : This is tau in the above equation. The MoCo paper uses 0.07, while SimCLR uses 0.5. Default distance : CosineSimilarity() Default reducer : MeanReducer Reducer input : loss : The loss per positive pair in the batch. Reduction type is \"pos_pair\" . ProxyAnchorLoss \u00b6 Proxy Anchor Loss for Deep Metric Learning losses . ProxyAnchorLoss ( num_classes , embedding_size , margin = 0.1 , alpha = 32 , ** kwargs ) Equation : Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. margin : This is delta in the above equation. The paper uses 0.1. alpha : This is alpha in the above equation. The paper uses 32. Other info This also extends WeightRegularizerMixin , so it accepts weight_regularizer , weight_reg_weight , and weight_init_func as optional arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . ProxyAnchorLoss ( ... ) . to ( torch . device ( 'cuda' )) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () Default distance : CosineSimilarity() Default reducer : DivisorReducer Reducer input : pos_loss : The positive pair loss per proxy. Reduction type is \"element\" . neg_loss : The negative pair loss per proxy. Reduction type is \"element\" . ProxyNCALoss \u00b6 No Fuss Distance Metric Learning using Proxies losses . ProxyNCALoss ( num_classes , embedding_size , softmax_scale = 1 , ** kwargs ) Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. softmax_scale : See NCALoss Other info This also extends WeightRegularizerMixin , so it accepts weight_regularizer , weight_reg_weight , and weight_init_func as optional arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . ProxyNCALoss ( ... ) . to ( torch . device ( 'cuda' )) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () Default distance : LpDistance(normalize_embeddings=True, p=2, power=2) Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch, that results in a non zero exponent in the cross entropy expression. Reduction type is \"element\" . SignalToNoiseRatioContrastiveLoss \u00b6 Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning losses . SignalToNoiseRatioContrastiveLoss ( pos_margin = 0 , neg_margin = 1 , ** kwargs ): Parameters : pos_margin : The noise-to-signal ratio over which positive pairs will contribute to the loss. neg_margin : The noise-to-signal ratio under which negative pairs will contribute to the loss. Default distance : SNRDistance() This is the only compatible distance. Default reducer : AvgNonZeroReducer Reducer input : pos_loss : The loss per positive pair in the batch. Reduction type is \"pos_pair\" . neg_loss : The loss per negative pair in the batch. Reduction type is \"neg_pair\" . SoftTripleLoss \u00b6 SoftTriple Loss: Deep Metric Learning Without Triplet Sampling losses . SoftTripleLoss ( num_classes , embedding_size , centers_per_class = 10 , la = 20 , gamma = 0.1 , margin = 0.01 , ** kwargs ) Equations : where Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. centers_per_class : The number of weight vectors per class. (The regular cross entropy loss has 1 center per class.) The paper uses 10. la : This is lambda in the above equation. gamma : This is gamma in the above equation. The paper uses 0.1. margin : The is delta in the above equations. The paper uses 0.01. Other info This also extends WeightRegularizerMixin , so it accepts weight_regularizer , weight_reg_weight , and weight_init_func as optional arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . SoftTripleLoss ( ... ) . to ( torch . device ( 'cuda' )) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () Default distance : CosineSimilarity() The distance measure must be inverted. For example, DotProductSimilarity(normalize_embeddings=False) is also compatible. Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" . SphereFaceLoss \u00b6 SphereFace: Deep Hypersphere Embedding for Face Recognition losses . SphereFaceLoss ( num_classes , embedding_size , margin = 4 , scale = 1 , ** kwargs ) Parameters : See LargeMarginSoftmaxLoss Other info This also extends WeightRegularizerMixin , so it accepts weight_regularizer , weight_reg_weight , and weight_init_func as optional arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . SphereFaceLoss ( ... ) . to ( torch . device ( 'cuda' )) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () Default distance : CosineSimilarity() This is the only compatible distance. Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" . SupConLoss \u00b6 Described in Supervised Contrastive Learning . losses . SupConLoss ( temperature = 0.1 , ** kwargs ) Equation : Parameters : temperature : This is tau in the above equation. The paper uses 0.1. Default distance : CosineSimilarity() Default reducer : AvgNonZeroReducer Reducer input : loss : The loss per element in the batch. If an element has only negative pairs or no pairs, it's ignored thanks to AvgNonZeroReducer . Reduction type is \"element\" . TripletMarginLoss \u00b6 losses . TripletMarginLoss ( margin = 0.05 , swap = False , smooth_loss = False , triplets_per_anchor = \"all\" , ** kwargs ) Equation : Parameters : margin : The desired difference between the anchor-positive distance and the anchor-negative distance. This is m in the above equation. swap : Use the positive-negative distance instead of anchor-negative distance, if it violates the margin more. smooth_loss : Use the log-exp version of the triplet loss triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Default reducer : AvgNonZeroReducer Reducer input : loss : The loss per triplet in the batch. Reduction type is \"triplet\" . TupletMarginLoss \u00b6 Deep Metric Learning with Tuplet Margin Loss losses . TupletMarginLoss ( margin = 5.73 , scale = 64 , ** kwargs ) Equation : Parameters : margin : The angular margin (in degrees) applied to positive pairs. This is beta in the above equation. The paper uses a value of 5.73 degrees (0.1 radians). scale : This is s in the above equation. The paper combines this loss with IntraPairVarianceLoss . You can accomplish this by using MultipleLosses : main_loss = losses . TupletMarginLoss () var_loss = losses . IntraPairVarianceLoss () complete_loss = losses . MultipleLosses ([ main_loss , var_loss ], weights = [ 1 , 0.5 ]) Default distance : CosineSimilarity() This is the only compatible distance. Default reducer : MeanReducer Reducer input : loss : The loss per positive pair in the batch. Reduction type is \"pos_pair\" . WeightRegularizerMixin \u00b6 Losses can extend this class in addition to BaseMetricLossFunction. You should extend this class if your loss function contains a learnable weight matrix. losses . WeightRegularizerMixin ( weight_init_func = None , weight_regularizer = None , weight_reg_weight = 1 , ** kwargs ) Parameters : weight_init_func : An TorchInitWrapper object, which will be used to initialize the weights of the loss function. weight_regularizer : The regularizer to apply to the loss's learned weights. weight_reg_weight : The amount the regularization loss will be multiplied by. Extended by: ArcFaceLoss CosFaceLoss LargeMarginSoftmaxLoss NormalizedSoftmaxLoss ProxyAnchorLoss ProxyNCALoss SoftTripleLoss SphereFaceLoss VICRegLoss \u00b6 VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning losses . VICRegLoss ( invariance_lambda = 25 , variance_mu = 25 , covariance_v = 1 , eps = 1e-4 , ** kwargs ) Usage : Unlike other loss functions, VICRegLoss does not accept labels or indices_tuple : loss_fn = VICRegLoss () loss = loss_fn ( embeddings , ref_emb ) Equations : where Parameters : invariance_lambda : The weight of the invariance term. variance_mu : The weight of the variance term. covariance_v : The weight of the covariance term. eps : Small scalar to prevent numerical instability. Default distance : Not applicable. You cannot pass in a distance function. Default reducer : MeanReducer Reducer input : invariance_loss : The MSE loss between embeddings[i] and ref_emb[i] . Reduction type is \"element\" . variance_loss1 : The variance loss for embeddings . Reduction type is \"element\" . variance_loss2 : The variance loss for ref_emb . Reduction type is \"element\" . covariance_loss : The covariance loss. This loss is already reduced to a single value.","title":"Losses"},{"location":"losses/#losses","text":"All loss functions are used as follows: from pytorch_metric_learning import losses loss_func = losses . SomeLoss () loss = loss_func ( embeddings , labels ) # in your training for-loop Or if you are using a loss in conjunction with a miner : from pytorch_metric_learning import miners miner_func = miners . SomeMiner () loss_func = losses . SomeLoss () miner_output = miner_func ( embeddings , labels ) # in your training for-loop loss = loss_func ( embeddings , labels , miner_output ) You can specify how losses get reduced to a single value by using a reducer : from pytorch_metric_learning import reducers reducer = reducers . SomeReducer () loss_func = losses . SomeLoss ( reducer = reducer ) loss = loss_func ( embeddings , labels ) # in your training for-loop For tuple losses, can separate the source of anchors and positives/negatives: loss_func = losses . SomeLoss () # anchors will come from embeddings # positives/negatives will come from ref_emb loss = loss_func ( embeddings , labels , ref_emb = ref_emb , ref_labels = ref_labels ) For classification losses, you can get logits using the get_logits function: loss_func = losses . SomeClassificationLoss () logits = loss_func . get_logits ( embeddings )","title":"Losses"},{"location":"losses/#angularloss","text":"Deep Metric Learning with Angular Loss losses . AngularLoss ( alpha = 40 , ** kwargs ) Equation : Parameters : alpha : The angle specified in degrees. The paper uses values between 36 and 55. Default distance : LpDistance(p=2, power=1, normalize_embeddings=True) This is the only compatible distance. Default reducer : MeanReducer Reducer input : loss : The loss for every a1 , where (a1,p) represents every positive pair in the batch. Reduction type is \"element\" .","title":"AngularLoss"},{"location":"losses/#arcfaceloss","text":"ArcFace: Additive Angular Margin Loss for Deep Face Recognition losses . ArcFaceLoss ( num_classes , embedding_size , margin = 28.6 , scale = 64 , ** kwargs ) Equation : Parameters : margin : The angular margin penalty in degrees. In the above equation, m = radians(margin) . The paper uses 0.5 radians, which is 28.6 degrees. num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : This is s in the above equation. The paper uses 64. Other info : This also extends WeightRegularizerMixin , so it accepts weight_regularizer , weight_reg_weight , and weight_init_func as optional arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . ArcFaceLoss ( ... ) . to ( torch . device ( 'cuda' )) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () Default distance : CosineSimilarity() This is the only compatible distance. Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" .","title":"ArcFaceLoss"},{"location":"losses/#basemetriclossfunction","text":"All loss functions extend this class and therefore inherit its __init__ parameters. losses . BaseMetricLossFunction ( collect_stats = False , reducer = None , distance = None , embedding_regularizer = None , embedding_reg_weight = 1 ) Parameters : collect_stats : If True, will collect various statistics that may be useful to analyze during experiments. If False, these computations will be skipped. Want to make True the default? Set the global COLLECT_STATS flag. reducer : A reducer object. If None, then the default reducer will be used. distance : A distance object. If None, then the default distance will be used. embedding_regularizer : A regularizer object that will be applied to embeddings. If None, then no embedding regularization will be used. embedding_reg_weight : If an embedding regularizer is used, then its loss will be multiplied by this amount before being added to the total loss. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Default reducer : MeanReducer Reducer input : embedding_reg_loss : Only exists if an embedding regularizer is used. It contains the loss per element in the batch. Reduction type is \"already_reduced\" . Required Implementations : def compute_loss ( self , embeddings , labels , indices_tuple = None ): raise NotImplementedError","title":"BaseMetricLossFunction"},{"location":"losses/#centroidtripletloss","text":"On the Unreasonable Effectiveness of Centroids in Image Retrieval This is like TripletMarginLoss , except the positives and negatives are class centroids. losses . CentroidTripletLoss ( margin = 0.05 , swap = False , smooth_loss = False , triplets_per_anchor = \"all\" , ** kwargs ) Parameters : See TripletMarginLoss Default distance : See TripletMarginLoss Default reducer : AvgNonZeroReducer","title":"CentroidTripletLoss"},{"location":"losses/#circleloss","text":"Circle Loss: A Unified Perspective of Pair Similarity Optimization losses . CircleLoss ( m = 0.4 , gamma = 80 , ** kwargs ) Equations : where Parameters : m : The relaxation factor that controls the radius of the decision boundary. The paper uses 0.25 for face recognition, and 0.4 for fine-grained image retrieval (images of birds, cars, and online products). gamma : The scale factor that determines the largest scale of each similarity score. The paper uses 256 for face recognition, and 80 for fine-grained image retrieval. Default distance : CosineSimilarity() This is the only compatible distance. Default reducer : AvgNonZeroReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" .","title":"CircleLoss"},{"location":"losses/#contrastiveloss","text":"losses . ContrastiveLoss ( pos_margin = 0 , neg_margin = 1 , ** kwargs ): Equation : If using a distance metric like LpDistance , the loss is: If using a similarity metric like CosineSimilarity , the loss is: Parameters : pos_margin : The distance (or similarity) over (under) which positive pairs will contribute to the loss. neg_margin : The distance (or similarity) under (over) which negative pairs will contribute to the loss. Note that the default values for pos_margin and neg_margin are suitable if you are using a non-inverted distance measure, like LpDistance . If you use an inverted distance measure like CosineSimilarity , then more appropriate values would be pos_margin = 1 and neg_margin = 0 . Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Default reducer : AvgNonZeroReducer Reducer input : pos_loss : The loss per positive pair in the batch. Reduction type is \"pos_pair\" . neg_loss : The loss per negative pair in the batch. Reduction type is \"neg_pair\" .","title":"ContrastiveLoss"},{"location":"losses/#cosfaceloss","text":"CosFace: Large Margin Cosine Loss for Deep Face Recognition losses . CosFaceLoss ( num_classes , embedding_size , margin = 0.35 , scale = 64 , ** kwargs ) Equation : Parameters : margin : The cosine margin penalty (m in the above equation). The paper used values between 0.25 and 0.45. num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : This is s in the above equation. The paper uses 64. Other info : This also extends WeightRegularizerMixin , so it accepts weight_regularizer , weight_reg_weight , and weight_init_func as optional arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . CosFaceLoss ( ... ) . to ( torch . device ( 'cuda' )) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () Default distance : CosineSimilarity() This is the only compatible distance. Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" .","title":"CosFaceLoss"},{"location":"losses/#crossbatchmemory","text":"This wraps a loss function, and implements Cross-Batch Memory for Embedding Learning . It stores embeddings from previous iterations in a queue, and uses them to form more pairs/triplets with the current iteration's embeddings. losses . CrossBatchMemory ( loss , embedding_size , memory_size = 1024 , miner = None ) Parameters : loss : The loss function to be wrapped. For example, you could pass in ContrastiveLoss() . embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. memory_size : The size of the memory queue. miner : An optional tuple miner , which will be used to mine pairs/triplets from the memory queue. Forward function loss_fn ( embeddings , labels , indices_tuple = None , enqueue_idx = None ) As shown above, CrossBatchMemory comes with a 4th argument in its forward function: enqueue_idx : The indices of embeddings that will be added to the memory queue. In other words, only embeddings[enqueue_idx] will be added to memory. This enables CrossBatchMemory to be used in self-supervision frameworks like MoCo . Check out the MoCo on CIFAR100 notebook to see how this works.","title":"CrossBatchMemory"},{"location":"losses/#fastaploss","text":"Deep Metric Learning to Rank losses . FastAPLoss ( num_bins = 10 , ** kwargs ) Parameters : num_bins : The number of soft histogram bins for calculating average precision. The paper suggests using 10. Default distance : LpDistance(normalize_embeddings=True, p=2, power=2) The only compatible distance is LpDistance(normalize_embeddings=True, p=2) . However, the power value can be changed. Default reducer : MeanReducer Reducer input : loss : The loss per element that has at least 1 positive in the batch. Reduction type is \"element\" .","title":"FastAPLoss"},{"location":"losses/#genericpairloss","text":"losses . GenericPairLoss ( mat_based_loss , ** kwargs ) Parameters : mat_based_loss : See required implementations. Required Implementations : # If mat_based_loss is True, then this takes in mat, pos_mask, neg_mask # If False, this takes in pos_pair, neg_pair, indices_tuple def _compute_loss ( self ): raise NotImplementedError","title":"GenericPairLoss"},{"location":"losses/#generalizedliftedstructureloss","text":"This was presented in In Defense of the Triplet Loss for Person Re-Identification . It is a modification of the original LiftedStructureLoss losses . GeneralizedLiftedStructureLoss ( neg_margin = 1 , pos_margin = 0 , ** kwargs ) Equation : Parameters : pos_margin : The margin in the expression e^(D - margin) . The paper uses pos_margin = 0 , which is why this margin does not appear in the above equation. neg_margin : This is m in the above equation. The paper used values between 0.1 and 1. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" .","title":"GeneralizedLiftedStructureLoss"},{"location":"losses/#intrapairvarianceloss","text":"Deep Metric Learning with Tuplet Margin Loss losses . IntraPairVarianceLoss ( pos_eps = 0.01 , neg_eps = 0.01 , ** kwargs ) Equations : Parameters : pos_eps : The epsilon in the L pos equation. The paper uses 0.01. neg_eps : The epsilon in the L neg equation. The paper uses 0.01. You should probably use this in conjunction with another loss, as described in the paper. You can accomplish this by using MultipleLosses : main_loss = losses . TupletMarginLoss () var_loss = losses . IntraPairVarianceLoss () complete_loss = losses . MultipleLosses ([ main_loss , var_loss ], weights = [ 1 , 0.5 ]) Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Default reducer : MeanReducer Reducer input : pos_loss : The loss per positive pair in the batch. Reduction type is \"pos_pair\" . neg_loss : The loss per negative pair in the batch. Reduction type is \"neg_pair\" .","title":"IntraPairVarianceLoss"},{"location":"losses/#largemarginsoftmaxloss","text":"Large-Margin Softmax Loss for Convolutional Neural Networks losses . LargeMarginSoftmaxLoss ( num_classes , embedding_size , margin = 4 , scale = 1 , ** kwargs ) Equations : where Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. margin : An integer which dictates the size of the angular margin. This is m in the above equation. The paper finds m=4 works best. scale : The exponent multiplier in the loss's softmax expression. The paper uses scale = 1 , which is why it does not appear in the above equation. Other info : This also extends WeightRegularizerMixin , so it accepts weight_regularizer , weight_reg_weight , and weight_init_func as optional arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . LargeMarginSoftmaxLoss ( ... ) . to ( torch . device ( 'cuda' )) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () Default distance : CosineSimilarity() This is the only compatible distance. Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" .","title":"LargeMarginSoftmaxLoss"},{"location":"losses/#liftedstructureloss","text":"The original lifted structure loss as presented in Deep Metric Learning via Lifted Structured Feature Embedding losses . LiftedStructureLoss ( neg_margin = 1 , pos_margin = 0 , ** kwargs ): Equation : Parameters : pos_margin : The margin in the expression D_(i,j) - margin . The paper uses pos_margin = 0 , which is why it does not appear in the above equation. neg_margin : This is alpha in the above equation. The paper uses 1. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Default reducer : MeanReducer Reducer input : loss : The loss per positive pair in the batch. Reduction type is \"pos_pair\" .","title":"LiftedStructureLoss"},{"location":"losses/#marginloss","text":"Sampling Matters in Deep Embedding Learning losses . MarginLoss ( margin = 0.2 , nu = 0 , beta = 1.2 , triplets_per_anchor = \"all\" , learn_beta = False , num_classes = None , ** kwargs ) Equations : where Parameters : margin : This is alpha in the above equation. The paper uses 0.2. nu : The regularization weight for the magnitude of beta. beta : This is beta in the above equation. The paper uses 1.2 as the initial value. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used. learn_beta : If True, beta will be a torch.nn.Parameter, which can be optimized using any PyTorch optimizer. num_classes : If not None, then beta will be of size num_classes , so that a separate beta is used for each class during training. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Default reducer : DivisorReducer Reducer input : margin_loss : The loss per triplet in the batch. Reduction type is \"triplet\" . beta_reg_loss : The regularization loss per element in self.beta . Reduction type is \"already_reduced\" if self.num_classes = None . Otherwise it is \"element\" .","title":"MarginLoss"},{"location":"losses/#multiplelosses","text":"This is a simple wrapper for multiple losses. Pass in a list of already-initialized loss functions. Then, when you call forward on this object, it will return the sum of all wrapped losses. losses . MultipleLosses ( losses , miners = None , weights = None ) Parameters : losses : A list or dictionary of initialized loss functions. On the forward call of MultipleLosses, each wrapped loss will be computed, and then the average will be returned. miners : Optional. A list or dictionary of mining functions. This allows you to pair mining functions with loss functions. For example, if losses = [loss_A, loss_B] , and miners = [None, miner_B] then no mining will be done for loss_A , but the output of miner_B will be passed to loss_B . The same logic applies if losses = {\"loss_A\": loss_A, \"loss_B\": loss_B} and miners = {\"loss_B\": miner_B} . weights : Optional. A list or dictionary of loss weights, which will be multiplied by the corresponding losses obtained by the loss functions. The default is to multiply each loss by 1. If losses is a list, then weights must be a list. If losses is a dictionary, weights must contain the same keys as losses .","title":"MultipleLosses"},{"location":"losses/#multisimilarityloss","text":"Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning losses . MultiSimilarityLoss ( alpha = 2 , beta = 50 , base = 0.5 , ** kwargs ) Equation : Parameters : alpha : The weight applied to positive pairs. The paper uses 2. beta : The weight applied to negative pairs. The paper uses 50. base : The offset applied to the exponent in the loss. This is lambda in the above equation. The paper uses 1. Default distance : CosineSimilarity() Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" .","title":"MultiSimilarityLoss"},{"location":"losses/#ncaloss","text":"Neighbourhood Components Analysis losses . NCALoss ( softmax_scale = 1 , ** kwargs ) Equations : where In this implementation, we use -g(A) as the loss. Parameters : softmax_scale : The exponent multiplier in the loss's softmax expression. The paper uses softmax_scale = 1 , which is why it does not appear in the above equations. Default distance : LpDistance(normalize_embeddings=True, p=2, power=2) Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch, that results in a non zero exponent in the cross entropy expression. Reduction type is \"element\" .","title":"NCALoss"},{"location":"losses/#normalizedsoftmaxloss","text":"Classification is a Strong Baseline for Deep Metric Learning losses . NormalizedSoftmaxLoss ( num_classes , embedding_size , temperature = 0.05 , ** kwargs ) Equation : Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. temperature : This is sigma in the above equation. The paper uses 0.05. Other info This also extends WeightRegularizerMixin , so it accepts weight_regularizer , weight_reg_weight , and weight_init_func as optional arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . NormalizedSoftmaxLoss ( ... ) . to ( torch . device ( 'cuda' )) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () Default distance : DotProductSimilarity() Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" .","title":"NormalizedSoftmaxLoss"},{"location":"losses/#npairsloss","text":"Improved Deep Metric Learning with Multi-class N-pair Loss Objective If your batch has more than 2 samples per label, then you should use NTXentLoss . losses . NPairsLoss ( ** kwargs ) Default distance : DotProductSimilarity() Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" .","title":"NPairsLoss"},{"location":"losses/#ntxentloss","text":"This is also known as InfoNCE, and is a generalization of the NPairsLoss . It has been used in self-supervision papers such as: Representation Learning with Contrastive Predictive Coding Momentum Contrast for Unsupervised Visual Representation Learning A Simple Framework for Contrastive Learning of Visual Representations losses . NTXentLoss ( temperature = 0.07 , ** kwargs ) Equation : Parameters : temperature : This is tau in the above equation. The MoCo paper uses 0.07, while SimCLR uses 0.5. Default distance : CosineSimilarity() Default reducer : MeanReducer Reducer input : loss : The loss per positive pair in the batch. Reduction type is \"pos_pair\" .","title":"NTXentLoss"},{"location":"losses/#proxyanchorloss","text":"Proxy Anchor Loss for Deep Metric Learning losses . ProxyAnchorLoss ( num_classes , embedding_size , margin = 0.1 , alpha = 32 , ** kwargs ) Equation : Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. margin : This is delta in the above equation. The paper uses 0.1. alpha : This is alpha in the above equation. The paper uses 32. Other info This also extends WeightRegularizerMixin , so it accepts weight_regularizer , weight_reg_weight , and weight_init_func as optional arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . ProxyAnchorLoss ( ... ) . to ( torch . device ( 'cuda' )) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () Default distance : CosineSimilarity() Default reducer : DivisorReducer Reducer input : pos_loss : The positive pair loss per proxy. Reduction type is \"element\" . neg_loss : The negative pair loss per proxy. Reduction type is \"element\" .","title":"ProxyAnchorLoss"},{"location":"losses/#proxyncaloss","text":"No Fuss Distance Metric Learning using Proxies losses . ProxyNCALoss ( num_classes , embedding_size , softmax_scale = 1 , ** kwargs ) Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. softmax_scale : See NCALoss Other info This also extends WeightRegularizerMixin , so it accepts weight_regularizer , weight_reg_weight , and weight_init_func as optional arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . ProxyNCALoss ( ... ) . to ( torch . device ( 'cuda' )) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () Default distance : LpDistance(normalize_embeddings=True, p=2, power=2) Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch, that results in a non zero exponent in the cross entropy expression. Reduction type is \"element\" .","title":"ProxyNCALoss"},{"location":"losses/#signaltonoiseratiocontrastiveloss","text":"Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning losses . SignalToNoiseRatioContrastiveLoss ( pos_margin = 0 , neg_margin = 1 , ** kwargs ): Parameters : pos_margin : The noise-to-signal ratio over which positive pairs will contribute to the loss. neg_margin : The noise-to-signal ratio under which negative pairs will contribute to the loss. Default distance : SNRDistance() This is the only compatible distance. Default reducer : AvgNonZeroReducer Reducer input : pos_loss : The loss per positive pair in the batch. Reduction type is \"pos_pair\" . neg_loss : The loss per negative pair in the batch. Reduction type is \"neg_pair\" .","title":"SignalToNoiseRatioContrastiveLoss"},{"location":"losses/#softtripleloss","text":"SoftTriple Loss: Deep Metric Learning Without Triplet Sampling losses . SoftTripleLoss ( num_classes , embedding_size , centers_per_class = 10 , la = 20 , gamma = 0.1 , margin = 0.01 , ** kwargs ) Equations : where Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. centers_per_class : The number of weight vectors per class. (The regular cross entropy loss has 1 center per class.) The paper uses 10. la : This is lambda in the above equation. gamma : This is gamma in the above equation. The paper uses 0.1. margin : The is delta in the above equations. The paper uses 0.01. Other info This also extends WeightRegularizerMixin , so it accepts weight_regularizer , weight_reg_weight , and weight_init_func as optional arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . SoftTripleLoss ( ... ) . to ( torch . device ( 'cuda' )) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () Default distance : CosineSimilarity() The distance measure must be inverted. For example, DotProductSimilarity(normalize_embeddings=False) is also compatible. Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" .","title":"SoftTripleLoss"},{"location":"losses/#spherefaceloss","text":"SphereFace: Deep Hypersphere Embedding for Face Recognition losses . SphereFaceLoss ( num_classes , embedding_size , margin = 4 , scale = 1 , ** kwargs ) Parameters : See LargeMarginSoftmaxLoss Other info This also extends WeightRegularizerMixin , so it accepts weight_regularizer , weight_reg_weight , and weight_init_func as optional arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . SphereFaceLoss ( ... ) . to ( torch . device ( 'cuda' )) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () Default distance : CosineSimilarity() This is the only compatible distance. Default reducer : MeanReducer Reducer input : loss : The loss per element in the batch. Reduction type is \"element\" .","title":"SphereFaceLoss"},{"location":"losses/#supconloss","text":"Described in Supervised Contrastive Learning . losses . SupConLoss ( temperature = 0.1 , ** kwargs ) Equation : Parameters : temperature : This is tau in the above equation. The paper uses 0.1. Default distance : CosineSimilarity() Default reducer : AvgNonZeroReducer Reducer input : loss : The loss per element in the batch. If an element has only negative pairs or no pairs, it's ignored thanks to AvgNonZeroReducer . Reduction type is \"element\" .","title":"SupConLoss"},{"location":"losses/#tripletmarginloss","text":"losses . TripletMarginLoss ( margin = 0.05 , swap = False , smooth_loss = False , triplets_per_anchor = \"all\" , ** kwargs ) Equation : Parameters : margin : The desired difference between the anchor-positive distance and the anchor-negative distance. This is m in the above equation. swap : Use the positive-negative distance instead of anchor-negative distance, if it violates the margin more. smooth_loss : Use the log-exp version of the triplet loss triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Default reducer : AvgNonZeroReducer Reducer input : loss : The loss per triplet in the batch. Reduction type is \"triplet\" .","title":"TripletMarginLoss"},{"location":"losses/#tupletmarginloss","text":"Deep Metric Learning with Tuplet Margin Loss losses . TupletMarginLoss ( margin = 5.73 , scale = 64 , ** kwargs ) Equation : Parameters : margin : The angular margin (in degrees) applied to positive pairs. This is beta in the above equation. The paper uses a value of 5.73 degrees (0.1 radians). scale : This is s in the above equation. The paper combines this loss with IntraPairVarianceLoss . You can accomplish this by using MultipleLosses : main_loss = losses . TupletMarginLoss () var_loss = losses . IntraPairVarianceLoss () complete_loss = losses . MultipleLosses ([ main_loss , var_loss ], weights = [ 1 , 0.5 ]) Default distance : CosineSimilarity() This is the only compatible distance. Default reducer : MeanReducer Reducer input : loss : The loss per positive pair in the batch. Reduction type is \"pos_pair\" .","title":"TupletMarginLoss"},{"location":"losses/#weightregularizermixin","text":"Losses can extend this class in addition to BaseMetricLossFunction. You should extend this class if your loss function contains a learnable weight matrix. losses . WeightRegularizerMixin ( weight_init_func = None , weight_regularizer = None , weight_reg_weight = 1 , ** kwargs ) Parameters : weight_init_func : An TorchInitWrapper object, which will be used to initialize the weights of the loss function. weight_regularizer : The regularizer to apply to the loss's learned weights. weight_reg_weight : The amount the regularization loss will be multiplied by. Extended by: ArcFaceLoss CosFaceLoss LargeMarginSoftmaxLoss NormalizedSoftmaxLoss ProxyAnchorLoss ProxyNCALoss SoftTripleLoss SphereFaceLoss","title":"WeightRegularizerMixin"},{"location":"losses/#vicregloss","text":"VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning losses . VICRegLoss ( invariance_lambda = 25 , variance_mu = 25 , covariance_v = 1 , eps = 1e-4 , ** kwargs ) Usage : Unlike other loss functions, VICRegLoss does not accept labels or indices_tuple : loss_fn = VICRegLoss () loss = loss_fn ( embeddings , ref_emb ) Equations : where Parameters : invariance_lambda : The weight of the invariance term. variance_mu : The weight of the variance term. covariance_v : The weight of the covariance term. eps : Small scalar to prevent numerical instability. Default distance : Not applicable. You cannot pass in a distance function. Default reducer : MeanReducer Reducer input : invariance_loss : The MSE loss between embeddings[i] and ref_emb[i] . Reduction type is \"element\" . variance_loss1 : The variance loss for embeddings . Reduction type is \"element\" . variance_loss2 : The variance loss for ref_emb . Reduction type is \"element\" . covariance_loss : The covariance loss. This loss is already reduced to a single value.","title":"VICRegLoss"},{"location":"miners/","text":"Miners \u00b6 Mining functions come in two flavors: Subset Batch Miners take a batch of N embeddings and return a subset n to be used by a tuple miner, or directly by a loss function. Without a subset batch miner, n == N . Tuple Miners take a batch of n embeddings and return k pairs/triplets to be used for calculating the loss: Pair miners output a tuple of size 4: (anchors, positives, anchors, negatives). Triplet miners output a tuple of size 3: (anchors, positives, negatives). Without a tuple miner, loss functions will by default use all possible pairs/triplets in the batch. Almost all current miners are tuple miners. You might be familiar with the terminology: \"online\" and \"offline\" miners. Tuple miners are online, while subset batch miners are a mix between online and offline. Completely offline miners should be implemented as a PyTorch Sampler . Tuple miners are used with loss functions as follows: from pytorch_metric_learning import miners , losses miner_func = miners . SomeMiner () loss_func = losses . SomeLoss () miner_output = miner_func ( embeddings , labels ) losses = loss_func ( embeddings , labels , miner_output ) AngularMiner \u00b6 miners . AngularMiner ( angle = 20 , ** kwargs ) Parameters angle : The miner will return triplets that form an angle greater than this input angle. The angle is computed as defined in the angular loss paper Default distance : LpDistance(p=2, power=1, normalize_embeddings=True) This is the only compatible distance. BaseMiner \u00b6 All miners extend this class and therefore inherit its __init__ parameters. miners . BaseMiner ( collect_stats = False , distance = None ) Parameters : collect_stats : If True, will collect various statistics that may be useful to analyze during experiments. If False, these computations will be skipped. Want to make True the default? Set the global COLLECT_STATS flag. distance : A distance object. If None, then the default distance will be used. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Required Implementations : # Return indices of some form def mine ( self , embeddings , labels , ref_emb , ref_labels ): raise NotImplementedError Note: by default, embeddings == ref_emb and labels == ref_labels . # Validate the output of the miner. def output_assertion ( self , output ): raise NotImplementedError BaseTupleMiner \u00b6 This extends BaseMiner , and most miners extend this class. It outputs a tuple of indices: Pair miners output a tuple of size 4: (anchors, positives, anchors, negatives) Triplet miners output a tuple of size 3: (anchors, positives, negatives) miners . BaseTupleMiner ( ** kwargs ) If you write your own miner, the mine function should work such that anchor indices correspond to embeddings and labels , and all other indices correspond to ref_emb and ref_labels . By default, embeddings == ref_emb and labels == ref_labels , but separating the anchor source from the positive/negative source allows for interesting use cases. For example, see CrossBatchMemory . See custom miners for details on how to write your own miner. BaseSubsetBatchMiner \u00b6 This extends BaseMiner . It outputs indices corresponding to a subset of the input batch. The idea is to use these miners with torch.no_grad(), and with a large input batch size. miners . BaseSubsetBatchMiner ( output_batch_size , ** kwargs ) Parameters output_batch_size : An integer that is the size of the subset that the miner will output. BatchEasyHardMiner \u00b6 Improved Embeddings with Easy Positive Triplet Mining Returns positive and negative pairs according to the specified pos_strategy and neg_strategy . To implement the loss function described in the paper, use this miner in combination with NTXentLoss(temperature=1) . miners . BatchEasyHardMiner ( pos_strategy = BatchEasyHardMiner . EASY , neg_strategy = BatchEasyHardMiner . SEMIHARD , allowed_pos_range = None , allowed_neg_range = None , ** kwargs ): Parameters pos_strategy : one of the following: BatchEasyHardMiner.HARD or \"hard\" : returns the hardest positive sample per anchor. BatchEasyHardMiner.SEMIHARD or \"semihard\" : returns the hardest positive sample per anchor, such that it is closer than the selected negative. BatchEasyHardMiner.EASY or \"easy\" : returns the easiest positive sample per anchor BatchEasyHardMiner.ALL or \"all\" : returns all possible positive pairs neg_strategy : one of the following: BatchEasyHardMiner.HARD or \"hard\" : returns the hardest negative sample per anchor. BatchEasyHardMiner.SEMIHARD or \"semihard\" : returns the hardest negative sample per anchor, such that it is further than the selected positive. BatchEasyHardMiner.EASY or \"easy\" : returns the easiest negative sample per anchor BatchEasyHardMiner.ALL or \"all\" : returns all possible negative pairs allowed_pos_range : Optional tuple containing the allowed range of anchor-positive distances/similarties. For example, allowed_pos_range = (0.2, 1) . If None , then a range is not applied. allowed_neg_range : Optional tuple containing the allowed range of anchor-negative distances/similarties. For example, allowed_neg_range = (0.2, 1) . If None , then a range is not applied. Restrictions pos_strategy and neg_strategy cannot both be set to \"semihard\" If pos_strategy is set to \"semihard\" , then neg_strategy cannot be set to \"all\" , and vice versa. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) BatchHardMiner \u00b6 In Defense of the Triplet Loss for Person Re-Identification For each element in the batch, this miner will find the hardest positive and hardest negative, and use those to form a single triplet. So for a batch size of N, this miner will output N triplets. This miner is equivalent to using miners.BatchEasyHardMiner(pos_strategy=\"hard\", neg_strategy=\"hard\") , and converting the output pairs to triplets. miners . BatchHardMiner ( ** kwargs ) Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) DistanceWeightedMiner \u00b6 Implementation of the miner from Sampling Matters in Deep Embedding Learning . miners . DistanceWeightedMiner ( cutoff = 0.5 , nonzero_loss_cutoff = 1.4 , ** kwargs ) Parameters cutoff : Pairwise distances are clipped to this value if they fall below it. nonzero_loss_cutoff : Pairs that have distance greater than this are discarded. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) This is the only compatible distance. Important note : This miner works well only with low dimensionality embeddings (e.g 64-dim) and L2-normalized distances. Check out UniformHistogramMiner for a miner that is roughly equivalent, but works with embeddings of any dimensionality and any distance metric. EmbeddingsAlreadyPackagedAsTriplets \u00b6 If your embeddings are already ordered sequentially as triplets, then use this miner to force your loss function to use the already-formed triplets. miners . EmbeddingsAlreadyPackagedAsTriplets () For example, here's what a batch size of size 6 should look like: torch . stack ([ anchor1 , positive1 , negative1 , anchor2 , positive2 , negative2 ], dim = 0 ) HDCMiner \u00b6 Hard-Aware Deeply Cascaded Embedding miners . HDCMiner ( filter_percentage = 0.5 , ** kwargs ) Parameters : filter_percentage : The percentage of pairs that will be returned. For example, if filter_percentage is 0.25, then the hardest 25% of pairs will be returned. The pool of pairs is either externally or internally set. See the important methods below for details. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Important methods : # Pairs or triplets extracted from another miner, # and then passed in to HDCMiner using this function def set_idx_externally ( self , external_indices_tuple , labels ): self . a1 , self . p , self . a2 , self . n = lmu . convert_to_pairs ( external_indices_tuple , labels ) self . was_set_externally = True # Reset the internal state of the HDCMiner def reset_idx ( self ): self . a1 , self . p , self . a2 , self . n = None , None , None , None self . was_set_externally = False Example of passing another miner output to HDCMiner : minerA = miners . MultiSimilarityMiner ( epsilon = 0.1 ) minerB = miners . HDCMiner ( filter_percentage = 0.25 ) hard_pairs = minerA ( embeddings , labels ) minerB . set_idx_externally ( hard_pairs , labels ) very_hard_pairs = minerB ( embeddings , labels ) MaximumLossMiner \u00b6 This is a simple subset batch miner . It computes the loss for random subsets of the input batch, num_trials times. Then it returns the subset with the highest loss. miners . MaximumLossMiner ( loss , miner = None , num_trials = 5 , ** kwargs ) Parameters loss : The loss function used to compute the loss. miner : Optional tuple miner which extracts pairs/triplets for the loss function. num_trials : The number of random subsets to try. MultiSimilarityMiner \u00b6 Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning miners . MultiSimilarityMiner ( epsilon = 0.1 , ** kwargs ) Parameters epsilon : Negative pairs are chosen if they have similarity greater than the hardest positive pair, minus this margin (epsilon). Positive pairs are chosen if they have similarity less than the hardest negative pair, plus this margin (epsilon). Default distance : CosineSimilarity() PairMarginMiner \u00b6 Returns positive and negative pairs that violate the specified margins. miners . PairMarginMiner ( pos_margin = 0.2 , neg_margin = 0.8 , ** kwargs ) Parameters pos_margin : The distance (or similarity) over (under) which positive pairs will be chosen. neg_margin : The distance (or similarity) under (over) which negative pairs will be chosen. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) TripletMarginMiner \u00b6 Returns hard, semihard, or all triplets. miners . TripletMarginMiner ( margin = 0.2 , type_of_triplets = \"all\" , ** kwargs ) Parameters margin : The difference between the anchor-positive distance and the anchor-negative distance. type_of_triplets : \"all\" means all triplets that violate the margin \"hard\" is a subset of \"all\", but the negative is closer to the anchor than the positive \"semihard\" is a subset of \"all\", but the negative is further from the anchor than the positive \"easy\" means all triplets that do not violate the margin. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) UniformHistogramMiner \u00b6 Returns pairs that have uniformly distributed distances. This is like DistanceWeightedMiner , except that it works well with high dimension embeddings, and works with any distance metric (not just L2 normalized distance). miners . UniformHistogramMiner ( num_bins = 100 , pos_per_bin = 10 , neg_per_bin = 10 , ** kwargs ): Parameters num_bins : The number of bins to divide the distances into. For example, if the distances for the current batch range from 0 to 2, and num_bins = 100, then each bin will have a width of 0.02. pos_per_bin : The number of positive pairs to mine for each bin. neg_per_bin : The number of negative pairs to mine for each bin. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Example : from pytorch_metric_learning.miners import UniformHistogramMiner from pytorch_metric_learning.distances import SNRDistance miner = UniformHistogramMiner ( num_bins = 100 , pos_per_bin = 25 , neg_per_bin = 33 , distance = SNRDistance (), ) In a given batch, this will: Divide up the positive distances into 100 bins, and return 25 positive pairs per bin, or 0 if no pairs exist in that bin Divide up the negative distances into 100 bins, and return 33 negative pairs per bin, or 0 if no pairs exist in that bin","title":"Miners"},{"location":"miners/#miners","text":"Mining functions come in two flavors: Subset Batch Miners take a batch of N embeddings and return a subset n to be used by a tuple miner, or directly by a loss function. Without a subset batch miner, n == N . Tuple Miners take a batch of n embeddings and return k pairs/triplets to be used for calculating the loss: Pair miners output a tuple of size 4: (anchors, positives, anchors, negatives). Triplet miners output a tuple of size 3: (anchors, positives, negatives). Without a tuple miner, loss functions will by default use all possible pairs/triplets in the batch. Almost all current miners are tuple miners. You might be familiar with the terminology: \"online\" and \"offline\" miners. Tuple miners are online, while subset batch miners are a mix between online and offline. Completely offline miners should be implemented as a PyTorch Sampler . Tuple miners are used with loss functions as follows: from pytorch_metric_learning import miners , losses miner_func = miners . SomeMiner () loss_func = losses . SomeLoss () miner_output = miner_func ( embeddings , labels ) losses = loss_func ( embeddings , labels , miner_output )","title":"Miners"},{"location":"miners/#angularminer","text":"miners . AngularMiner ( angle = 20 , ** kwargs ) Parameters angle : The miner will return triplets that form an angle greater than this input angle. The angle is computed as defined in the angular loss paper Default distance : LpDistance(p=2, power=1, normalize_embeddings=True) This is the only compatible distance.","title":"AngularMiner"},{"location":"miners/#baseminer","text":"All miners extend this class and therefore inherit its __init__ parameters. miners . BaseMiner ( collect_stats = False , distance = None ) Parameters : collect_stats : If True, will collect various statistics that may be useful to analyze during experiments. If False, these computations will be skipped. Want to make True the default? Set the global COLLECT_STATS flag. distance : A distance object. If None, then the default distance will be used. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Required Implementations : # Return indices of some form def mine ( self , embeddings , labels , ref_emb , ref_labels ): raise NotImplementedError Note: by default, embeddings == ref_emb and labels == ref_labels . # Validate the output of the miner. def output_assertion ( self , output ): raise NotImplementedError","title":"BaseMiner"},{"location":"miners/#basetupleminer","text":"This extends BaseMiner , and most miners extend this class. It outputs a tuple of indices: Pair miners output a tuple of size 4: (anchors, positives, anchors, negatives) Triplet miners output a tuple of size 3: (anchors, positives, negatives) miners . BaseTupleMiner ( ** kwargs ) If you write your own miner, the mine function should work such that anchor indices correspond to embeddings and labels , and all other indices correspond to ref_emb and ref_labels . By default, embeddings == ref_emb and labels == ref_labels , but separating the anchor source from the positive/negative source allows for interesting use cases. For example, see CrossBatchMemory . See custom miners for details on how to write your own miner.","title":"BaseTupleMiner"},{"location":"miners/#basesubsetbatchminer","text":"This extends BaseMiner . It outputs indices corresponding to a subset of the input batch. The idea is to use these miners with torch.no_grad(), and with a large input batch size. miners . BaseSubsetBatchMiner ( output_batch_size , ** kwargs ) Parameters output_batch_size : An integer that is the size of the subset that the miner will output.","title":"BaseSubsetBatchMiner"},{"location":"miners/#batcheasyhardminer","text":"Improved Embeddings with Easy Positive Triplet Mining Returns positive and negative pairs according to the specified pos_strategy and neg_strategy . To implement the loss function described in the paper, use this miner in combination with NTXentLoss(temperature=1) . miners . BatchEasyHardMiner ( pos_strategy = BatchEasyHardMiner . EASY , neg_strategy = BatchEasyHardMiner . SEMIHARD , allowed_pos_range = None , allowed_neg_range = None , ** kwargs ): Parameters pos_strategy : one of the following: BatchEasyHardMiner.HARD or \"hard\" : returns the hardest positive sample per anchor. BatchEasyHardMiner.SEMIHARD or \"semihard\" : returns the hardest positive sample per anchor, such that it is closer than the selected negative. BatchEasyHardMiner.EASY or \"easy\" : returns the easiest positive sample per anchor BatchEasyHardMiner.ALL or \"all\" : returns all possible positive pairs neg_strategy : one of the following: BatchEasyHardMiner.HARD or \"hard\" : returns the hardest negative sample per anchor. BatchEasyHardMiner.SEMIHARD or \"semihard\" : returns the hardest negative sample per anchor, such that it is further than the selected positive. BatchEasyHardMiner.EASY or \"easy\" : returns the easiest negative sample per anchor BatchEasyHardMiner.ALL or \"all\" : returns all possible negative pairs allowed_pos_range : Optional tuple containing the allowed range of anchor-positive distances/similarties. For example, allowed_pos_range = (0.2, 1) . If None , then a range is not applied. allowed_neg_range : Optional tuple containing the allowed range of anchor-negative distances/similarties. For example, allowed_neg_range = (0.2, 1) . If None , then a range is not applied. Restrictions pos_strategy and neg_strategy cannot both be set to \"semihard\" If pos_strategy is set to \"semihard\" , then neg_strategy cannot be set to \"all\" , and vice versa. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1)","title":"BatchEasyHardMiner"},{"location":"miners/#batchhardminer","text":"In Defense of the Triplet Loss for Person Re-Identification For each element in the batch, this miner will find the hardest positive and hardest negative, and use those to form a single triplet. So for a batch size of N, this miner will output N triplets. This miner is equivalent to using miners.BatchEasyHardMiner(pos_strategy=\"hard\", neg_strategy=\"hard\") , and converting the output pairs to triplets. miners . BatchHardMiner ( ** kwargs ) Default distance : LpDistance(normalize_embeddings=True, p=2, power=1)","title":"BatchHardMiner"},{"location":"miners/#distanceweightedminer","text":"Implementation of the miner from Sampling Matters in Deep Embedding Learning . miners . DistanceWeightedMiner ( cutoff = 0.5 , nonzero_loss_cutoff = 1.4 , ** kwargs ) Parameters cutoff : Pairwise distances are clipped to this value if they fall below it. nonzero_loss_cutoff : Pairs that have distance greater than this are discarded. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) This is the only compatible distance. Important note : This miner works well only with low dimensionality embeddings (e.g 64-dim) and L2-normalized distances. Check out UniformHistogramMiner for a miner that is roughly equivalent, but works with embeddings of any dimensionality and any distance metric.","title":"DistanceWeightedMiner"},{"location":"miners/#embeddingsalreadypackagedastriplets","text":"If your embeddings are already ordered sequentially as triplets, then use this miner to force your loss function to use the already-formed triplets. miners . EmbeddingsAlreadyPackagedAsTriplets () For example, here's what a batch size of size 6 should look like: torch . stack ([ anchor1 , positive1 , negative1 , anchor2 , positive2 , negative2 ], dim = 0 )","title":"EmbeddingsAlreadyPackagedAsTriplets"},{"location":"miners/#hdcminer","text":"Hard-Aware Deeply Cascaded Embedding miners . HDCMiner ( filter_percentage = 0.5 , ** kwargs ) Parameters : filter_percentage : The percentage of pairs that will be returned. For example, if filter_percentage is 0.25, then the hardest 25% of pairs will be returned. The pool of pairs is either externally or internally set. See the important methods below for details. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Important methods : # Pairs or triplets extracted from another miner, # and then passed in to HDCMiner using this function def set_idx_externally ( self , external_indices_tuple , labels ): self . a1 , self . p , self . a2 , self . n = lmu . convert_to_pairs ( external_indices_tuple , labels ) self . was_set_externally = True # Reset the internal state of the HDCMiner def reset_idx ( self ): self . a1 , self . p , self . a2 , self . n = None , None , None , None self . was_set_externally = False Example of passing another miner output to HDCMiner : minerA = miners . MultiSimilarityMiner ( epsilon = 0.1 ) minerB = miners . HDCMiner ( filter_percentage = 0.25 ) hard_pairs = minerA ( embeddings , labels ) minerB . set_idx_externally ( hard_pairs , labels ) very_hard_pairs = minerB ( embeddings , labels )","title":"HDCMiner"},{"location":"miners/#maximumlossminer","text":"This is a simple subset batch miner . It computes the loss for random subsets of the input batch, num_trials times. Then it returns the subset with the highest loss. miners . MaximumLossMiner ( loss , miner = None , num_trials = 5 , ** kwargs ) Parameters loss : The loss function used to compute the loss. miner : Optional tuple miner which extracts pairs/triplets for the loss function. num_trials : The number of random subsets to try.","title":"MaximumLossMiner"},{"location":"miners/#multisimilarityminer","text":"Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning miners . MultiSimilarityMiner ( epsilon = 0.1 , ** kwargs ) Parameters epsilon : Negative pairs are chosen if they have similarity greater than the hardest positive pair, minus this margin (epsilon). Positive pairs are chosen if they have similarity less than the hardest negative pair, plus this margin (epsilon). Default distance : CosineSimilarity()","title":"MultiSimilarityMiner"},{"location":"miners/#pairmarginminer","text":"Returns positive and negative pairs that violate the specified margins. miners . PairMarginMiner ( pos_margin = 0.2 , neg_margin = 0.8 , ** kwargs ) Parameters pos_margin : The distance (or similarity) over (under) which positive pairs will be chosen. neg_margin : The distance (or similarity) under (over) which negative pairs will be chosen. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1)","title":"PairMarginMiner"},{"location":"miners/#tripletmarginminer","text":"Returns hard, semihard, or all triplets. miners . TripletMarginMiner ( margin = 0.2 , type_of_triplets = \"all\" , ** kwargs ) Parameters margin : The difference between the anchor-positive distance and the anchor-negative distance. type_of_triplets : \"all\" means all triplets that violate the margin \"hard\" is a subset of \"all\", but the negative is closer to the anchor than the positive \"semihard\" is a subset of \"all\", but the negative is further from the anchor than the positive \"easy\" means all triplets that do not violate the margin. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1)","title":"TripletMarginMiner"},{"location":"miners/#uniformhistogramminer","text":"Returns pairs that have uniformly distributed distances. This is like DistanceWeightedMiner , except that it works well with high dimension embeddings, and works with any distance metric (not just L2 normalized distance). miners . UniformHistogramMiner ( num_bins = 100 , pos_per_bin = 10 , neg_per_bin = 10 , ** kwargs ): Parameters num_bins : The number of bins to divide the distances into. For example, if the distances for the current batch range from 0 to 2, and num_bins = 100, then each bin will have a width of 0.02. pos_per_bin : The number of positive pairs to mine for each bin. neg_per_bin : The number of negative pairs to mine for each bin. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Example : from pytorch_metric_learning.miners import UniformHistogramMiner from pytorch_metric_learning.distances import SNRDistance miner = UniformHistogramMiner ( num_bins = 100 , pos_per_bin = 25 , neg_per_bin = 33 , distance = SNRDistance (), ) In a given batch, this will: Divide up the positive distances into 100 bins, and return 25 positive pairs per bin, or 0 if no pairs exist in that bin Divide up the negative distances into 100 bins, and return 33 negative pairs per bin, or 0 if no pairs exist in that bin","title":"UniformHistogramMiner"},{"location":"reducers/","text":"Reducers \u00b6 Reducers specify how to go from many loss values to a single loss value. For example, the ContrastiveLoss computes a loss for every positive and negative pair in a batch. A reducer will take all these per-pair losses, and reduce them to a single value. Here's where reducers fit in this library's flow of filters and computations: Your Data --> Sampler --> Miner --> Loss --> Reducer --> Final loss value Reducers are passed into loss functions like this: from pytorch_metric_learning import losses , reducers reducer = reducers . SomeReducer () loss_func = losses . SomeLoss ( reducer = reducer ) loss = loss_func ( embeddings , labels ) # in your training for-loop Internally, the loss function creates a dictionary that contains the losses and other information. The reducer takes this dictionary, performs the reduction, and returns a single value on which .backward() can be called. Most reducers are written such that they can be passed into any loss function. AvgNonZeroReducer \u00b6 This computes the average loss, using only the losses that are greater than 0. For example, if the losses are [0, 2, 0, 3] , then this reducer will return 2.5 . reducers . AvgNonZeroReducer ( ** kwargs ) This class is equivalent to using ThresholdReducer(low=0) . See ThresholdReducer . BaseReducer \u00b6 All reducers extend this class. reducers . BaseReducer ( collect_stats = False ) Parameters : collect_stats : If True, will collect various statistics that may be useful to analyze during experiments. If False, these computations will be skipped. Want to make True the default? Set the global COLLECT_STATS flag. ClassWeightedReducer \u00b6 This multiplies each loss by a class weight, and then takes the average. reducers . ClassWeightedReducer ( weights , ** kwargs ) Parameters : weights : A tensor of weights, where weights[i] is the weight for the ith class. DivisorReducer \u00b6 This divides each loss by a custom value specified inside the loss function. This is useful if you want to hardcode a reduction behavior in your loss function (i.e. by using DivisorReducer), while still having the option to use other reducers. reducers . DivisorReducer ( ** kwargs ) To use this reducer, the loss function must include divisor in its loss dictionary. For example, the ProxyAnchorLoss uses DivisorReducer by default, and returns the following dictionary: loss_dict = { \"pos_loss\" : { \"losses\" : pos_term . squeeze ( 0 ), \"indices\" : loss_indices , \"reduction_type\" : \"element\" , \"divisor\" : len ( with_pos_proxies ), }, \"neg_loss\" : { \"losses\" : neg_term . squeeze ( 0 ), \"indices\" : loss_indices , \"reduction_type\" : \"element\" , \"divisor\" : self . num_classes , }, } DoNothingReducer \u00b6 This returns its input. In other words, no reduction is performed. The output will be the loss dictionary that is passed into it. reducers . DoNothingReducer ( ** kwargs ) MeanReducer \u00b6 This will return the average of the losses. reducers . MeanReducer ( ** kwargs ) MultipleReducers \u00b6 This wraps multiple reducers. Each reducer is applied to a different sub-loss, as specified in the host loss function. Then the reducer outputs are summed to obtain the final loss. reducers . MultipleReducers ( reducers , default_reducer = None , ** kwargs ) Parameters : reducers : A dictionary mapping from strings to reducers. The strings must match sub-loss names of the host loss function. default_reducer : This reducer will be used for any sub-losses that are not included in the keys of reducers . If None, then MeanReducer() will be the default. Example usage : The ContrastiveLoss has two sub-losses: pos_loss for the positive pairs, and neg_loss for the negative pairs. In this example, a ThresholdReducer is used for the pos_loss and a MeanReducer is used for the neg_loss . from pytorch_metric_learning.losses import ContrastiveLoss from pytorch_metric_learning.reducers import MultipleReducers , ThresholdReducer , MeanReducer reducer_dict = { \"pos_loss\" : ThresholdReducer ( 0.1 ), \"neg_loss\" : MeanReducer ()} reducer = MultipleReducers ( reducer_dict ) loss_func = ContrastiveLoss ( reducer = reducer ) PerAnchorReducer \u00b6 This converts unreduced pairs to unreduced elements. For example, NTXentLoss returns losses per positive pair. If you used PerAnchorReducer with NTXentLoss, then the losses per pair would first be converted to losses per batch element, before being passed to the inner reducer. def aggregation_func ( x , num_per_row ): zero_denom = num_per_row == 0 x = torch . sum ( x , dim = 1 ) / num_per_row x [ zero_denom ] = 0 return x reducers . PerAnchorReducer ( reducer = None , aggregation_func = aggregation_func , ** kwargs ): Parameters : reducer : The reducer that will be fed per-element losses. The default is MeanReducer aggregation_func : A function that takes in (x, num_per_row) and returns a loss per row of x . The default is the aggregation_func defined in the code snippet above. It returns the mean per row. x is an NxN array of pairwise losses, where N is the batch size. num_per_row is a size N array which indicates how many non-zero losses there are per-row of x . ThresholdReducer \u00b6 This computes the average loss, using only the losses that fall within a specified range. reducers . ThresholdReducer ( low = None , high = None ** kwargs ) At least one of low or high must be specified. Parameters : low : Losses less than this value will be ignored. high : Losses greater than this value will be ignored. Examples : ThresholdReducer(low=6) : the filter is losses > 6 If the losses are [3, 7, 1, 13, 5] , then this reducer will return (7+13)/2 = 10 . ThresholdReducer(high=6) : the filter is losses < 6 If the losses are [3, 7, 1, 13, 5] , then this reducer will return (1+3+5)/3 = 3 . ThresholdReducer(low=6, high=12) : the filter is 6 < losses < 12 If the losses are [3, 7, 1, 13, 5] , then this reducer will return (7)/1 = 7 .","title":"Reducers"},{"location":"reducers/#reducers","text":"Reducers specify how to go from many loss values to a single loss value. For example, the ContrastiveLoss computes a loss for every positive and negative pair in a batch. A reducer will take all these per-pair losses, and reduce them to a single value. Here's where reducers fit in this library's flow of filters and computations: Your Data --> Sampler --> Miner --> Loss --> Reducer --> Final loss value Reducers are passed into loss functions like this: from pytorch_metric_learning import losses , reducers reducer = reducers . SomeReducer () loss_func = losses . SomeLoss ( reducer = reducer ) loss = loss_func ( embeddings , labels ) # in your training for-loop Internally, the loss function creates a dictionary that contains the losses and other information. The reducer takes this dictionary, performs the reduction, and returns a single value on which .backward() can be called. Most reducers are written such that they can be passed into any loss function.","title":"Reducers"},{"location":"reducers/#avgnonzeroreducer","text":"This computes the average loss, using only the losses that are greater than 0. For example, if the losses are [0, 2, 0, 3] , then this reducer will return 2.5 . reducers . AvgNonZeroReducer ( ** kwargs ) This class is equivalent to using ThresholdReducer(low=0) . See ThresholdReducer .","title":"AvgNonZeroReducer"},{"location":"reducers/#basereducer","text":"All reducers extend this class. reducers . BaseReducer ( collect_stats = False ) Parameters : collect_stats : If True, will collect various statistics that may be useful to analyze during experiments. If False, these computations will be skipped. Want to make True the default? Set the global COLLECT_STATS flag.","title":"BaseReducer"},{"location":"reducers/#classweightedreducer","text":"This multiplies each loss by a class weight, and then takes the average. reducers . ClassWeightedReducer ( weights , ** kwargs ) Parameters : weights : A tensor of weights, where weights[i] is the weight for the ith class.","title":"ClassWeightedReducer"},{"location":"reducers/#divisorreducer","text":"This divides each loss by a custom value specified inside the loss function. This is useful if you want to hardcode a reduction behavior in your loss function (i.e. by using DivisorReducer), while still having the option to use other reducers. reducers . DivisorReducer ( ** kwargs ) To use this reducer, the loss function must include divisor in its loss dictionary. For example, the ProxyAnchorLoss uses DivisorReducer by default, and returns the following dictionary: loss_dict = { \"pos_loss\" : { \"losses\" : pos_term . squeeze ( 0 ), \"indices\" : loss_indices , \"reduction_type\" : \"element\" , \"divisor\" : len ( with_pos_proxies ), }, \"neg_loss\" : { \"losses\" : neg_term . squeeze ( 0 ), \"indices\" : loss_indices , \"reduction_type\" : \"element\" , \"divisor\" : self . num_classes , }, }","title":"DivisorReducer"},{"location":"reducers/#donothingreducer","text":"This returns its input. In other words, no reduction is performed. The output will be the loss dictionary that is passed into it. reducers . DoNothingReducer ( ** kwargs )","title":"DoNothingReducer"},{"location":"reducers/#meanreducer","text":"This will return the average of the losses. reducers . MeanReducer ( ** kwargs )","title":"MeanReducer"},{"location":"reducers/#multiplereducers","text":"This wraps multiple reducers. Each reducer is applied to a different sub-loss, as specified in the host loss function. Then the reducer outputs are summed to obtain the final loss. reducers . MultipleReducers ( reducers , default_reducer = None , ** kwargs ) Parameters : reducers : A dictionary mapping from strings to reducers. The strings must match sub-loss names of the host loss function. default_reducer : This reducer will be used for any sub-losses that are not included in the keys of reducers . If None, then MeanReducer() will be the default. Example usage : The ContrastiveLoss has two sub-losses: pos_loss for the positive pairs, and neg_loss for the negative pairs. In this example, a ThresholdReducer is used for the pos_loss and a MeanReducer is used for the neg_loss . from pytorch_metric_learning.losses import ContrastiveLoss from pytorch_metric_learning.reducers import MultipleReducers , ThresholdReducer , MeanReducer reducer_dict = { \"pos_loss\" : ThresholdReducer ( 0.1 ), \"neg_loss\" : MeanReducer ()} reducer = MultipleReducers ( reducer_dict ) loss_func = ContrastiveLoss ( reducer = reducer )","title":"MultipleReducers"},{"location":"reducers/#peranchorreducer","text":"This converts unreduced pairs to unreduced elements. For example, NTXentLoss returns losses per positive pair. If you used PerAnchorReducer with NTXentLoss, then the losses per pair would first be converted to losses per batch element, before being passed to the inner reducer. def aggregation_func ( x , num_per_row ): zero_denom = num_per_row == 0 x = torch . sum ( x , dim = 1 ) / num_per_row x [ zero_denom ] = 0 return x reducers . PerAnchorReducer ( reducer = None , aggregation_func = aggregation_func , ** kwargs ): Parameters : reducer : The reducer that will be fed per-element losses. The default is MeanReducer aggregation_func : A function that takes in (x, num_per_row) and returns a loss per row of x . The default is the aggregation_func defined in the code snippet above. It returns the mean per row. x is an NxN array of pairwise losses, where N is the batch size. num_per_row is a size N array which indicates how many non-zero losses there are per-row of x .","title":"PerAnchorReducer"},{"location":"reducers/#thresholdreducer","text":"This computes the average loss, using only the losses that fall within a specified range. reducers . ThresholdReducer ( low = None , high = None ** kwargs ) At least one of low or high must be specified. Parameters : low : Losses less than this value will be ignored. high : Losses greater than this value will be ignored. Examples : ThresholdReducer(low=6) : the filter is losses > 6 If the losses are [3, 7, 1, 13, 5] , then this reducer will return (7+13)/2 = 10 . ThresholdReducer(high=6) : the filter is losses < 6 If the losses are [3, 7, 1, 13, 5] , then this reducer will return (1+3+5)/3 = 3 . ThresholdReducer(low=6, high=12) : the filter is 6 < losses < 12 If the losses are [3, 7, 1, 13, 5] , then this reducer will return (7)/1 = 7 .","title":"ThresholdReducer"},{"location":"regularizers/","text":"Regularizers \u00b6 Regularizers are applied to weights and embeddings without the need for labels or tuples. Here is an example of a weight regularizer being passed to a loss function. from pytorch_metric_learning import losses , regularizers R = regularizers . RegularFaceRegularizer () loss = losses . ArcFaceLoss ( margin = 30 , num_classes = 100 , embedding_size = 128 , weight_regularizer = R ) BaseRegularizer \u00b6 regularizers . BaseWeightRegularizer ( collect_stats = False , reducer = None , distance = None ) An object that extends this class can be passed as the embedding_regularizer into any loss function. It can also be passed as the weight_regularizer into any class that extends WeightRegularizerMixin . Parameters collect_stats : If True, will collect various statistics that may be useful to analyze during experiments. If False, these computations will be skipped. Want to make True the default? Set the global COLLECT_STATS flag. reducer : A reducer object. If None, then the default reducer will be used. distance : A distance object. If None, then the default distance will be used. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Default reducer : MeanReducer CenterInvariantRegularizer \u00b6 Deep Face Recognition with Center Invariant Loss This encourages unnormalized embeddings or weights to all have the same Lp norm. regularizers . CenterInvariantRegularizer ( ** kwargs ) Default distance : LpDistance(normalize_embeddings=False, p=2, power=1) The distance must be LpDistance(normalize_embeddings=False, power=1) . However, p can be changed. Default reducer : MeanReducer LpRegularizer \u00b6 This encourages embeddings/weights to have a small Lp norm. regularizers . LpRegularizer ( p = 2 , power = 1 , ** kwargs ) Parameters p : The type of norm. For example, p=1 is the Manhattan distance, and p=2 is Euclidean distance. Default distance : This regularizer does not use a distance object, so setting this parameter will have no effect. Default reducer : MeanReducer RegularFaceRegularizer \u00b6 RegularFace: Deep Face Recognition via Exclusive Regularization This should be applied as a weight regularizer. It penalizes class vectors that are very close together. regularizers . RegularFaceRegularizer ( ** kwargs ) Default distance : CosineSimilarity() Only inverted distances are compatible. For example, DotProductSimilarity() also works. Default reducer : MeanReducer SparseCentersRegularizer \u00b6 SoftTriple Loss: Deep Metric Learning Without Triplet Sampling This should be applied as a weight regularizer. It encourages multiple class centers to \"merge\", i.e. group together. regularizers . SparseCentersRegularizer ( num_classes , centers_per_class , ** kwargs ) Parameters num_classes : The number of classes in your training dataset. centers_per_class : The number of rows in the weight matrix that correspond to 1 class. Default distance : CosineSimilarity() This is the only compatible distance. Default reducer : DivisorReducer ZeroMeanRegularizer \u00b6 Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning regularizers . ZeroMeanRegularizer ( num_classes , centers_per_class , ** kwargs ) Equation In this equation, N is the batch size, M is the size of each embedding. Default distance : This regularizer does not use a distance object, so setting this parameter will have no effect. Default reducer : MeanReducer","title":"Regularizers"},{"location":"regularizers/#regularizers","text":"Regularizers are applied to weights and embeddings without the need for labels or tuples. Here is an example of a weight regularizer being passed to a loss function. from pytorch_metric_learning import losses , regularizers R = regularizers . RegularFaceRegularizer () loss = losses . ArcFaceLoss ( margin = 30 , num_classes = 100 , embedding_size = 128 , weight_regularizer = R )","title":"Regularizers"},{"location":"regularizers/#baseregularizer","text":"regularizers . BaseWeightRegularizer ( collect_stats = False , reducer = None , distance = None ) An object that extends this class can be passed as the embedding_regularizer into any loss function. It can also be passed as the weight_regularizer into any class that extends WeightRegularizerMixin . Parameters collect_stats : If True, will collect various statistics that may be useful to analyze during experiments. If False, these computations will be skipped. Want to make True the default? Set the global COLLECT_STATS flag. reducer : A reducer object. If None, then the default reducer will be used. distance : A distance object. If None, then the default distance will be used. Default distance : LpDistance(normalize_embeddings=True, p=2, power=1) Default reducer : MeanReducer","title":"BaseRegularizer"},{"location":"regularizers/#centerinvariantregularizer","text":"Deep Face Recognition with Center Invariant Loss This encourages unnormalized embeddings or weights to all have the same Lp norm. regularizers . CenterInvariantRegularizer ( ** kwargs ) Default distance : LpDistance(normalize_embeddings=False, p=2, power=1) The distance must be LpDistance(normalize_embeddings=False, power=1) . However, p can be changed. Default reducer : MeanReducer","title":"CenterInvariantRegularizer"},{"location":"regularizers/#lpregularizer","text":"This encourages embeddings/weights to have a small Lp norm. regularizers . LpRegularizer ( p = 2 , power = 1 , ** kwargs ) Parameters p : The type of norm. For example, p=1 is the Manhattan distance, and p=2 is Euclidean distance. Default distance : This regularizer does not use a distance object, so setting this parameter will have no effect. Default reducer : MeanReducer","title":"LpRegularizer"},{"location":"regularizers/#regularfaceregularizer","text":"RegularFace: Deep Face Recognition via Exclusive Regularization This should be applied as a weight regularizer. It penalizes class vectors that are very close together. regularizers . RegularFaceRegularizer ( ** kwargs ) Default distance : CosineSimilarity() Only inverted distances are compatible. For example, DotProductSimilarity() also works. Default reducer : MeanReducer","title":"RegularFaceRegularizer"},{"location":"regularizers/#sparsecentersregularizer","text":"SoftTriple Loss: Deep Metric Learning Without Triplet Sampling This should be applied as a weight regularizer. It encourages multiple class centers to \"merge\", i.e. group together. regularizers . SparseCentersRegularizer ( num_classes , centers_per_class , ** kwargs ) Parameters num_classes : The number of classes in your training dataset. centers_per_class : The number of rows in the weight matrix that correspond to 1 class. Default distance : CosineSimilarity() This is the only compatible distance. Default reducer : DivisorReducer","title":"SparseCentersRegularizer"},{"location":"regularizers/#zeromeanregularizer","text":"Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning regularizers . ZeroMeanRegularizer ( num_classes , centers_per_class , ** kwargs ) Equation In this equation, N is the batch size, M is the size of each embedding. Default distance : This regularizer does not use a distance object, so setting this parameter will have no effect. Default reducer : MeanReducer","title":"ZeroMeanRegularizer"},{"location":"samplers/","text":"Samplers \u00b6 Samplers are just extensions of the torch.utils.data.Sampler class, i.e. they are passed to a PyTorch Dataloader. The purpose of samplers is to determine how batches should be formed. This is also where any offline pair or triplet miners should exist. MPerClassSampler \u00b6 At every iteration, this will return m samples per class, assuming that the batch size is a multiple of m . For example, if your dataloader's batch size is 100, and m = 5, then 20 classes with 5 samples each will be returned. Note that if batch_size is not specified, then most batches will have m samples per class, but it's not guaranteed for every batch. samplers . MPerClassSampler ( labels , m , batch_size = None , length_before_new_iter = 100000 ) Parameters : labels : The list of labels for your dataset, i.e. the labels[x] should be the label of the xth element in your dataset. m : The number of samples per class to fetch at every iteration. If a class has less than m samples, then there will be duplicates in the returned batch. batch_size : Optional. If specified, then every batch is guaranteed to have m samples per class. There are a few restrictions on this value: batch_size must be a multiple of m length_before_new_iter >= batch_size must be true m * (number of unique labels) >= batch_size must be true length_before_new_iter : How many iterations will pass before a new iterable is created. HierarchicalSampler \u00b6 Implementation of the sampler used in Deep Metric Learning to Rank . It will do the following per batch: Randomly select X super classes. For each super class, randomly select Y samples from Z classes, such that Y * Z equals the batch size divided by X. (X, Y, and the batch size are controllable parameters. See below for details.) This is a BatchSampler , so you should pass it into your dataloader as the batch_sampler parameter. samplers . HierarchicalSampler ( labels , batch_size , samples_per_class , batches_per_super_tuple = 4 , super_classes_per_batch = 2 , inner_label = 0 , outer_label = 1 , ) Parameters : labels : 2D array, where rows correspond to elements, and columns correspond to the hierarchical labels. batch_size : because this is a BatchSampler the batch size must be specified. batch_size must be a multiple of super_classes_per_batch and samples_per_class samples_per_class : number of samples per class per batch. Corresponds to Y in the above explanation. You can also set this to \"all\" to use all elements of a class, but this is suitable only for few-shot datasets. batches_per_super_tuple : number of batches to create for each tuple of super classes. This affects the length of the iterator returned by the sampler. super_classes_per_batch : the number of super classes per batch. Corresponds to X in the above explanation. inner_label : column index of labels corresponding to classes. outer_label : column index of labels corresponding to super classes. TuplesToWeightsSampler \u00b6 This is a simple offline miner. It does the following: Take a random subset of the dataset, if you provide subset_size . Use a specified miner to mine tuples from the subset dataset. Compute weights based on how often each element appears in the mined tuples. Randomly sample, using the weights as probabilities. samplers . TuplesToWeightsSampler ( model , miner , dataset , subset_size = None , ** tester_kwargs ) Parameters : model : This model will be used to compute embeddings. miner : This miner will find hard tuples from the computed embeddings. dataset : The dataset you want to sample from. subset_size : Optional. If None , then the entire dataset will be mined, and the iterable will have length len(dataset) . Most likely though, you will run out of memory if you do this. So to avoid that, set subset_size to a number of embeddings that can be passed to the miner without running out of memory. Then a random subset of dataset of size subset_size will be used for mining. The iterable will also have length subset_size . tester_kwargs : Any other keyword options will be passed to BaseTester , which is used internally to compute embeddings. This allows you to set things like dataloader_num_workers etc, if you want to. FixedSetOfTriplets \u00b6 When initialized, this class creates a fixed set of triplets. This is useful for determining the performance of algorithms in cases where the only ground truth data is a set of triplets. samplers . FixedSetOfTriplets ( labels , num_triplets ) Parameters : labels : The list of labels for your dataset, i.e. the labels[x] should be the label of the xth element in your dataset. num_triplets : The number of triplets to create.","title":"Samplers"},{"location":"samplers/#samplers","text":"Samplers are just extensions of the torch.utils.data.Sampler class, i.e. they are passed to a PyTorch Dataloader. The purpose of samplers is to determine how batches should be formed. This is also where any offline pair or triplet miners should exist.","title":"Samplers"},{"location":"samplers/#mperclasssampler","text":"At every iteration, this will return m samples per class, assuming that the batch size is a multiple of m . For example, if your dataloader's batch size is 100, and m = 5, then 20 classes with 5 samples each will be returned. Note that if batch_size is not specified, then most batches will have m samples per class, but it's not guaranteed for every batch. samplers . MPerClassSampler ( labels , m , batch_size = None , length_before_new_iter = 100000 ) Parameters : labels : The list of labels for your dataset, i.e. the labels[x] should be the label of the xth element in your dataset. m : The number of samples per class to fetch at every iteration. If a class has less than m samples, then there will be duplicates in the returned batch. batch_size : Optional. If specified, then every batch is guaranteed to have m samples per class. There are a few restrictions on this value: batch_size must be a multiple of m length_before_new_iter >= batch_size must be true m * (number of unique labels) >= batch_size must be true length_before_new_iter : How many iterations will pass before a new iterable is created.","title":"MPerClassSampler"},{"location":"samplers/#hierarchicalsampler","text":"Implementation of the sampler used in Deep Metric Learning to Rank . It will do the following per batch: Randomly select X super classes. For each super class, randomly select Y samples from Z classes, such that Y * Z equals the batch size divided by X. (X, Y, and the batch size are controllable parameters. See below for details.) This is a BatchSampler , so you should pass it into your dataloader as the batch_sampler parameter. samplers . HierarchicalSampler ( labels , batch_size , samples_per_class , batches_per_super_tuple = 4 , super_classes_per_batch = 2 , inner_label = 0 , outer_label = 1 , ) Parameters : labels : 2D array, where rows correspond to elements, and columns correspond to the hierarchical labels. batch_size : because this is a BatchSampler the batch size must be specified. batch_size must be a multiple of super_classes_per_batch and samples_per_class samples_per_class : number of samples per class per batch. Corresponds to Y in the above explanation. You can also set this to \"all\" to use all elements of a class, but this is suitable only for few-shot datasets. batches_per_super_tuple : number of batches to create for each tuple of super classes. This affects the length of the iterator returned by the sampler. super_classes_per_batch : the number of super classes per batch. Corresponds to X in the above explanation. inner_label : column index of labels corresponding to classes. outer_label : column index of labels corresponding to super classes.","title":"HierarchicalSampler"},{"location":"samplers/#tuplestoweightssampler","text":"This is a simple offline miner. It does the following: Take a random subset of the dataset, if you provide subset_size . Use a specified miner to mine tuples from the subset dataset. Compute weights based on how often each element appears in the mined tuples. Randomly sample, using the weights as probabilities. samplers . TuplesToWeightsSampler ( model , miner , dataset , subset_size = None , ** tester_kwargs ) Parameters : model : This model will be used to compute embeddings. miner : This miner will find hard tuples from the computed embeddings. dataset : The dataset you want to sample from. subset_size : Optional. If None , then the entire dataset will be mined, and the iterable will have length len(dataset) . Most likely though, you will run out of memory if you do this. So to avoid that, set subset_size to a number of embeddings that can be passed to the miner without running out of memory. Then a random subset of dataset of size subset_size will be used for mining. The iterable will also have length subset_size . tester_kwargs : Any other keyword options will be passed to BaseTester , which is used internally to compute embeddings. This allows you to set things like dataloader_num_workers etc, if you want to.","title":"TuplesToWeightsSampler"},{"location":"samplers/#fixedsetoftriplets","text":"When initialized, this class creates a fixed set of triplets. This is useful for determining the performance of algorithms in cases where the only ground truth data is a set of triplets. samplers . FixedSetOfTriplets ( labels , num_triplets ) Parameters : labels : The list of labels for your dataset, i.e. the labels[x] should be the label of the xth element in your dataset. num_triplets : The number of triplets to create.","title":"FixedSetOfTriplets"},{"location":"testers/","text":"Testers \u00b6 Testers take your model and dataset, and compute nearest-neighbor based accuracy metrics. Note that the testers require the faiss package , which you can install with conda. In general, testers are used as follows: from pytorch_metric_learning import testers t = testers . SomeTestingFunction ( * args , ** kwargs ) dataset_dict = { \"train\" : train_dataset , \"val\" : val_dataset } all_accuracies = tester . test ( dataset_dict , epoch , model ) # Or if your model is composed of a trunk + embedder all_accuracies = tester . test ( dataset_dict , epoch , trunk , embedder ) You can perform custom actions by writing an end-of-testing hook (see the documentation for BaseTester ), and you can access the test results directly via the all_accuracies attribute: def end_of_testing_hook ( tester ): print ( tester . all_accuracies ) This will print out a dictionary of accuracy metrics, per dataset split. You'll see something like this: { \"train\" : { \"AMI_level0\" : 0.53 , ... }, \"val\" : { \"AMI_level0\" : 0.44 , ... }} Each of the accuracy metric names is appended with level0 , which refers to the 0th label hierarchy level (see the documentation for BaseTester ). This is only relevant if you're dealing with multi-label datasets. For an explanation of the default accuracy metrics, see the AccuracyCalculator documentation . Testing splits \u00b6 By default, every dataset in dataset_dict will be evaluated using itself as the query and reference (on which to find nearest neighbors). More flexibility is allowed with the optional argument splits_to_eval taken by tester.test() . splits_to_eval is a list of (query_split, [list_of_reference_splits]) tuples. For example, let's say your dataset_dict has two keys: \"dataset_a\" and \"train\" . The default splits_to_eval = None is equivalent to: splits_to_eval = [( 'dataset_a' , [ 'dataset_a' ]), ( 'train' , [ 'train' ])] dataset_a as the query, and train as the reference: splits_to_eval = [( 'dataset_a' , [ 'train' ])] dataset_a as the query, and dataset_a + train as the reference: splits_to_eval = [( 'dataset_a' , [ 'dataset_a' , 'train' ])] BaseTester \u00b6 All trainers extend this class and therefore inherit its __init__ arguments. testers . BaseTester ( normalize_embeddings = True , use_trunk_output = False , batch_size = 32 , dataloader_num_workers = 2 , pca = None , data_device = None , dtype = None , data_and_label_getter = None , label_hierarchy_level = 0 , end_of_testing_hook = None , dataset_labels = None , set_min_label_to_zero = False , accuracy_calculator = None , visualizer = None , visualizer_hook = None ,) Parameters : normalize_embeddings : If True, embeddings will be normalized to Euclidean norm of 1 before nearest neighbors are computed. use_trunk_output : If True, the output of the trunk_model will be used to compute nearest neighbors, i.e. the output of the embedder model will be ignored. batch_size : How many dataset samples to process at each iteration when computing embeddings. dataloader_num_workers : How many processes the dataloader will use. pca : The number of dimensions that your embeddings will be reduced to, using PCA. The default is None, meaning PCA will not be applied. data_device : Which gpu to use for the loaded dataset samples. If None, then the gpu or cpu will be used (whichever is available). dtype : The type that the dataset output will be converted to, e.g. torch.float16 . If set to None , then no type casting will be done. data_and_label_getter : A function that takes the output of your dataset's __getitem__ function, and returns a tuple of (data, labels). If None, then it is assumed that __getitem__ returns (data, labels). label_hierarchy_level : If each sample in your dataset has multiple labels, then this integer argument can be used to select which \"level\" to use. This assumes that your labels are \"2-dimensional\" with shape (num_samples, num_hierarchy_levels). Leave this at the default value, 0, if your data does not have multiple labels per sample. end_of_testing_hook : This is an optional function that has one input argument (the tester object) and performs some action (e.g. logging data) at the end of testing. You'll probably want to access the accuracy metrics, which are stored in tester.all_accuracies . This is a nested dictionary with the following format: tester.all_accuracies[split_name][metric_name] = metric_value If you want ready-to-use hooks, take a look at the logging_presets module . dataset_labels : The labels for your dataset. Can be 1-dimensional (1 label per datapoint) or 2-dimensional, where each row represents a datapoint, and the columns are the multiple labels that the datapoint has. Labels can be integers or strings. This option needs to be specified only if set_min_label_to_zero is True. set_min_label_to_zero : If True, labels will be mapped such that they represent their rank in the label set. For example, if your dataset has labels 5, 10, 12, 13, then at each iteration, these would become 0, 1, 2, 3. You should also set this to True if you want to use string labels. In that case, 'dog', 'cat', 'monkey' would get mapped to 1, 0, 2. If True, you must pass in dataset_labels (see above). The default is False. accuracy_calculator : Optional. An object that extends AccuracyCalculator . This will be used to compute the accuracy of your model. By default, AccuracyCalculator is used. visualizer : Optional. An object that has implemented the fit_transform method, as done by UMAP and many scikit-learn functions. For example, you can set visualizer = umap.UMAP() . The object's fit_transform function should take in a 2D array of embeddings, and reduce the dimensionality, such that calling visualizer.fit_transform(embeddings) results in a 2D array of size (N, 2). visualizer_hook : Optional. This function will be passed the following args. You can do whatever you want in this function, but the reason it exists is to allow you to save a plot of the embeddings etc. visualizer: The visualizer object that you passed in. embeddings: The dimensionality reduced embeddings. label: The corresponding labels for each embedding. split_name: The name of the split (train, val, etc.) keyname: The name of the dictionary key where the embeddings and labels are stored. epoch: The epoch for which the embeddings are being computed. Functions : tester.test Call this to test your model on a dataset dict. It returns a dictionary of accuracies. all_accuracies = tester . test ( dataset_dict , # dictionary mapping strings to datasets epoch , # used for logging trunk_model , # your model embedder_model = None , # by default this will be a no-op splits_to_eval = None , collate_fn = None # custom collate_fn for the dataloader ) tester.get_all_embeddings Returns all the embeddings and labels for the input dataset and model. embeddings , labels = tester . get_all_embeddings ( dataset , # Any pytorch dataset trunk_model , # your model embedder_model = None , # by default this will be a no-op collate_fn = None , # custom collate_fn for the dataloader eval = True , # set models to eval mode return_as_numpy = False ) GlobalEmbeddingSpaceTester \u00b6 Computes nearest neighbors by looking at all points in the embedding space (rather than a subset). This is probably the tester you are looking for. To see it in action, check one of the example notebooks testers . GlobalEmbeddingSpaceTester ( * args , ** kwargs ) WithSameParentLabelTester \u00b6 This assumes there is a label hierarchy. For each sample, the search space is narrowed by only looking at sibling samples, i.e. samples with the same parent label. For example, consider a dataset with 4 fine-grained classes {cat, dog, car, truck}, and 2 coarse-grained classes {animal, vehicle}. The nearest neighbor search for cats and dogs will consist of animals, and the nearest-neighbor search for cars and trucks will consist of vehicles. testers . WithSameParentLabelTester ( * args , ** kwargs ) GlobalTwoStreamEmbeddingSpaceTester \u00b6 This is the corresponding tester for TwoStreamMetricLoss . The supplied dataset must return (anchor, positive, label) . testers . GlobalTwoStreamEmbeddingSpaceTester ( * args , ** kwargs ) Requirements : This tester only supports the default value for splits_to_eval : each split is used for both query and reference","title":"Testers"},{"location":"testers/#testers","text":"Testers take your model and dataset, and compute nearest-neighbor based accuracy metrics. Note that the testers require the faiss package , which you can install with conda. In general, testers are used as follows: from pytorch_metric_learning import testers t = testers . SomeTestingFunction ( * args , ** kwargs ) dataset_dict = { \"train\" : train_dataset , \"val\" : val_dataset } all_accuracies = tester . test ( dataset_dict , epoch , model ) # Or if your model is composed of a trunk + embedder all_accuracies = tester . test ( dataset_dict , epoch , trunk , embedder ) You can perform custom actions by writing an end-of-testing hook (see the documentation for BaseTester ), and you can access the test results directly via the all_accuracies attribute: def end_of_testing_hook ( tester ): print ( tester . all_accuracies ) This will print out a dictionary of accuracy metrics, per dataset split. You'll see something like this: { \"train\" : { \"AMI_level0\" : 0.53 , ... }, \"val\" : { \"AMI_level0\" : 0.44 , ... }} Each of the accuracy metric names is appended with level0 , which refers to the 0th label hierarchy level (see the documentation for BaseTester ). This is only relevant if you're dealing with multi-label datasets. For an explanation of the default accuracy metrics, see the AccuracyCalculator documentation .","title":"Testers"},{"location":"testers/#testing-splits","text":"By default, every dataset in dataset_dict will be evaluated using itself as the query and reference (on which to find nearest neighbors). More flexibility is allowed with the optional argument splits_to_eval taken by tester.test() . splits_to_eval is a list of (query_split, [list_of_reference_splits]) tuples. For example, let's say your dataset_dict has two keys: \"dataset_a\" and \"train\" . The default splits_to_eval = None is equivalent to: splits_to_eval = [( 'dataset_a' , [ 'dataset_a' ]), ( 'train' , [ 'train' ])] dataset_a as the query, and train as the reference: splits_to_eval = [( 'dataset_a' , [ 'train' ])] dataset_a as the query, and dataset_a + train as the reference: splits_to_eval = [( 'dataset_a' , [ 'dataset_a' , 'train' ])]","title":"Testing splits"},{"location":"testers/#basetester","text":"All trainers extend this class and therefore inherit its __init__ arguments. testers . BaseTester ( normalize_embeddings = True , use_trunk_output = False , batch_size = 32 , dataloader_num_workers = 2 , pca = None , data_device = None , dtype = None , data_and_label_getter = None , label_hierarchy_level = 0 , end_of_testing_hook = None , dataset_labels = None , set_min_label_to_zero = False , accuracy_calculator = None , visualizer = None , visualizer_hook = None ,) Parameters : normalize_embeddings : If True, embeddings will be normalized to Euclidean norm of 1 before nearest neighbors are computed. use_trunk_output : If True, the output of the trunk_model will be used to compute nearest neighbors, i.e. the output of the embedder model will be ignored. batch_size : How many dataset samples to process at each iteration when computing embeddings. dataloader_num_workers : How many processes the dataloader will use. pca : The number of dimensions that your embeddings will be reduced to, using PCA. The default is None, meaning PCA will not be applied. data_device : Which gpu to use for the loaded dataset samples. If None, then the gpu or cpu will be used (whichever is available). dtype : The type that the dataset output will be converted to, e.g. torch.float16 . If set to None , then no type casting will be done. data_and_label_getter : A function that takes the output of your dataset's __getitem__ function, and returns a tuple of (data, labels). If None, then it is assumed that __getitem__ returns (data, labels). label_hierarchy_level : If each sample in your dataset has multiple labels, then this integer argument can be used to select which \"level\" to use. This assumes that your labels are \"2-dimensional\" with shape (num_samples, num_hierarchy_levels). Leave this at the default value, 0, if your data does not have multiple labels per sample. end_of_testing_hook : This is an optional function that has one input argument (the tester object) and performs some action (e.g. logging data) at the end of testing. You'll probably want to access the accuracy metrics, which are stored in tester.all_accuracies . This is a nested dictionary with the following format: tester.all_accuracies[split_name][metric_name] = metric_value If you want ready-to-use hooks, take a look at the logging_presets module . dataset_labels : The labels for your dataset. Can be 1-dimensional (1 label per datapoint) or 2-dimensional, where each row represents a datapoint, and the columns are the multiple labels that the datapoint has. Labels can be integers or strings. This option needs to be specified only if set_min_label_to_zero is True. set_min_label_to_zero : If True, labels will be mapped such that they represent their rank in the label set. For example, if your dataset has labels 5, 10, 12, 13, then at each iteration, these would become 0, 1, 2, 3. You should also set this to True if you want to use string labels. In that case, 'dog', 'cat', 'monkey' would get mapped to 1, 0, 2. If True, you must pass in dataset_labels (see above). The default is False. accuracy_calculator : Optional. An object that extends AccuracyCalculator . This will be used to compute the accuracy of your model. By default, AccuracyCalculator is used. visualizer : Optional. An object that has implemented the fit_transform method, as done by UMAP and many scikit-learn functions. For example, you can set visualizer = umap.UMAP() . The object's fit_transform function should take in a 2D array of embeddings, and reduce the dimensionality, such that calling visualizer.fit_transform(embeddings) results in a 2D array of size (N, 2). visualizer_hook : Optional. This function will be passed the following args. You can do whatever you want in this function, but the reason it exists is to allow you to save a plot of the embeddings etc. visualizer: The visualizer object that you passed in. embeddings: The dimensionality reduced embeddings. label: The corresponding labels for each embedding. split_name: The name of the split (train, val, etc.) keyname: The name of the dictionary key where the embeddings and labels are stored. epoch: The epoch for which the embeddings are being computed. Functions : tester.test Call this to test your model on a dataset dict. It returns a dictionary of accuracies. all_accuracies = tester . test ( dataset_dict , # dictionary mapping strings to datasets epoch , # used for logging trunk_model , # your model embedder_model = None , # by default this will be a no-op splits_to_eval = None , collate_fn = None # custom collate_fn for the dataloader ) tester.get_all_embeddings Returns all the embeddings and labels for the input dataset and model. embeddings , labels = tester . get_all_embeddings ( dataset , # Any pytorch dataset trunk_model , # your model embedder_model = None , # by default this will be a no-op collate_fn = None , # custom collate_fn for the dataloader eval = True , # set models to eval mode return_as_numpy = False )","title":"BaseTester"},{"location":"testers/#globalembeddingspacetester","text":"Computes nearest neighbors by looking at all points in the embedding space (rather than a subset). This is probably the tester you are looking for. To see it in action, check one of the example notebooks testers . GlobalEmbeddingSpaceTester ( * args , ** kwargs )","title":"GlobalEmbeddingSpaceTester"},{"location":"testers/#withsameparentlabeltester","text":"This assumes there is a label hierarchy. For each sample, the search space is narrowed by only looking at sibling samples, i.e. samples with the same parent label. For example, consider a dataset with 4 fine-grained classes {cat, dog, car, truck}, and 2 coarse-grained classes {animal, vehicle}. The nearest neighbor search for cats and dogs will consist of animals, and the nearest-neighbor search for cars and trucks will consist of vehicles. testers . WithSameParentLabelTester ( * args , ** kwargs )","title":"WithSameParentLabelTester"},{"location":"testers/#globaltwostreamembeddingspacetester","text":"This is the corresponding tester for TwoStreamMetricLoss . The supplied dataset must return (anchor, positive, label) . testers . GlobalTwoStreamEmbeddingSpaceTester ( * args , ** kwargs ) Requirements : This tester only supports the default value for splits_to_eval : each split is used for both query and reference","title":"GlobalTwoStreamEmbeddingSpaceTester"},{"location":"trainers/","text":"Trainers \u00b6 Trainers exist in this library because some metric learning algorithms are more than just loss or mining functions. Some algorithms require additional networks, data augmentations, learning rate schedules etc. The goal of the trainers module is to provide access to these type of metric learning algorithms. In general, trainers are used as follows: from pytorch_metric_learning import trainers t = trainers . SomeTrainingFunction ( * args , ** kwargs ) t . train ( num_epochs = 10 ) BaseTrainer \u00b6 All trainers extend this class and therefore inherit its __init__ arguments. trainers . BaseTrainer ( models , optimizers , batch_size , loss_funcs , mining_funcs , dataset , iterations_per_epoch = None , data_device = None , dtype = None , loss_weights = None , sampler = None , collate_fn = None , lr_schedulers = None , gradient_clippers = None , freeze_these = (), freeze_trunk_batchnorm = False , label_hierarchy_level = 0 , dataloader_num_workers = 2 , data_and_label_getter = None , dataset_labels = None , set_min_label_to_zero = False , end_of_iteration_hook = None , end_of_epoch_hook = None ) Parameters : models : A dictionary of the form: {\"trunk\": trunk_model, \"embedder\": embedder_model} The \"embedder\" key is optional. optimizers : A dictionary mapping strings to optimizers. The base class does not require any specific keys. For example, you could provide an empty dictionary, in which case no optimization will happen. Or you could provide just an optimizer for your trunk_model. But most likely, you'll want to pass in: {\"trunk_optimizer\": trunk_optimizer, \"embedder_optimizer\": embedder_optimizer}. batch_size : The number of elements that are retrieved at each iteration. loss_funcs : A dictionary mapping strings to loss functions. The required keys depend on the training method, but all methods are likely to require at least: {\"metric_loss\": loss_func}. mining_funcs : A dictionary mapping strings to mining functions. Pass in an empty dictionary, or one or more of the following keys: {\"subset_batch_miner\": mining_func1, \"tuple_miner\": mining_func2} dataset : The dataset you want to train on. Note that training methods do not perform validation, so do not pass in your validation or test set. data_device : The device that you want to put batches of data on. If not specified, the trainer will put the data on any available GPUs. dtype : The type that the dataset output will be converted to, e.g. torch.float16 . If set to None , then no type casting will be done. iterations_per_epoch : Optional. If you don't specify iterations_per_epoch : 1 epoch = 1 pass through the dataloader iterator. If sampler=None , then 1 pass through the iterator is 1 pass through the dataset. If you use a sampler, then 1 pass through the iterator is 1 pass through the iterable returned by the sampler. For samplers like MPerClassSampler or some offline mining method, the iterable returned might be very long or very short etc, and might not be related to the length of the dataset. The length of the epoch might vary each time the sampler creates a new iterable. In these cases, it can be useful to specify iterations_per_epoch so that each \"epoch\" is just a fixed number of iterations. The definition of epoch matters because there's various things like LR schedulers and hooks that depend on an epoch ending. loss_weights : A dictionary mapping loss names to numbers. Each loss will be multiplied by the corresponding value in the dictionary. If not specified, then no loss weighting will occur. If not specified, then the original labels are used. sampler : The sampler used by the dataloader. If not specified, then random sampling will be used. collate_fn : The collate function used by the dataloader. lr_schedulers : A dictionary of PyTorch learning rate schedulers. Your keys should be strings of the form <model>_<step_type> , where <model> is a key that comes from either the models or loss_funcs dictionary, and <step_type> is one of the following: \"scheduler_by_iteration\" (will be stepped at every iteration) \"scheduler_by_epoch\" (will be stepped at the end of each epoch) \"scheduler_by_plateau\" (will step if accuracy plateaus. This requires you to write your own end-of-epoch hook, compute validation accuracy, and call trainer.step_lr_plateau_schedulers(validation_accuracy) . Or you can use HookContainer .) Here are some example valid lr_scheduler keys: trunk_scheduler_by_iteration metric_loss_scheduler_by_epoch embedder_scheduler_by_plateau gradient_clippers : A dictionary of gradient clipping functions. Each function will be called before the optimizers. freeze_these : Optional. A list or tuple of the names of models or loss functions that should have their parameters frozen during training. These models will have requires_grad set to False, and their optimizers will not be stepped. freeze_trunk_batchnorm : If True, then the BatchNorm parameters of the trunk model will be frozen during training. label_hierarchy_level : If each sample in your dataset has multiple labels, then this integer argument can be used to select which \"level\" to use. This assumes that your labels are \"2-dimensional\" with shape (num_samples, num_hierarchy_levels). Leave this at the default value, 0, if your data does not have multiple labels per sample. dataloader_num_workers : The number of processes your dataloader will use to load data. data_and_label_getter : A function that takes the output of your dataset's __getitem__ function, and returns a tuple of (data, labels). If None, then it is assumed that __getitem__ returns (data, labels). dataset_labels : The labels for your dataset. Can be 1-dimensional (1 label per datapoint) or 2-dimensional, where each row represents a datapoint, and the columns are the multiple labels that the datapoint has. Labels can be integers or strings. This option needs to be specified only if set_min_label_to_zero is True. set_min_label_to_zero : If True, labels will be mapped such that they represent their rank in the label set. For example, if your dataset has labels 5, 10, 12, 13, then at each iteration, these would become 0, 1, 2, 3. You should also set this to True if you want to use string labels. In that case, 'dog', 'cat', 'monkey' would get mapped to 1, 0, 2. If True, you must pass in dataset_labels (see above). The default is False. end_of_iteration_hook : This is an optional function that has one input argument (the trainer object), and performs some action (e.g. logging data) at the end of every iteration. Here are some things you might want to log: trainer.losses : this dictionary contains all loss values at the current iteration. trainer.loss_funcs and trainer.mining_funcs : these dictionaries contain the loss and mining functions. All loss and mining functions in pytorch-metric-learning have an attribute called record_these . This attribute is a list of strings, which are the names of other attributes that are worth recording for the purpose of analysis. For example, the record_these list for TripletMarginLoss is [\"avg_embedding_norm, \"num_non_zero_triplets\"] , so at each iteration you could log the value of trainer.loss_funcs[\"metric_loss\"].avg_embedding_norm and trainer.loss_funcs[\"metric_loss\"].num_non_zero_triplets . To accomplish this programmatically, you can loop through record_these and use the python function getattr to retrieve the attribute value. If you want ready-to-use hooks, take a look at the logging_presets module . end_of_epoch_hook : This is an optional function that operates like end_of_iteration_hook , except this occurs at the end of every epoch, so this might be a suitable place to run validation and save models. To end training early, your hook should return the boolean value False. Note, it must specifically return False , not None , 0 , [] etc. For this hook, you might want to access the following dictionaries: trainer.models , trainer.optimizers , trainer.lr_schedulers , trainer.loss_funcs , and trainer.mining_funcs . If you want ready-to-use hooks, take a look at the logging_presets module . MetricLossOnly \u00b6 This trainer just computes a metric loss from the output of your embedder network. See the example notebook . trainers . MetricLossOnly ( * args , ** kwargs ) Requirements : models : Must have the following form: {\"trunk\": trunk_model} Optionally include \"embedder\": embedder_model loss_funcs : Must have the following form: {\"metric_loss\": loss_func} TrainWithClassifier \u00b6 This trainer is for the case where your architecture is trunk -> embedder -> classifier. It applies a metric loss to the output of the embedder network, and a classification loss to the output of the classifier network. See the example notebook . trainers . TrainWithClassifier ( * args , ** kwargs ) Requirements : models : Must have the following form: {\"trunk\": trunk_model, \"classifier\": classifier_model} Optionally include \"embedder\": embedder_model loss_funcs : Must have the following form: {\"metric_loss\": loss_func1, \"classifier_loss\": loss_func2} CascadedEmbeddings \u00b6 This trainer is a generalization of Hard-Aware Deeply Cascaded Embedding . It splits the output of your embedder network, computing a separate loss for each section. In other words, the output of your embedder should be the concatenation of your cascaded models. See the example notebook . trainers . CascadedEmbeddings ( embedding_sizes , * args , ** kwargs ) Parameters : embedding_sizes: A list of integers, which represent the size of the output of each cascaded model. Requirements : models : Must have the following form: {\"trunk\": trunk_model} Optionally include \"embedder\": embedder_model Optionally include key:values of the form \"classifier_%d\": classifier_model_%d. The integer appended to \"classifier_\" represents the cascaded model that the classifier will be appended to. For example, if the dictionary has classifier_0 and classifier_2, then the 0th and 2nd cascaded models will have classifier_model_0 and classifier_model_2 appended to them. loss_funcs : Must have the following form: {\"metric_loss_%d\": metric_loss_func_%d} Optionally include key:values of the form \"classifier_loss_%d\": classifier_loss_func_%d. The appended integer represents which cascaded model the loss applies to. mining_funcs : Must have the following form: {\"tuple_miner_%d\": mining_func_%d} Optionally include \"subset_batch_miner\": subset_batch_miner DeepAdversarialMetricLearning \u00b6 This is an implementation of Deep Adversarial Metric Learning . See the example notebook . trainers . DeepAdversarialMetricLearning ( metric_alone_epochs = 0 , g_alone_epochs = 0 , g_triplets_per_anchor = 100 , * args , ** kwargs ): Parameters : metric_alone_epochs : At the beginning of training, this many epochs will consist of only the metric_loss. g_alone_epochs : After metric_alone_epochs, this many epochs will consist of only the adversarial generator loss. g_triplets_per_anchor : The number of real triplets per sample that should be passed into the generator. For each real triplet, the generator will output a synthetic triplet. Requirements : models : Must have the following form: {\"trunk\": trunk_model, \"generator\": generator_model} Optionally include \"embedder\": embedder_model Optionally include \"classifier\": classifier_model The input size to the generator must be 3*(size of trunk_model output). The output size must be (size of trunk_model output). loss_funcs : Must have the following form: {\"metric_loss\": metric_loss, \"g_adv_loss\": g_adv_loss, \"synth_loss\": synth_loss} Optionally include \"classifier_loss\": classifier_loss metric_loss applies to the embeddings of real data. g_adv_loss is the adversarial generator loss. Currently, only TripletMarginLoss is supported synth_loss applies to the embeddings of the synthetic generator triplets. loss_weights : Must be one of the following: None {\"metric_loss\": weight1, \"g_adv_loss\": weight2, \"synth_loss\": weight3, \"g_reg_loss\": weight4, \"g_hard_lss\": weight5} Optionally include \"classifier_loss\": classifier_loss \"g_reg_loss\" and \"g_hard_loss\" refer to the regularization losses described in the paper. UnsupervisedEmbeddingsUsingAugmentations \u00b6 This is an implementation of a general approach that has been used in recent unsupervised learning papers, e.g. Unsupervised Embedding Learning via Invariant and Spreading Instance Feature and Unsupervised Deep Metric Learning via Auxiliary Rotation Loss . The idea is that augmented versions of a datapoint should be close to each other in the embedding space. trainers . UnsupervisedEmbeddingsUsingAugmentations ( transforms , data_and_label_setter = None , * args , ** kwargs ) Parameters : transforms : A list of transforms. For every sample in a batch, each transform will be applied to the sample. If there are N transforms and the batch size is B, then there will be a total of B*N augmented samples. data_and_label_setter : A function that takes in a tuple of (augmented_data, pseudo_labels) and outputs whatever is expected by self.data_and_label_getter. TwoStreamMetricLoss \u00b6 This trainer is the same as MetricLossOnly but operates on separate streams of anchors and positives/negatives. The supplied dataset must return (anchor, positive, label) . Given a batch of (anchor, positive, label) , triplets are formed using anchor as the anchor, and positive as either the positive or negative. See the example notebook . trainers . TwoStreamMetricLoss ( * args , ** kwargs ) Requirements : models : Must have the following form: {\"trunk\": trunk_model} Optionally include \"embedder\": embedder_model loss_funcs : Must have the following form: {\"metric_loss\": loss_func} mining_funcs : Only tuple miners are supported","title":"Trainers"},{"location":"trainers/#trainers","text":"Trainers exist in this library because some metric learning algorithms are more than just loss or mining functions. Some algorithms require additional networks, data augmentations, learning rate schedules etc. The goal of the trainers module is to provide access to these type of metric learning algorithms. In general, trainers are used as follows: from pytorch_metric_learning import trainers t = trainers . SomeTrainingFunction ( * args , ** kwargs ) t . train ( num_epochs = 10 )","title":"Trainers"},{"location":"trainers/#basetrainer","text":"All trainers extend this class and therefore inherit its __init__ arguments. trainers . BaseTrainer ( models , optimizers , batch_size , loss_funcs , mining_funcs , dataset , iterations_per_epoch = None , data_device = None , dtype = None , loss_weights = None , sampler = None , collate_fn = None , lr_schedulers = None , gradient_clippers = None , freeze_these = (), freeze_trunk_batchnorm = False , label_hierarchy_level = 0 , dataloader_num_workers = 2 , data_and_label_getter = None , dataset_labels = None , set_min_label_to_zero = False , end_of_iteration_hook = None , end_of_epoch_hook = None ) Parameters : models : A dictionary of the form: {\"trunk\": trunk_model, \"embedder\": embedder_model} The \"embedder\" key is optional. optimizers : A dictionary mapping strings to optimizers. The base class does not require any specific keys. For example, you could provide an empty dictionary, in which case no optimization will happen. Or you could provide just an optimizer for your trunk_model. But most likely, you'll want to pass in: {\"trunk_optimizer\": trunk_optimizer, \"embedder_optimizer\": embedder_optimizer}. batch_size : The number of elements that are retrieved at each iteration. loss_funcs : A dictionary mapping strings to loss functions. The required keys depend on the training method, but all methods are likely to require at least: {\"metric_loss\": loss_func}. mining_funcs : A dictionary mapping strings to mining functions. Pass in an empty dictionary, or one or more of the following keys: {\"subset_batch_miner\": mining_func1, \"tuple_miner\": mining_func2} dataset : The dataset you want to train on. Note that training methods do not perform validation, so do not pass in your validation or test set. data_device : The device that you want to put batches of data on. If not specified, the trainer will put the data on any available GPUs. dtype : The type that the dataset output will be converted to, e.g. torch.float16 . If set to None , then no type casting will be done. iterations_per_epoch : Optional. If you don't specify iterations_per_epoch : 1 epoch = 1 pass through the dataloader iterator. If sampler=None , then 1 pass through the iterator is 1 pass through the dataset. If you use a sampler, then 1 pass through the iterator is 1 pass through the iterable returned by the sampler. For samplers like MPerClassSampler or some offline mining method, the iterable returned might be very long or very short etc, and might not be related to the length of the dataset. The length of the epoch might vary each time the sampler creates a new iterable. In these cases, it can be useful to specify iterations_per_epoch so that each \"epoch\" is just a fixed number of iterations. The definition of epoch matters because there's various things like LR schedulers and hooks that depend on an epoch ending. loss_weights : A dictionary mapping loss names to numbers. Each loss will be multiplied by the corresponding value in the dictionary. If not specified, then no loss weighting will occur. If not specified, then the original labels are used. sampler : The sampler used by the dataloader. If not specified, then random sampling will be used. collate_fn : The collate function used by the dataloader. lr_schedulers : A dictionary of PyTorch learning rate schedulers. Your keys should be strings of the form <model>_<step_type> , where <model> is a key that comes from either the models or loss_funcs dictionary, and <step_type> is one of the following: \"scheduler_by_iteration\" (will be stepped at every iteration) \"scheduler_by_epoch\" (will be stepped at the end of each epoch) \"scheduler_by_plateau\" (will step if accuracy plateaus. This requires you to write your own end-of-epoch hook, compute validation accuracy, and call trainer.step_lr_plateau_schedulers(validation_accuracy) . Or you can use HookContainer .) Here are some example valid lr_scheduler keys: trunk_scheduler_by_iteration metric_loss_scheduler_by_epoch embedder_scheduler_by_plateau gradient_clippers : A dictionary of gradient clipping functions. Each function will be called before the optimizers. freeze_these : Optional. A list or tuple of the names of models or loss functions that should have their parameters frozen during training. These models will have requires_grad set to False, and their optimizers will not be stepped. freeze_trunk_batchnorm : If True, then the BatchNorm parameters of the trunk model will be frozen during training. label_hierarchy_level : If each sample in your dataset has multiple labels, then this integer argument can be used to select which \"level\" to use. This assumes that your labels are \"2-dimensional\" with shape (num_samples, num_hierarchy_levels). Leave this at the default value, 0, if your data does not have multiple labels per sample. dataloader_num_workers : The number of processes your dataloader will use to load data. data_and_label_getter : A function that takes the output of your dataset's __getitem__ function, and returns a tuple of (data, labels). If None, then it is assumed that __getitem__ returns (data, labels). dataset_labels : The labels for your dataset. Can be 1-dimensional (1 label per datapoint) or 2-dimensional, where each row represents a datapoint, and the columns are the multiple labels that the datapoint has. Labels can be integers or strings. This option needs to be specified only if set_min_label_to_zero is True. set_min_label_to_zero : If True, labels will be mapped such that they represent their rank in the label set. For example, if your dataset has labels 5, 10, 12, 13, then at each iteration, these would become 0, 1, 2, 3. You should also set this to True if you want to use string labels. In that case, 'dog', 'cat', 'monkey' would get mapped to 1, 0, 2. If True, you must pass in dataset_labels (see above). The default is False. end_of_iteration_hook : This is an optional function that has one input argument (the trainer object), and performs some action (e.g. logging data) at the end of every iteration. Here are some things you might want to log: trainer.losses : this dictionary contains all loss values at the current iteration. trainer.loss_funcs and trainer.mining_funcs : these dictionaries contain the loss and mining functions. All loss and mining functions in pytorch-metric-learning have an attribute called record_these . This attribute is a list of strings, which are the names of other attributes that are worth recording for the purpose of analysis. For example, the record_these list for TripletMarginLoss is [\"avg_embedding_norm, \"num_non_zero_triplets\"] , so at each iteration you could log the value of trainer.loss_funcs[\"metric_loss\"].avg_embedding_norm and trainer.loss_funcs[\"metric_loss\"].num_non_zero_triplets . To accomplish this programmatically, you can loop through record_these and use the python function getattr to retrieve the attribute value. If you want ready-to-use hooks, take a look at the logging_presets module . end_of_epoch_hook : This is an optional function that operates like end_of_iteration_hook , except this occurs at the end of every epoch, so this might be a suitable place to run validation and save models. To end training early, your hook should return the boolean value False. Note, it must specifically return False , not None , 0 , [] etc. For this hook, you might want to access the following dictionaries: trainer.models , trainer.optimizers , trainer.lr_schedulers , trainer.loss_funcs , and trainer.mining_funcs . If you want ready-to-use hooks, take a look at the logging_presets module .","title":"BaseTrainer"},{"location":"trainers/#metriclossonly","text":"This trainer just computes a metric loss from the output of your embedder network. See the example notebook . trainers . MetricLossOnly ( * args , ** kwargs ) Requirements : models : Must have the following form: {\"trunk\": trunk_model} Optionally include \"embedder\": embedder_model loss_funcs : Must have the following form: {\"metric_loss\": loss_func}","title":"MetricLossOnly"},{"location":"trainers/#trainwithclassifier","text":"This trainer is for the case where your architecture is trunk -> embedder -> classifier. It applies a metric loss to the output of the embedder network, and a classification loss to the output of the classifier network. See the example notebook . trainers . TrainWithClassifier ( * args , ** kwargs ) Requirements : models : Must have the following form: {\"trunk\": trunk_model, \"classifier\": classifier_model} Optionally include \"embedder\": embedder_model loss_funcs : Must have the following form: {\"metric_loss\": loss_func1, \"classifier_loss\": loss_func2}","title":"TrainWithClassifier"},{"location":"trainers/#cascadedembeddings","text":"This trainer is a generalization of Hard-Aware Deeply Cascaded Embedding . It splits the output of your embedder network, computing a separate loss for each section. In other words, the output of your embedder should be the concatenation of your cascaded models. See the example notebook . trainers . CascadedEmbeddings ( embedding_sizes , * args , ** kwargs ) Parameters : embedding_sizes: A list of integers, which represent the size of the output of each cascaded model. Requirements : models : Must have the following form: {\"trunk\": trunk_model} Optionally include \"embedder\": embedder_model Optionally include key:values of the form \"classifier_%d\": classifier_model_%d. The integer appended to \"classifier_\" represents the cascaded model that the classifier will be appended to. For example, if the dictionary has classifier_0 and classifier_2, then the 0th and 2nd cascaded models will have classifier_model_0 and classifier_model_2 appended to them. loss_funcs : Must have the following form: {\"metric_loss_%d\": metric_loss_func_%d} Optionally include key:values of the form \"classifier_loss_%d\": classifier_loss_func_%d. The appended integer represents which cascaded model the loss applies to. mining_funcs : Must have the following form: {\"tuple_miner_%d\": mining_func_%d} Optionally include \"subset_batch_miner\": subset_batch_miner","title":"CascadedEmbeddings"},{"location":"trainers/#deepadversarialmetriclearning","text":"This is an implementation of Deep Adversarial Metric Learning . See the example notebook . trainers . DeepAdversarialMetricLearning ( metric_alone_epochs = 0 , g_alone_epochs = 0 , g_triplets_per_anchor = 100 , * args , ** kwargs ): Parameters : metric_alone_epochs : At the beginning of training, this many epochs will consist of only the metric_loss. g_alone_epochs : After metric_alone_epochs, this many epochs will consist of only the adversarial generator loss. g_triplets_per_anchor : The number of real triplets per sample that should be passed into the generator. For each real triplet, the generator will output a synthetic triplet. Requirements : models : Must have the following form: {\"trunk\": trunk_model, \"generator\": generator_model} Optionally include \"embedder\": embedder_model Optionally include \"classifier\": classifier_model The input size to the generator must be 3*(size of trunk_model output). The output size must be (size of trunk_model output). loss_funcs : Must have the following form: {\"metric_loss\": metric_loss, \"g_adv_loss\": g_adv_loss, \"synth_loss\": synth_loss} Optionally include \"classifier_loss\": classifier_loss metric_loss applies to the embeddings of real data. g_adv_loss is the adversarial generator loss. Currently, only TripletMarginLoss is supported synth_loss applies to the embeddings of the synthetic generator triplets. loss_weights : Must be one of the following: None {\"metric_loss\": weight1, \"g_adv_loss\": weight2, \"synth_loss\": weight3, \"g_reg_loss\": weight4, \"g_hard_lss\": weight5} Optionally include \"classifier_loss\": classifier_loss \"g_reg_loss\" and \"g_hard_loss\" refer to the regularization losses described in the paper.","title":"DeepAdversarialMetricLearning"},{"location":"trainers/#unsupervisedembeddingsusingaugmentations","text":"This is an implementation of a general approach that has been used in recent unsupervised learning papers, e.g. Unsupervised Embedding Learning via Invariant and Spreading Instance Feature and Unsupervised Deep Metric Learning via Auxiliary Rotation Loss . The idea is that augmented versions of a datapoint should be close to each other in the embedding space. trainers . UnsupervisedEmbeddingsUsingAugmentations ( transforms , data_and_label_setter = None , * args , ** kwargs ) Parameters : transforms : A list of transforms. For every sample in a batch, each transform will be applied to the sample. If there are N transforms and the batch size is B, then there will be a total of B*N augmented samples. data_and_label_setter : A function that takes in a tuple of (augmented_data, pseudo_labels) and outputs whatever is expected by self.data_and_label_getter.","title":"UnsupervisedEmbeddingsUsingAugmentations"},{"location":"trainers/#twostreammetricloss","text":"This trainer is the same as MetricLossOnly but operates on separate streams of anchors and positives/negatives. The supplied dataset must return (anchor, positive, label) . Given a batch of (anchor, positive, label) , triplets are formed using anchor as the anchor, and positive as either the positive or negative. See the example notebook . trainers . TwoStreamMetricLoss ( * args , ** kwargs ) Requirements : models : Must have the following form: {\"trunk\": trunk_model} Optionally include \"embedder\": embedder_model loss_funcs : Must have the following form: {\"metric_loss\": loss_func} mining_funcs : Only tuple miners are supported","title":"TwoStreamMetricLoss"},{"location":"extend/losses/","text":"How to write custom loss functions \u00b6 The simplest possible loss function \u00b6 from pytorch_metric_learning.losses import BaseMetricLossFunction import torch class BarebonesLoss ( BaseMetricLossFunction ): def compute_loss ( self , embeddings , labels , indices_tuple , ref_emb , ref_labels ): # perform some calculation # some_loss = torch . mean ( embeddings ) # put into dictionary # return { \"loss\" : { \"losses\" : some_loss , \"indices\" : None , \"reduction_type\" : \"already_reduced\" , } } Compatability with distances and reducers \u00b6 You can make your loss function a lot more powerful by adding support for distance metrics and reducers: from pytorch_metric_learning.losses import BaseMetricLossFunction from pytorch_metric_learning.reducers import AvgNonZeroReducer from pytorch_metric_learning.distances import CosineSimilarity from pytorch_metric_learning.utils import loss_and_miner_utils as lmu import torch class FullFeaturedLoss ( BaseMetricLossFunction ): def compute_loss ( self , embeddings , labels , indices_tuple , ref_emb , ref_labels ): indices_tuple = lmu . convert_to_triplets ( indices_tuple , labels ) anchors , positives , negatives = indices_tuple if len ( anchors ) == 0 : return self . zero_losses () mat = self . distance ( embeddings ) ap_dists = mat [ anchors , positives ] an_dists = mat [ anchors , negatives ] # perform some calculations # losses1 = ap_dists - an_dists losses2 = ap_dists * 5 losses3 = torch . mean ( embeddings ) # put into dictionary # return { \"loss1\" : { \"losses\" : losses1 , \"indices\" : indices_tuple , \"reduction_type\" : \"triplet\" , }, \"loss2\" : { \"losses\" : losses2 , \"indices\" : ( anchors , positives ), \"reduction_type\" : \"pos_pair\" , }, \"loss3\" : { \"losses\" : losses3 , \"indices\" : None , \"reduction_type\" : \"already_reduced\" , }, } def get_default_reducer ( self ): return AvgNonZeroReducer () def get_default_distance ( self ): return CosineSimilarity () def _sub_loss_names ( self ): return [ \"loss1\" , \"loss2\" , \"loss3\" ] Here are a few details about this loss function: It operates on triplets, so convert_to_triplets is used to convert indices_tuple to triplet form. self.distance returns a pairwise distance matrix The output of the loss function is a dictionary that contains multiple sub losses. This is why it overrides the _sub_loss_names function. get_default_reducer is overriden to use AvgNonZeroReducer by default, rather than MeanReducer . get_default_distance is overriden to use CosineSimilarity by default, rather than LpDistances(p=2) . More on distances \u00b6 To make your loss compatible with inverted distances (like cosine similarity), you can check self.distance.is_inverted , and write whatever logic necessary to make your loss make sense in that context. There are also a few functions in self.distance that provide some of this logic, specifically self.distance.smallest_dist , self.distance.largest_dist , and self.distance.margin . The function definitions are pretty straightforward, and you can find them here . Using indices_tuple \u00b6 This is an optional argument passed in from the outside. (See the overview for an example.) It currently has 3 possible forms: None A tuple of size 4, representing the indices of mined pairs (anchors, positives, anchors, negatives) A tuple of size 3, representing the indices of mined triplets (anchors, positives, negatives) To use indices_tuple , use the appropriate conversion function. You don't need to know what type will be passed in, as the conversion function takes care of that: from pytorch_metric_learning.utils import loss_and_miner_utils as lmu # For a pair based loss # After conversion, indices_tuple will be a tuple of size 4 indices_tuple = lmu . convert_to_pairs ( indices_tuple , labels ) # For a triplet based loss # After conversion, indices_tuple will be a tuple of size 3 indices_tuple = lmu . convert_to_triplets ( indices_tuple , labels ) # For a classification based loss # miner_weights.shape == labels.shape # You can use these to weight your loss miner_weights = lmu . convert_to_weights ( indices_tuple , labels , dtype = torch . float32 ) Reduction type \u00b6 The purpose of reduction types is to provide extra information to the reducer, if it needs it. For example, you could write a reducer that behaves differently depending on what kind of loss it receives. Here's a summary of each reduction type: Reduction type Meaning Shape of \"indices\" \"triplet\" Each entry in \"losses\" represents a triplet. A tuple of 3 tensors (anchors, positives, negatives), each of size (N,). \"pos_pair\" Each entry in \"losses\" represents a positive pair. A tuple of 2 tensors (anchors, positives), each of size (N,). \"neg_pair\" Each entry in \"losses\" represents a negative pair. A tuple of 2 tensors (anchors, negatives), each of size (N,). \"element\" Each entry in \"losses\" represents something other than a tuple, e.g. an element in a batch. A tensor of size (N,) \"already_reduced\" \"losses\" is a single number, i.e. the loss has already been reduced. Should be None Some useful examples to look at \u00b6 Here are some existing loss functions that might be useful for reference: ContrastiveLoss MultiSimilarityLoss NormalizedSoftmaxLoss","title":"Custom losses"},{"location":"extend/losses/#how-to-write-custom-loss-functions","text":"","title":"How to write custom loss functions"},{"location":"extend/losses/#the-simplest-possible-loss-function","text":"from pytorch_metric_learning.losses import BaseMetricLossFunction import torch class BarebonesLoss ( BaseMetricLossFunction ): def compute_loss ( self , embeddings , labels , indices_tuple , ref_emb , ref_labels ): # perform some calculation # some_loss = torch . mean ( embeddings ) # put into dictionary # return { \"loss\" : { \"losses\" : some_loss , \"indices\" : None , \"reduction_type\" : \"already_reduced\" , } }","title":"The simplest possible loss function"},{"location":"extend/losses/#compatability-with-distances-and-reducers","text":"You can make your loss function a lot more powerful by adding support for distance metrics and reducers: from pytorch_metric_learning.losses import BaseMetricLossFunction from pytorch_metric_learning.reducers import AvgNonZeroReducer from pytorch_metric_learning.distances import CosineSimilarity from pytorch_metric_learning.utils import loss_and_miner_utils as lmu import torch class FullFeaturedLoss ( BaseMetricLossFunction ): def compute_loss ( self , embeddings , labels , indices_tuple , ref_emb , ref_labels ): indices_tuple = lmu . convert_to_triplets ( indices_tuple , labels ) anchors , positives , negatives = indices_tuple if len ( anchors ) == 0 : return self . zero_losses () mat = self . distance ( embeddings ) ap_dists = mat [ anchors , positives ] an_dists = mat [ anchors , negatives ] # perform some calculations # losses1 = ap_dists - an_dists losses2 = ap_dists * 5 losses3 = torch . mean ( embeddings ) # put into dictionary # return { \"loss1\" : { \"losses\" : losses1 , \"indices\" : indices_tuple , \"reduction_type\" : \"triplet\" , }, \"loss2\" : { \"losses\" : losses2 , \"indices\" : ( anchors , positives ), \"reduction_type\" : \"pos_pair\" , }, \"loss3\" : { \"losses\" : losses3 , \"indices\" : None , \"reduction_type\" : \"already_reduced\" , }, } def get_default_reducer ( self ): return AvgNonZeroReducer () def get_default_distance ( self ): return CosineSimilarity () def _sub_loss_names ( self ): return [ \"loss1\" , \"loss2\" , \"loss3\" ] Here are a few details about this loss function: It operates on triplets, so convert_to_triplets is used to convert indices_tuple to triplet form. self.distance returns a pairwise distance matrix The output of the loss function is a dictionary that contains multiple sub losses. This is why it overrides the _sub_loss_names function. get_default_reducer is overriden to use AvgNonZeroReducer by default, rather than MeanReducer . get_default_distance is overriden to use CosineSimilarity by default, rather than LpDistances(p=2) .","title":"Compatability with distances and reducers"},{"location":"extend/losses/#more-on-distances","text":"To make your loss compatible with inverted distances (like cosine similarity), you can check self.distance.is_inverted , and write whatever logic necessary to make your loss make sense in that context. There are also a few functions in self.distance that provide some of this logic, specifically self.distance.smallest_dist , self.distance.largest_dist , and self.distance.margin . The function definitions are pretty straightforward, and you can find them here .","title":"More on distances"},{"location":"extend/losses/#using-indices_tuple","text":"This is an optional argument passed in from the outside. (See the overview for an example.) It currently has 3 possible forms: None A tuple of size 4, representing the indices of mined pairs (anchors, positives, anchors, negatives) A tuple of size 3, representing the indices of mined triplets (anchors, positives, negatives) To use indices_tuple , use the appropriate conversion function. You don't need to know what type will be passed in, as the conversion function takes care of that: from pytorch_metric_learning.utils import loss_and_miner_utils as lmu # For a pair based loss # After conversion, indices_tuple will be a tuple of size 4 indices_tuple = lmu . convert_to_pairs ( indices_tuple , labels ) # For a triplet based loss # After conversion, indices_tuple will be a tuple of size 3 indices_tuple = lmu . convert_to_triplets ( indices_tuple , labels ) # For a classification based loss # miner_weights.shape == labels.shape # You can use these to weight your loss miner_weights = lmu . convert_to_weights ( indices_tuple , labels , dtype = torch . float32 )","title":"Using indices_tuple"},{"location":"extend/losses/#reduction-type","text":"The purpose of reduction types is to provide extra information to the reducer, if it needs it. For example, you could write a reducer that behaves differently depending on what kind of loss it receives. Here's a summary of each reduction type: Reduction type Meaning Shape of \"indices\" \"triplet\" Each entry in \"losses\" represents a triplet. A tuple of 3 tensors (anchors, positives, negatives), each of size (N,). \"pos_pair\" Each entry in \"losses\" represents a positive pair. A tuple of 2 tensors (anchors, positives), each of size (N,). \"neg_pair\" Each entry in \"losses\" represents a negative pair. A tuple of 2 tensors (anchors, negatives), each of size (N,). \"element\" Each entry in \"losses\" represents something other than a tuple, e.g. an element in a batch. A tensor of size (N,) \"already_reduced\" \"losses\" is a single number, i.e. the loss has already been reduced. Should be None","title":"Reduction type"},{"location":"extend/losses/#some-useful-examples-to-look-at","text":"Here are some existing loss functions that might be useful for reference: ContrastiveLoss MultiSimilarityLoss NormalizedSoftmaxLoss","title":"Some useful examples to look at"},{"location":"extend/miners/","text":"How to write custom mining functions \u00b6 Extend BaseTupleMiner Implement the mine method Inside mine , return a tuple of tensors An example pair miner \u00b6 from pytorch_metric_learning.miners import BaseTupleMiner from pytorch_metric_learning.utils import loss_and_miner_utils as lmu class ExamplePairMiner ( BaseTupleMiner ): def __init__ ( self , margin = 0.1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . margin = margin def mine ( self , embeddings , labels , ref_emb , ref_labels ): mat = self . distance ( embeddings , ref_emb ) a1 , p , a2 , n = lmu . get_all_pairs_indices ( labels , ref_labels ) pos_pairs = mat [ a1 , p ] neg_pairs = mat [ a2 , n ] pos_mask = ( pos_pairs < self . margin if self . distance . is_inverted else pos_pairs > self . margin ) neg_mask = ( neg_pairs > self . margin if self . distance . is_inverted else neg_pairs < self . margin ) return a1 [ pos_mask ], p [ pos_mask ], a2 [ neg_mask ], n [ neg_mask ] The ExamplePairMiner does the following: Computes the distance matrix between embeddings and ref_emb . Finds the indices of all positive and negative pairs Returns the indices of pairs that violate the margin Example usage: miner = ExamplePairMiner () embeddings = torch . randn ( 128 , 512 ) labels = torch . randint ( 0 , 10 , size = ( 128 ,)) pairs = miner ( embeddings , labels ) An example triplet miner \u00b6 from pytorch_metric_learning.miners import BaseTupleMiner from pytorch_metric_learning.utils import loss_and_miner_utils as lmu class ExampleTripletMiner ( BaseTupleMiner ): def __init__ ( self , margin = 0.1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . margin = margin def mine ( self , embeddings , labels , ref_emb , ref_labels ): mat = self . distance ( embeddings , ref_emb ) a , p , n = lmu . get_all_triplets_indices ( labels , ref_labels ) pos_pairs = mat [ a , p ] neg_pairs = mat [ a , n ] triplet_margin = pos_pairs - neg_pairs if self . distance . is_inverted else neg_pairs - pos_pairs triplet_mask = triplet_margin <= self . margin return a [ triplet_mask ], p [ triplet_mask ], n [ triplet_mask ] This miner works similarly to ExamplePairMiner , but finds triplets instead of pairs. What is ref_emb ? \u00b6 The forward function of BaseTupleMiner has optional ref_emb and ref_labels arguments. The miner should return anchors from embeddings and positives and negatives from ref_emb . For example: miner = ExamplePairMiner () embeddings = torch . randn ( 128 , 512 ) labels = torch . randint ( 0 , 10 , size = ( 128 ,)) ref_emb = torch . randn ( 32 , 512 ) ref_labels = torch . randint ( 0 , 10 , size = ( 32 ,)) a1 , p , a2 , n = miner ( embeddings , labels , ref_emb , ref_labels ) # a1 and a2 contain indices of \"embeddings\" # p and n contain indices of \"ref_emb\" Typically though, ref_emb and ref_labels are left to their default value of None , in which case they are set to embeddings and labels before being passed to the mine function.","title":"Custom miners"},{"location":"extend/miners/#how-to-write-custom-mining-functions","text":"Extend BaseTupleMiner Implement the mine method Inside mine , return a tuple of tensors","title":"How to write custom mining functions"},{"location":"extend/miners/#an-example-pair-miner","text":"from pytorch_metric_learning.miners import BaseTupleMiner from pytorch_metric_learning.utils import loss_and_miner_utils as lmu class ExamplePairMiner ( BaseTupleMiner ): def __init__ ( self , margin = 0.1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . margin = margin def mine ( self , embeddings , labels , ref_emb , ref_labels ): mat = self . distance ( embeddings , ref_emb ) a1 , p , a2 , n = lmu . get_all_pairs_indices ( labels , ref_labels ) pos_pairs = mat [ a1 , p ] neg_pairs = mat [ a2 , n ] pos_mask = ( pos_pairs < self . margin if self . distance . is_inverted else pos_pairs > self . margin ) neg_mask = ( neg_pairs > self . margin if self . distance . is_inverted else neg_pairs < self . margin ) return a1 [ pos_mask ], p [ pos_mask ], a2 [ neg_mask ], n [ neg_mask ] The ExamplePairMiner does the following: Computes the distance matrix between embeddings and ref_emb . Finds the indices of all positive and negative pairs Returns the indices of pairs that violate the margin Example usage: miner = ExamplePairMiner () embeddings = torch . randn ( 128 , 512 ) labels = torch . randint ( 0 , 10 , size = ( 128 ,)) pairs = miner ( embeddings , labels )","title":"An example pair miner"},{"location":"extend/miners/#an-example-triplet-miner","text":"from pytorch_metric_learning.miners import BaseTupleMiner from pytorch_metric_learning.utils import loss_and_miner_utils as lmu class ExampleTripletMiner ( BaseTupleMiner ): def __init__ ( self , margin = 0.1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . margin = margin def mine ( self , embeddings , labels , ref_emb , ref_labels ): mat = self . distance ( embeddings , ref_emb ) a , p , n = lmu . get_all_triplets_indices ( labels , ref_labels ) pos_pairs = mat [ a , p ] neg_pairs = mat [ a , n ] triplet_margin = pos_pairs - neg_pairs if self . distance . is_inverted else neg_pairs - pos_pairs triplet_mask = triplet_margin <= self . margin return a [ triplet_mask ], p [ triplet_mask ], n [ triplet_mask ] This miner works similarly to ExamplePairMiner , but finds triplets instead of pairs.","title":"An example triplet miner"},{"location":"extend/miners/#what-is-ref_emb","text":"The forward function of BaseTupleMiner has optional ref_emb and ref_labels arguments. The miner should return anchors from embeddings and positives and negatives from ref_emb . For example: miner = ExamplePairMiner () embeddings = torch . randn ( 128 , 512 ) labels = torch . randint ( 0 , 10 , size = ( 128 ,)) ref_emb = torch . randn ( 32 , 512 ) ref_labels = torch . randint ( 0 , 10 , size = ( 32 ,)) a1 , p , a2 , n = miner ( embeddings , labels , ref_emb , ref_labels ) # a1 and a2 contain indices of \"embeddings\" # p and n contain indices of \"ref_emb\" Typically though, ref_emb and ref_labels are left to their default value of None , in which case they are set to embeddings and labels before being passed to the mine function.","title":"What is ref_emb?"}]}

<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../samplers/">
      
      
        <link rel="next" href="../testers/">
      
      <link rel="icon" href="../imgs/Favicon.png">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.16">
    
    
      
        <title>Trainers - PyTorch Metric Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.26e3688c.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ecc896b0.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="red">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#trainers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="PyTorch Metric Learning" class="md-header__button md-logo" aria-label="PyTorch Metric Learning" data-md-component="logo">
      
  <img src="../imgs/TinyLogo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            PyTorch Metric Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Trainers
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/KevinMusgrave/pytorch-metric-learning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    KevinMusgrave/pytorch-metric-learning
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="PyTorch Metric Learning" class="md-nav__button md-logo" aria-label="PyTorch Metric Learning" data-md-component="logo">
      
  <img src="../imgs/TinyLogo.png" alt="logo">

    </a>
    PyTorch Metric Learning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/KevinMusgrave/pytorch-metric-learning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    KevinMusgrave/pytorch-metric-learning
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../distances/" class="md-nav__link">
        Distances
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../losses/" class="md-nav__link">
        Losses
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../miners/" class="md-nav__link">
        Miners
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../reducers/" class="md-nav__link">
        Reducers
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../regularizers/" class="md-nav__link">
        Regularizers
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../samplers/" class="md-nav__link">
        Samplers
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Trainers
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Trainers
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#basetrainer" class="md-nav__link">
    BaseTrainer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#metriclossonly" class="md-nav__link">
    MetricLossOnly
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#trainwithclassifier" class="md-nav__link">
    TrainWithClassifier
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cascadedembeddings" class="md-nav__link">
    CascadedEmbeddings
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deepadversarialmetriclearning" class="md-nav__link">
    DeepAdversarialMetricLearning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#twostreammetricloss" class="md-nav__link">
    TwoStreamMetricLoss
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../testers/" class="md-nav__link">
        Testers
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10" >
      
      
      
        <label class="md-nav__link" for="__nav_10" id="__nav_10_label" tabindex="0">
          Utils
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_10">
          <span class="md-nav__icon md-icon"></span>
          Utils
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../accuracy_calculation/" class="md-nav__link">
        Accuracy Calculation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../inference_models/" class="md-nav__link">
        Inference Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../logging_presets/" class="md-nav__link">
        Logging Presets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../common_functions/" class="md-nav__link">
        Common Functions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../distributed/" class="md-nav__link">
        Distributed
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11" >
      
      
      
        <label class="md-nav__link" for="__nav_11" id="__nav_11_label" tabindex="0">
          How to extend this library
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_11_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_11">
          <span class="md-nav__icon md-icon"></span>
          How to extend this library
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../extend/losses/" class="md-nav__link">
        Custom losses
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../extend/miners/" class="md-nav__link">
        Custom miners
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../faq/" class="md-nav__link">
        Frequently Asked Questions
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#basetrainer" class="md-nav__link">
    BaseTrainer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#metriclossonly" class="md-nav__link">
    MetricLossOnly
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#trainwithclassifier" class="md-nav__link">
    TrainWithClassifier
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cascadedembeddings" class="md-nav__link">
    CascadedEmbeddings
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deepadversarialmetriclearning" class="md-nav__link">
    DeepAdversarialMetricLearning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#twostreammetricloss" class="md-nav__link">
    TwoStreamMetricLoss
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="trainers">Trainers<a class="headerlink" href="#trainers" title="Permanent link">&para;</a></h1>
<p>Trainers exist in this library because some metric learning algorithms are more than just loss or mining functions. Some algorithms require additional networks, data augmentations, learning rate schedules etc. The goal of the trainers module is to provide access to these type of metric learning algorithms. </p>
<p>In general, trainers are used as follows:
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">pytorch_metric_learning</span> <span class="kn">import</span> <span class="n">trainers</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">trainers</span><span class="o">.</span><span class="n">SomeTrainingFunction</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">t</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></p>
<h2 id="basetrainer">BaseTrainer<a class="headerlink" href="#basetrainer" title="Permanent link">&para;</a></h2>
<p>All trainers extend this class and therefore inherit its <code>__init__</code> arguments.
<div class="highlight"><pre><span></span><code><span class="n">trainers</span><span class="o">.</span><span class="n">BaseTrainer</span><span class="p">(</span><span class="n">models</span><span class="p">,</span>
                    <span class="n">optimizers</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="p">,</span>
                    <span class="n">loss_funcs</span><span class="p">,</span>
                    <span class="n">dataset</span><span class="p">,</span>
                    <span class="n">mining_funcs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">iterations_per_epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">data_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">loss_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">sampler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">collate_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">lr_schedulers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">gradient_clippers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">freeze_these</span><span class="o">=</span><span class="p">(),</span>
                    <span class="n">freeze_trunk_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">label_hierarchy_level</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="n">dataloader_num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                    <span class="n">data_and_label_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">dataset_labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">set_min_label_to_zero</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">end_of_iteration_hook</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">end_of_epoch_hook</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>models</strong>: A dictionary of the form: <ul>
<li>{"trunk": trunk_model, "embedder": embedder_model}</li>
<li>The "embedder" key is optional.</li>
</ul>
</li>
<li><strong>optimizers</strong>: A dictionary mapping strings to optimizers. The base class does not require any specific keys. For example, you could provide an empty dictionary, in which case no optimization will happen. Or you could provide just an optimizer for your trunk_model. But most likely, you'll want to pass in: <ul>
<li>{"trunk_optimizer": trunk_optimizer, "embedder_optimizer": embedder_optimizer}.</li>
</ul>
</li>
<li><strong>batch_size</strong>: The number of elements that are retrieved at each iteration.</li>
<li><strong>loss_funcs</strong>: A dictionary mapping strings to loss functions. The required keys depend on the training method, but all methods are likely to require at least: <ul>
<li>{"metric_loss": loss_func}.</li>
</ul>
</li>
<li><strong>dataset</strong>: The dataset you want to train on. Note that training methods do not perform validation, so do not pass in your validation or test set.</li>
<li><strong>mining_funcs</strong>: A dictionary mapping strings to mining functions. Pass in an empty dictionary, or one or more of the following keys: <ul>
<li>{"subset_batch_miner": mining_func1, "tuple_miner": mining_func2}</li>
</ul>
</li>
<li><strong>data_device</strong>: The device that you want to put batches of data on. If not specified, the trainer will put the data on any available GPUs.</li>
<li><strong>dtype</strong>: The type that the dataset output will be converted to, e.g. <code>torch.float16</code>. If set to <code>None</code>, then no type casting will be done.</li>
<li><strong>iterations_per_epoch</strong>: Optional. <ul>
<li>If you don't specify <code>iterations_per_epoch</code>:<ul>
<li>1 epoch = 1 pass through the dataloader iterator. If <code>sampler=None</code>, then 1 pass through the iterator is 1 pass through the dataset. </li>
<li>If you use a sampler, then 1 pass through the iterator is 1 pass through the iterable returned by the sampler.</li>
</ul>
</li>
<li>For samplers like <code>MPerClassSampler</code> or some offline mining method, the iterable returned might be very long or very short etc, and might not be related to the length of the dataset. The length of the epoch might vary each time the sampler creates a new iterable. In these cases, it can be useful to specify <code>iterations_per_epoch</code> so that each "epoch" is just a fixed number of iterations. The definition of epoch matters because there's various things like LR schedulers and hooks that depend on an epoch ending.</li>
</ul>
</li>
<li><strong>loss_weights</strong>: A dictionary mapping loss names to numbers. Each loss will be multiplied by the corresponding value in the dictionary. If not specified, then no loss weighting will occur.
If not specified, then the original labels are used.</li>
<li><strong>sampler</strong>: The sampler used by the dataloader. If not specified, then random sampling will be used.</li>
<li><strong>collate_fn</strong>: The collate function used by the dataloader.</li>
<li><strong>lr_schedulers</strong>: A dictionary of PyTorch learning rate schedulers. Your keys should be strings of the form <code>&lt;model&gt;_&lt;step_type&gt;</code>, where <code>&lt;model&gt;</code> is a key that comes from either the <code>models</code> or <code>loss_funcs</code> dictionary, and <code>&lt;step_type&gt;</code> is one of the following:<ul>
<li>"scheduler_by_iteration" (will be stepped at every iteration)</li>
<li>"scheduler_by_epoch" (will be stepped at the end of each epoch)</li>
<li>"scheduler_by_plateau" (will step if accuracy plateaus. This requires you to write your own end-of-epoch hook, compute validation accuracy, and call <code>trainer.step_lr_plateau_schedulers(validation_accuracy)</code>. Or you can use <a href="../logging_presets/">HookContainer</a>.)</li>
<li>Here are some example valid <code>lr_scheduler</code> keys: <ul>
<li><code>trunk_scheduler_by_iteration</code></li>
<li><code>metric_loss_scheduler_by_epoch</code></li>
<li><code>embedder_scheduler_by_plateau</code></li>
</ul>
</li>
</ul>
</li>
<li><strong>gradient_clippers</strong>: A dictionary of gradient clipping functions. Each function will be called before the optimizers.</li>
<li><strong>freeze_these</strong>: Optional. A list or tuple of the names of models or loss functions that should have their parameters frozen during training. These models will have <code>requires_grad</code> set to False, and their optimizers will not be stepped. </li>
<li><strong>freeze_trunk_batchnorm</strong>: If True, then the BatchNorm parameters of the trunk model will be frozen during training.</li>
<li><strong>label_hierarchy_level</strong>: If each sample in your dataset has multiple labels, then this integer argument can be used to select which "level" to use. This assumes that your labels are "2-dimensional" with shape (num_samples, num_hierarchy_levels). Leave this at the default value, 0, if your data does not have multiple labels per sample.</li>
<li><strong>dataloader_num_workers</strong>: The number of processes your dataloader will use to load data.</li>
<li><strong>data_and_label_getter</strong>: A function that takes the output of your dataset's <code>__getitem__</code> function, and returns a tuple of (data, labels). If None, then it is assumed that <code>__getitem__</code> returns (data, labels). </li>
<li><strong>dataset_labels</strong>: The labels for your dataset. Can be 1-dimensional (1 label per datapoint) or 2-dimensional, where each row represents a datapoint, and the columns are the multiple labels that the datapoint has. Labels can be integers or strings. <strong>This option needs to be specified only if <code>set_min_label_to_zero</code> is True.</strong></li>
<li><strong>set_min_label_to_zero</strong>: If True, labels will be mapped such that they represent their rank in the label set. For example, if your dataset has labels 5, 10, 12, 13, then at each iteration, these would become 0, 1, 2, 3. You should also set this to True if you want to use string labels. In that case, 'dog', 'cat', 'monkey' would get mapped to 1, 0, 2. If True, you must pass in <code>dataset_labels</code> (see above). The default is False.</li>
<li><strong>end_of_iteration_hook</strong>: This is an optional function that has one input argument (the trainer object), and performs some action (e.g. logging data) at the end of every iteration. Here are some things you might want to log:<ul>
<li><code>trainer.losses</code>: this dictionary contains all loss values at the current iteration. </li>
<li><code>trainer.loss_funcs</code> and <code>trainer.mining_funcs</code>: these dictionaries contain the loss and mining functions. <ul>
<li>Some loss and mining functions have attributes called <code>_record_these</code> or <code>_record_these_stats</code>. These are lists of names of other attributes that might be useful to log. (The list of attributes might change depending on the value of <a href="../common_functions/#collect_stats">COLLECT_STATS</a>.) For example, the <code>_record_these_stats</code> list for <code>BaseMiner</code> is <code>["num_pos_pairs", "num_neg_pairs", "num_triplets"]</code>, so at each iteration you could log the value of <code>trainer.mining_funcs["tuple_miner"].num_pos_pairs</code>. To accomplish this programmatically, you can use <a href="https://github.com/KevinMusgrave/record-keeper">record-keeper</a>. Or you can do it yourself: first check if the object has <code>_record_these</code> or <code>_record_these_stats</code>, and use the python function <code>getattr</code> to retrieve the specified attributes. </li>
</ul>
</li>
<li>If you want ready-to-use hooks, take a look at the <a href="../logging_presets/">logging_presets module</a>.</li>
</ul>
</li>
<li><strong>end_of_epoch_hook</strong>: This is an optional function that operates like <code>end_of_iteration_hook</code>, except this occurs at the end of every epoch, so this might be a suitable place to run validation and save models. <ul>
<li>To end training early, your hook should return the boolean value False. Note, it must specifically <code>return False</code>, not <code>None</code>, <code>0</code>, <code>[]</code> etc.</li>
<li>For this hook, you might want to access the following dictionaries: <code>trainer.models</code>, <code>trainer.optimizers</code>, <code>trainer.lr_schedulers</code>, <code>trainer.loss_funcs</code>, and <code>trainer.mining_funcs</code>.</li>
<li>If you want ready-to-use hooks, take a look at the <a href="../logging_presets/">logging_presets module</a>.</li>
</ul>
</li>
</ul>
<h2 id="metriclossonly">MetricLossOnly<a class="headerlink" href="#metriclossonly" title="Permanent link">&para;</a></h2>
<p>This trainer just computes a metric loss from the output of your embedder network. See <a href="https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/MetricLossOnly.ipynb">the example notebook</a>.
<div class="highlight"><pre><span></span><code><span class="n">trainers</span><span class="o">.</span><span class="n">MetricLossOnly</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Requirements</strong>:</p>
<ul>
<li><strong>models</strong>: Must have the following form:<ul>
<li>{"trunk": trunk_model}</li>
<li>Optionally include "embedder": embedder_model</li>
</ul>
</li>
<li><strong>loss_funcs</strong>: Must have the following form:<ul>
<li>{"metric_loss": loss_func}</li>
</ul>
</li>
</ul>
<h2 id="trainwithclassifier">TrainWithClassifier<a class="headerlink" href="#trainwithclassifier" title="Permanent link">&para;</a></h2>
<p>This trainer is for the case where your architecture is trunk -&gt; embedder -&gt; classifier. It applies a metric loss to the output of the embedder network, and a classification loss to the output of the classifier network. See <a href="https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/TrainWithClassifier.ipynb">the example notebook</a>.
<div class="highlight"><pre><span></span><code><span class="n">trainers</span><span class="o">.</span><span class="n">TrainWithClassifier</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<strong>Requirements</strong>:</p>
<ul>
<li><strong>models</strong>: Must have the following form: <ul>
<li>{"trunk": trunk_model, "classifier": classifier_model}</li>
<li>Optionally include "embedder": embedder_model</li>
</ul>
</li>
<li><strong>loss_funcs</strong>: Must have the following form:<ul>
<li>{"metric_loss": loss_func1, "classifier_loss": loss_func2}</li>
</ul>
</li>
</ul>
<h2 id="cascadedembeddings">CascadedEmbeddings<a class="headerlink" href="#cascadedembeddings" title="Permanent link">&para;</a></h2>
<p>This trainer is a generalization of <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Yuan_Hard-Aware_Deeply_Cascaded_ICCV_2017_paper.pdf">Hard-Aware Deeply Cascaded Embedding</a>. It splits the output of your embedder network, computing a separate loss for each section. In other words, the output of your embedder should be the concatenation of your cascaded models. See <a href="https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/CascadedEmbeddings.ipynb">the example notebook</a>.</p>
<div class="highlight"><pre><span></span><code><span class="n">trainers</span><span class="o">.</span><span class="n">CascadedEmbeddings</span><span class="p">(</span><span class="n">embedding_sizes</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<p><strong>Parameters</strong>:</p>
<ul>
<li>embedding_sizes: A list of integers, which represent the size of the output of each cascaded model.</li>
</ul>
<p><strong>Requirements</strong>:</p>
<ul>
<li>
<p><strong>models</strong>: Must have the following form:</p>
<ul>
<li>{"trunk": trunk_model}<ul>
<li>Optionally include "embedder": embedder_model</li>
<li>Optionally include key:values of the form "classifier_%d": classifier_model_%d. The integer appended to "classifier_" represents the cascaded model that the classifier will be appended to. For example, if the dictionary has classifier_0 and classifier_2, then the 0th and 2nd cascaded models will have classifier_model_0 and classifier_model_2 appended to them.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>loss_funcs</strong>: Must have the following form:</p>
<ul>
<li>{"metric_loss_%d": metric_loss_func_%d}<ul>
<li>Optionally include key:values of the form "classifier_loss_%d": classifier_loss_func_%d. The appended integer represents which cascaded model the loss applies to.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>mining_funcs</strong>: Must have the following form:</p>
<ul>
<li>{"tuple_miner_%d": mining_func_%d}<ul>
<li>Optionally include "subset_batch_miner": subset_batch_miner</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="deepadversarialmetriclearning">DeepAdversarialMetricLearning<a class="headerlink" href="#deepadversarialmetriclearning" title="Permanent link">&para;</a></h2>
<p>This is an implementation of <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Duan_Deep_Adversarial_Metric_CVPR_2018_paper.pdf">Deep Adversarial Metric Learning</a>. See <a href="https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/DeepAdversarialMetricLearning.ipynb">the example notebook</a>.
<div class="highlight"><pre><span></span><code><span class="n">trainers</span><span class="o">.</span><span class="n">DeepAdversarialMetricLearning</span><span class="p">(</span><span class="n">metric_alone_epochs</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                    <span class="n">g_alone_epochs</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                    <span class="n">g_triplets_per_anchor</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                                    <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</code></pre></div></p>
<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>metric_alone_epochs</strong>: At the beginning of training, this many epochs will consist of only the metric_loss.</li>
<li><strong>g_alone_epochs</strong>: After metric_alone_epochs, this many epochs will consist of only the adversarial generator loss.</li>
<li><strong>g_triplets_per_anchor</strong>: The number of real triplets per sample that should be passed into the generator. For each real triplet, the generator will output a synthetic triplet.</li>
</ul>
<p><strong>Requirements</strong>:</p>
<ul>
<li>
<p><strong>models</strong>: Must have the following form:</p>
<ul>
<li>{"trunk": trunk_model, "generator": generator_model}<ul>
<li>Optionally include "embedder": embedder_model</li>
<li>Optionally include "classifier": classifier_model</li>
<li>The input size to the generator must be 3*(size of trunk_model output). The output size must be (size of trunk_model output).</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>loss_funcs</strong>: Must have the following form:</p>
<ul>
<li>{"metric_loss": metric_loss, "g_adv_loss": g_adv_loss, "synth_loss": synth_loss}<ul>
<li>Optionally include "classifier_loss": classifier_loss</li>
<li>metric_loss applies to the embeddings of real data.</li>
<li>g_adv_loss is the adversarial generator loss. <strong>Currently, only TripletMarginLoss is supported</strong></li>
<li>synth_loss applies to the embeddings of the synthetic generator triplets.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>loss_weights</strong>: Must be one of the following:</p>
<ul>
<li>None</li>
<li>{"metric_loss": weight1, "g_adv_loss": weight2, "synth_loss": weight3, "g_reg_loss": weight4, "g_hard_lss": weight5}<ul>
<li>Optionally include "classifier_loss": classifier_loss</li>
<li>"g_reg_loss" and "g_hard_loss" refer to the regularization losses described in the paper.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="twostreammetricloss">TwoStreamMetricLoss<a class="headerlink" href="#twostreammetricloss" title="Permanent link">&para;</a></h2>
<p>This trainer is the same as <a href="./#metriclossonly">MetricLossOnly</a> but operates on separate streams of anchors and positives/negatives.
The supplied <strong>dataset</strong> must return <code>(anchor, positive, label)</code>.
Given a batch of <code>(anchor, positive, label)</code>, triplets are formed using <code>anchor</code> as the anchor, and <code>positive</code> as either the positive or negative. See <a href="https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/TwoStreamMetricLoss.ipynb">the example notebook</a>.
<div class="highlight"><pre><span></span><code><span class="n">trainers</span><span class="o">.</span><span class="n">TwoStreamMetricLoss</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
<strong>Requirements</strong>:</p>
<ul>
<li><strong>models</strong>: Must have the following form:<ul>
<li>{"trunk": trunk_model}</li>
<li>Optionally include "embedder": embedder_model</li>
</ul>
</li>
<li><strong>loss_funcs</strong>: Must have the following form:<ul>
<li>{"metric_loss": loss_func}</li>
</ul>
</li>
<li><strong>mining_funcs</strong>: Only tuple miners are supported</li>
</ul>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.a51614de.min.js"></script>
      
    
  </body>
</html>